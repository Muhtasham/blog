---
title: "ðŸ‘¾ Exploring Watermark Security in Language Models"
author: "muhtasham"
date: "2023-11-20"
categories: [llm, papers, security, tokenizer]
image: "attack.png"
toc: true
---

![From the pioneering era of Spacewar! on the DEC PDP-1, this post transitions into the contemporary sphere of language model security. Image captured at the Computer History Museum by the author.](attack.png)

In this brief exploration, I delve into a groundbreaking concept: a unique watermarking technique for large language models (LLMs), as detailed in [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226). While the idea of watermarking LLMs itself is intriguing, what captivated my attention even more were the various strategies to attack these watermarks. In this post, I'll share key insights from this research, highlighting adversarial attacks.

## Threat Model and Adversarial Behavior
Exploring the threat model, we understand the diverse ways adversaries might attempt to manipulate or remove watermarks in machine-generated text. These methods range from social media bots to CAPTCHA circumvention and academic dishonesty.

## Parties Involved
Two primary parties are central to this narrative:
- **Model Owner**: Provides a text generation API, incorporating watermarks to track text origin.
- **Attacker**: Strives to erase watermarks, aware of their presence and possibly even the underlying technology.

## Modes of Operation
The interaction occurs in two distinct modes:
- **Public Mode**: Attackers possess complete knowledge of the hashing scheme.
- **Private Mode**: Attackers are aware of the watermark but lack insights into the key mechanisms.

## Attack Strategies
We categorize attack strategies into three main types:
1. **Text Insertion Attacks**
2. **Text Deletion Attacks**
3. **Text Substitution Attacks**

Each type carries unique challenges and implications for text integrity and computational efficiency.

## Specific Attacks and Mitigations
- **Paraphrasing Attacks**: Ranging from manual to automated, these attacks involve rephrasing using weaker models.
- **Discreet Alterations**: Minor modifications like whitespace addition or misspelling, underlining the importance of text normalization.
- **Tokenization Attacks**: Altering text to modify sub-word tokenization, impacting the watermarking process.
- **Homoglyph and Zero-Width Attacks**: Using visually similar or invisible characters to disrupt standard tokenization.
- **Generative Attacks**: Manipulating model output predictably, such as the "Emoji Attack."

## Conclusion
This brief exploration into watermarking techniques and their vulnerabilities in LLMs touches only the surface of a much larger topic. For a more in-depth understanding of Adversarial Attacks on LLMs, Lilian Weng's blog is an excellent resource. As of 11/20/2023, she remains a pivotal figure at OpenAI, possibly leading the AI Safety team. In light of recent challenges within OpenAI's board, where attempts at adversarial tactics were met with unity and loyalty from the team, Weng's insights are particularly relevant and enlightening [Lilian Weng's Blog](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/).
