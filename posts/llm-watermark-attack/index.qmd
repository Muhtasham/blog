---
title: "ðŸ‘¾ Exploring Watermark Security in Language Models"
author: "muhtasham"
date: "2023-11-20"
categories: [llm, papers, security, tokenizer]
image: "attack.png"
toc: true
---

![Spacewar! on DEC PDP-1](attack.png)

_From the pioneering era of Spacewar! on the DEC PDP-1, this post transitions into the contemporary sphere of language model security._

As I dove into the fascinating world of language models, I recently stumbled upon a groundbreaking concept: a unique watermarking technique for large language models (LLMs), as detailed in [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226). While the idea of watermarking LLMs itself is intriguing, what captivated my attention even more were the various strategies to attack these watermarks. In this post, I'll share key insights from this research, highlighting both the innovative approach to watermarking and the intriguing complexity of its potential vulnerabilities.


## Threat Model and Adversarial Behavior
Exploring the threat model, we understand the diverse ways adversaries might attempt to manipulate or remove watermarks in machine-generated text. These methods range from social media bots to CAPTCHA circumvention and academic dishonesty.

## Parties Involved
Two primary parties are central to this narrative:
- **Model Owner**: Provides a text generation API, incorporating watermarks to track text origin.
- **Attacker**: Strives to erase watermarks, aware of their presence and possibly even the underlying technology.

## Modes of Operation
The interaction occurs in two distinct modes:
- **Public Mode**: Attackers possess complete knowledge of the hashing scheme.
- **Private Mode**: Attackers are aware of the watermark but lack insights into the key mechanisms.

## Attack Strategies
We categorize attack strategies into three main types:
1. **Text Insertion Attacks**
2. **Text Deletion Attacks**
3. **Text Substitution Attacks**

Each type carries unique challenges and implications for text integrity and computational efficiency.

## Specific Attacks and Mitigations
- **Paraphrasing Attacks**: Ranging from manual to automated, these attacks involve rephrasing using weaker models.
- **Discreet Alterations**: Minor modifications like whitespace addition or misspelling, underlining the importance of text normalization.
- **Tokenization Attacks**: Altering text to modify sub-word tokenization, impacting the watermarking process.
- **Homoglyph and Zero-Width Attacks**: Using visually similar or invisible characters to disrupt standard tokenization.
- **Generative Attacks**: Manipulating model output predictably, such as the "Emoji Attack."

## Evaluating and Mitigating Repetitive Text
The paper also addresses the evaluation of repetitive text and proposes strategies to mitigate related challenges.

## Conclusion
This exploration into watermarking techniques and their vulnerabilities in LLMs reveals a dynamic and evolving field. For those interested in a more comprehensive understanding of Adversarial Attacks on LLMs, I highly recommend Lilian Weng's blog. As of 11/20/2023, she's possibly leading a team on AI Safety at OpenAI, delving deep into adversarial attacks. Amidst the current developments within OpenAI's board, Weng's insights provide a compelling read on the subject [Lilian Weng's Blog](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/).
