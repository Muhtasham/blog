---
title: "ðŸ˜® GANs are so back! It is so over"
author: "muhtasham"
date: "2024-02-24"
toc: true
number-sections: false
image: z.jpeg
categories: [deep-learning, papers]
---

Last week, an intriguing project by my friends at [fal](https://fal.ai/) caught the internet by storm:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This is wild<a href="https://t.co/tStncI6aip">https://t.co/tStncI6aip</a><br><br>You can clone &amp; deploy this on <a href="https://twitter.com/vercel?ref_src=twsrc%5Etfw">@vercel</a>. Install the <a href="https://twitter.com/fal_ai_data?ref_src=twsrc%5Etfw">@fal_ai_data</a> integration with a couple clicks. <a href="https://t.co/AbrJlgWkjJ">pic.twitter.com/AbrJlgWkjJ</a></p>&mdash; Guillermo Rauch (@rauchg) <a href="https://twitter.com/rauchg/status/1761138889019077104?ref_src=twsrc%5Etfw">February 23, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

I was so impressed that I dove deep into the technology behind the scenes, beyond the infrastructure of fal, to share with you here. 

**Spoiler alert**: *it involves GANs.*

GANs hold a special place in my heart since I started my ML journey with them, oh gosh it was so hard to train them due to mode collapse back in 2018. Anyway, here's a brief overview of the key ideas from the paper ["SDXL-Lightning: Progressive Adversarial Diffusion Distillation"](https://arxiv.org/abs/2402.13929)

**Denoising Diffusion GAN for Distillation:**

- The challenge with distilling diffusion models at small step sizes lies in the MSE ambiguity problem.
- Small steps require modeling a complex flow, where a Gaussian distribution isn't sufficient.
- The solution? Distill using a GAN, with the discriminator conditioned on pairs of (x, x_nstep_denoised), sampling from either the teacher network or the student.
![](eq1.png)
and re-use the weights from the teacher encoder for the discriminator.

**Janus Artifacts:**

![](fig2.png)

- Despite the student model having less capacity than the teacher, leading to a mismatch, relaxing mode-coverage can reduce complexity.
- The approach involves fine-tuning on unconditional flows for better results.


**Training Details:**

- The distillation process involves transitioning from 128 steps to 32 with MSE loss, and then further distilling to 32, 8, 4, 3, and 1 step with GAN loss.
- The process incorporates both conditional and unconditional training at each stage, employing LoRA for initial training before fully fine-tuning the model.
- Training spans multiple timesteps, enhancing generation capabilities even when not strictly necessary.

Check [Model card](https://huggingface.co/ByteDance/SDXL-Lightning) for more details.
