---
title: "Meta-Engineering: Letting Codex-Spark Simulates the Silicon that it runs on"
description: "Prompting a wafer-scale chip simulator at 1,000 tokens per second"
author: "muhtasham"
date: "2026-02-14"
image: "images/thumbnail.png"
image-alt: "Cerebras defect-tolerance simulator and Codex-Spark real-time coding workflow"
categories: [llm, agents, software-engineering]
title-block-banner: true
---

## A Chip Simulating Itself

[GPT-5.3-Codex-Spark](https://openai.com/index/introducing-gpt-5-3-codex-spark/) is a smaller version of Codex optimized for fast inference. Powered by the **Cerebras Wafer-Scale Engine**, it runs at over **1,000 tokens/s**—the same chip that runs OpenAI's GPT OSS 120B at 3,000 tokens/s. I used it to build an interactive simulator of that very chip. The hardware helped explain itself—a recursive loop of silicon and AI.

This isn't just meta for fun. The infra behind this is coming in screaming hot—directly from the surface of the sun—but it's a cool glimpse into what will become mainstream in the next few months. And when your hardware is literally sparking at thousands of tokens per second...

::: {.callout-tip appearance="simple"}
...this one sparks joy ✨
:::

## Why Wafer-Scale?

![Four trillion transistors. 125 petaflops. One silicon wafer.](images/chip.png)

Look closely at the Cerebras chip—you'll notice a grid-and-holes pattern covering the entire surface. That's because wafers this large are normally diced into dozens of separate chips. Cerebras instead etches one giant chip across the entire wafer.

Why? **On-chip memory.** A wafer-scale design can pack **44GB of SRAM** directly into the chip. Standard GPUs have tens of megabytes. That difference matters: inference typically spends significant time streaming model weights from external memory into the GPU. With enough SRAM to fit the entire model, those transfers happen at SRAM speed instead—yielding a **15x speedup**.

## The Problem: 900,000 Cores, Guaranteed Defects

When you manufacture ~900,000 cores on a dinner-plate-sized chip, defects aren't a matter of *if*—they're *how many*. The Cerebras approach: assume defects will happen, then route around them intelligently.

But how do you explain defect-tolerant mesh routing to someone who's never thought about chip yields?

Being a [Cerebras Fellow](https://www.cerebras.ai/fellows) and now [Codex Ambassador](https://developers.openai.com/codex/ambassadors/), so right after Codex-Spark launched, I wanted to build something immediately graspable—a single interactive page where you adjust defect rates, run remap logic, and watch packets reroute in real time.

The tight iteration loop (prompt → verify → refine) made it possible to clarify both the explanation and the routing logic until everything clicked.

This simulator is the result. It was built through prompting Codex-Spark and references these public sources:

- [Cerebras 100x defect tolerance post](https://www.cerebras.ai/blog/100x-defect-tolerance-how-cerebras-solved-the-yield-problem)
- [IEEE Micro: Path to Successful Wafer-Scale Integration](https://8968533.fs1.hubspotusercontent-na2.net/hubfs/8968533/IEEE%20Micro%202021-11%20Path%20to%20Wafer%20Scale%20Integration.pdf)
- [Cerebras WSE architecture whitepaper](https://cdn.sanity.io/files/e4qjo92p/production/b3779b8136a3a03a7b31913cb680e33d68423e58.pdf)

Codex-Spark is also powered by [Cerebras’ Wafer Scale Engine 3](https://www.cerebras.ai/chip), a purpose-built AI accelerator for high-speed inference that gives the model a latency-first serving tier. That is why this post is able to move from idea to working interaction so quickly.

## Try It First

The simulator is embedded below and uses client-side JavaScript only. Start with the defaults, then experiment.

::: {.callout-note}
**Default settings:** manufacturing tile defects **12%**, inter-tile link defects **6%**, spare-core remap budget **7%**, and **40** yield simulation runs.

**Then try:**

- Disable remap to see failure impact.
- Increase link defects and compare route resilience.
- Turn mesh links off and compare observability.
:::

```{=html}
<iframe src="cerebras-visualization.html" title="Cerebras defect-tolerance simulator" style="width:100%; height:860px; border:0; border-radius:12px;" loading="lazy"></iframe>
```

::: {style="text-align: center; margin-top: 1rem;"}
[Open simulator in full screen →](cerebras-visualization.html){target="_blank" style="font-size: 0.9rem;"}
:::

## How It Was Built: The Real-Time Codex-Spark Workflow

This project is as much about the workflow as it is about hardware simulation. Building with Codex-Spark felt genuinely different from normal coding cycles.

### What Makes It Feel Different

1. **Ask, don't script**: I could just ask for concrete changes like "add a spare-core budget slider" or "make the status line conversational," and get it immediately.
2. **Ship faster, learn faster**: each iteration was visible in seconds, so wrong assumptions were corrected in minutes, not a full afternoon.
3. **Real-time stays real-time**: the UI updates and simulation loops stayed fluid while iterating—no waiting, no lag.
4. **You can ask for features, not just code**: examples like source-note cards, clearer beginner text, and route status semantics were all handled the same way through conversation.

### The Latency Breakthrough That Enables This

On agentic software engineering benchmarks like **SWE-Bench Pro** and **Terminal-Bench 2.0**, Codex-Spark produces more capable responses than GPT-5.1-Codex-mini while completing tasks in a fraction of the time. It excels at precise edits, plan revision, and contextual questions about your codebase—exactly the tight loop this simulator needed.

As OpenAI trained Codex-Spark, they realized model speed was just part of the equation for real-time collaboration. They implemented **end-to-end latency improvements** that benefit all models:

- **80% reduction** in overhead per client/server roundtrip
- **30% reduction** in per-token overhead
- **50% reduction** in time-to-first-token

Under the hood, they streamlined how responses stream from client to server and back, rewrote key pieces of the inference stack, and reworked session initialization. Through a persistent **WebSocket connection** and targeted optimizations inside the Responses API, Codex stays responsive as you iterate.

This is why "ask for a feature, see it work, ask for the next pass" actually works in practice—the latency is low enough that the conversation never breaks your flow.

```{=html}
<div class="callout callout-style-default">
  <div class="callout-body">
    You can literally treat this as an interactive design conversation: ask Codex to build a feature, ask it to simplify wording, then ask for the next pass. Then repeat.
  </div>
</div>
```

## What this simulator shows

The simulation has three practical layers:

- A **tile mesh**: simplified compute regions connected by a mesh network.
- **Fault controls**: manufacturing defects, inter-tile link defects, remap budget, and reliability-run count.
- **Packet flow + remap**: route search around blocked regions and optional repair conversion using spare capacity.

Each render reports:

- healthy vs defective/repaired tiles
- bad links and blocked connectivity
- route found / blocked outcomes
- path length and a reliability summary from repeated random trials

That output mirrors the core lesson from wafer-scale systems: local defects are manageable when routing options and spare resources are intelligently coordinated.