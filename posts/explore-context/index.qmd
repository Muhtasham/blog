---
title: "ðŸ—¾ Exploring Ways to Extend Context Length in Transformers"
title-block-banner: true
author: "muhtasham"
date: "2023-08-07"
categories: [LLM]
toc: true
---

## Introduction 
Diving deep into the intricacies of large language models (LLMs), one hurdle quickly becomes evident: the context length limitation. While many recognize its implications, the question remains: how can we train models on more extensive context lengths without falling into the predicament of quadratic complexity?


::: {.callout-note collapse="true"}
## Expand to learn quadratic complexity in Transformers

Transformers, especially their self-attention mechanism, have a quadratic computational complexity in relation to the input sequence length. This arises from each token in the sequence interacting with every other token.

For an input sequence of length \(N\), there are \(N^2\) interactions, causing the computational cost to be proportional to \(N^2\). This means that as the sequence length doubles, the computational cost quadruples. This rapid growth in computational demands becomes prohibitive for very long sequences, posing challenges in tasks where extended context is crucial.
:::

To put things in perspective and provide a clearer understanding, the following chart compares the context lengths of various models. This visualization can help elucidate the inherent limitations and possibilities of different Transformer-based models.


![Here is a comparison of the context and how much it is human terms](token_count.png)

The context length of the base model varies, with Falcon allowing up to 2K tokens and LLAMA2 accommodating up to 4K tokens, but we also have models like MPT trained with Alibi attention that can in theory support up to infinite context lengths. 

## Which parts of Transformer architecture, can we hack to extend context length?

As you may know by heart, Transformer has 4 main parts:

- Tokenization
- Embedding
- Positional encoding
- Transformer block (several of these)
- Softmax

![The architecture of a Transformer model  Source: LLM University](arch.png)

Our quest to stretch context length will zero in on the positional encoding and the attention mechanism. Given the computational heft of attention, it's worth diving deeper into Rotary Positional Encoding (RoPE).

::: {.callout-caution collapse="true"}
## Expand to learn about positional encoding

Transformers process input, not sequentially but in parallel, hence we need a way to know the order of the words, after tokening and getting the embeddings, we need to know the order of the words, and that is what positional encoding does, it adds a positional vector to each word, in order to keep track of the positions of the words. 
So if you want to give lot of context to LLM, you need to make sure that you don't essentially lose the order of the words. 

![Here is simple visualisation of positional encoding Source: LLM University](pos_emb.png)

:::

## The Linear Scaling Trick
::: {.callout-tip collapse="true"}
## TL;DR
Simple but needs minimal fine-tuning to observe the best results.
:::

Rotary position embeddings (RoPE) used by LLAMA and
Falcon are a variant of the positional encoding and they have very nice mathematical properties, but it turns out they are really bad at extrapolating, But turns out we can linearly interpolate by simply downscaling and dividing the position index by a scaling factor. 

Fun fact: this method of Linear scaling was independently and simultaneously discovered by the Reddit user /u/kaiokendev and the [Meta team](https://arxiv.org/abs/2306.15595)

![An illustration of our Position Interpolation method ... Lower left illustrates Position Interpolation where we downscale the position indices (blue and green dots) themselves from [0, 4096] to [0, 2048] to force them to reside in the pretrained range Source: Extending Context Window of Large Language Models via Positional Interpolation
](linear_interpolation.png)

## Dynamic Scaling Trick

::: {.callout-tip collapse="true"}
## TL;DR
More advanced technique, works okay without fine-tuning, but can benefit from it.
:::

Turns out Scaling the RoPE linearly is not optimal to evenly distribute information, in Fourier space as an alternative to the linear scaling trick, the dynamic scaling trick was suggested by [u/bloc97/](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/?utm_source=share&utm_medium=web2x&context=3) provides an even more efficient solution. 

a) works quite well without fine-tuningÂ 
b) doesn't degrade the performance if the model is used with short sequences
c)Â gracefully scalesÂ to longÂ sequences, under a fixedÂ parameterization


::: {.callout-warning collapse="true"}
There is significant performance drop of the above mentioned RoPe scaling methods due to static quantization, dynamic quantization should be good to go though. 
:::

## Curse of Naive Evaluation
After the rise of these tricks many in open-source community started to use them, but they were all evaluating via perplexity, while it is good starting point it is not the best way to evaluate the performance of the model 

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The first ~5 tokens in each segment contribute *most* of the loss. The shorter model has to deal with *double* the amount of early token areas!<br><br>We showed in our Shortformer paper that if you use this naÃ¯ve evaluation method, longer models will always appear to be better--&gt; <a href="https://t.co/mbHsIwrYxe">pic.twitter.com/mbHsIwrYxe</a></p>&mdash; Ofir Press (@OfirPress) <a href="https://twitter.com/OfirPress/status/1679015108675239937?ref_src=twsrc%5Etfw">July 12, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


## Do We Really Need It?

An observation from [Information Overload: The Challenges of Expanding Context Windows in Large Language Models
](https://samaya.ai/blog/):

> Current language models do not effectively use their entire context, and that retrieval is still
a crucial ingredient for effectively augmenting language models with external knowledge.  

I agree with this quote and would like to add that this behavior is typical of the Transformer-based architecture. The ML community has been paying significant "attention" to this architecture. For instance, magic.dev is not using Transformers, and they recently announced [LTM-1](https://magic.dev/blog/ltm-1), an LLM with a staggering *5,000,000* token context window. After speaking with Eric Steinberger, the CEO of magic, I am highly optimistic that their approach utilizes context more effectively than Transformers.

## Closing Thoughts
Our journey through the intricacies of extending context length in LLMs showcases the constant innovation in this space. The challenges are many, but with every hurdle, the community finds novel ways to leap forward.  