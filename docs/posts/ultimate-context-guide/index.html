<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.340">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="muhtasham">
<meta name="dcterms.date" content="2023-08-07">

<title>Machine Learners Guide to Real World - üóæ Exploring Ways to Extend Context Length in Transformers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learners Guide to Real World</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://huggingface.co/muhtasham" rel="" target=""><i class="bi bi-gpu-card" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/muhtasham" rel="" target=""><i class="bi bi-cpu" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/muhtasham9" rel="" target=""><i class="bi bi-hash" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/muhtasham/" rel="" target=""><i class="bi bi-file-earmark-person" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">üóæ Exploring Ways to Extend Context Length in Transformers</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">LLM</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>muhtasham </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 7, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#quadratic-complexity-in-transformers" id="toc-quadratic-complexity-in-transformers" class="nav-link" data-scroll-target="#quadratic-complexity-in-transformers">Quadratic Complexity in Transformers</a></li>
  <li><a href="#which-parts-of-transformer-architecture-can-we-hack-to-extend-context-length" id="toc-which-parts-of-transformer-architecture-can-we-hack-to-extend-context-length" class="nav-link" data-scroll-target="#which-parts-of-transformer-architecture-can-we-hack-to-extend-context-length">Which parts of Transformer architecture, can we hack to extend context length?</a></li>
  <li><a href="#the-linear-scaling-trick" id="toc-the-linear-scaling-trick" class="nav-link" data-scroll-target="#the-linear-scaling-trick">The Linear Scaling Trick</a></li>
  <li><a href="#dynamic-scaling-trick" id="toc-dynamic-scaling-trick" class="nav-link" data-scroll-target="#dynamic-scaling-trick">Dynamic Scaling Trick</a></li>
  <li><a href="#curse-of-naive-evaluation" id="toc-curse-of-naive-evaluation" class="nav-link" data-scroll-target="#curse-of-naive-evaluation">Curse of Naive Evaluation</a></li>
  <li><a href="#do-we-really-need-it" id="toc-do-we-really-need-it" class="nav-link" data-scroll-target="#do-we-really-need-it">Do We Really Need It?</a></li>
  <li><a href="#takeaways" id="toc-takeaways" class="nav-link" data-scroll-target="#takeaways">Takeaways</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>In this post, I aim to summarize my learnings so far and share them for you dear reader of this blog post.</p>
<p>As many might be aware, In the realm of large language models (LLMs) context length stands as a critical limitation. Training on extensive context lengths poses a significant challenge due to the quadratic complexity involved.</p>
<section id="quadratic-complexity-in-transformers" class="level2">
<h2 class="anchored" data-anchor-id="quadratic-complexity-in-transformers">Quadratic Complexity in Transformers</h2>
<p><strong>Definition:</strong><br>
Transformers, especially their self-attention mechanism, exhibit a quadratic computational complexity concerning the input sequence length. This arises due to each token in the sequence interacting with every other token, resulting in (N^2) interactions for an input sequence of length (N).</p>
<p><strong>Implications:</strong><br>
- The computational cost is proportional to (N^2). - As the sequence length doubles, the computational cost quadruples. - This rapid growth in computational demands can become prohibitive for very long sequences, making it challenging for tasks where extended context is crucial.</p>
<p>Having understood the fundamentals, let‚Äôs delve deeper into the challenges associated with quadratic complexity.</p>
<p><strong>Challenges:</strong><br>
The primary challenge lies in managing this complexity, especially when working with extensive context lengths, without compromising the model‚Äôs performance. This balance between complexity and performance remains a focal point in ongoing research in the field.</p>
<p>Transformers, especially their self-attention mechanism, have a quadratic computational complexity in relation to the input sequence length. This arises from each token in the sequence interacting with every other token.</p>
<p>For an input sequence of length (N), there are (N^2) interactions, causing the computational cost to be proportional to (N^2). This means that as the sequence length doubles, the computational cost quadruples. This rapid growth in computational demands becomes prohibitive for very long sequences, posing challenges in tasks where extended context is crucial. :::</p>
<p><strong>Figure:</strong> A visual comparison of context length in human-readable terms.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="token_count.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Comparison of context length</figcaption>
</figure>
</div>
<p>The context length of the base model varies, with Falcon allowing up to 2K tokens and LLAMA2 accommodating up to 4K tokens, but we also have models like MPT trained with Alibi attention that can in theory support up to infinite context lengths.</p>
</section>
<section id="which-parts-of-transformer-architecture-can-we-hack-to-extend-context-length" class="level2">
<h2 class="anchored" data-anchor-id="which-parts-of-transformer-architecture-can-we-hack-to-extend-context-length">Which parts of Transformer architecture, can we hack to extend context length?</h2>
<p>Transformers serve as the backbone of many state-of-the-art NLP model Transformer has 4 main parts:</p>
<ul>
<li>Tokenization</li>
<li>Embedding</li>
<li>Positional encoding</li>
<li>Transformer block (several of these)</li>
<li>Softmax</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="arch.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The architecture of a Transformer model Source: LLM University</figcaption>
</figure>
</div>
<p>Typically, to extend the context length, the context length we need to hack the positional encoding or/and attention mechanism, but given the fact that the attention mechanism is the most computationally expensive part of the Transformer, and there is lot of attention (pun intened) to open-acess models like LLAMA2 and Falcon lets try to better understand attention to the positional encoding, specifically in the context of Rotatry Positional Encoding (RoPE)</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expand To Learn About positional encoding?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Transformers process input, Not sequentially but in parallel, hence need a way to know the order of the words So if you want to give lot of context to LLM, you need to make sure that&nbsp;you don‚Äôt essentially lose the order of the words.</p>
<p>So this part just adds a positional vector to each word, in order to keep track of the positions of the words.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="pos_emb.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Here is simple visualisation of positional encoding Source: LLM University</figcaption>
</figure>
</div>
<p>Now that we know we have a unique vector corresponding to the sentence, and that this vector carries the information on all the words in the sentence and their order.</p>
</div>
</div>
</div>
</section>
<section id="the-linear-scaling-trick" class="level2">
<h2 class="anchored" data-anchor-id="the-linear-scaling-trick">The Linear Scaling Trick</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR
</div>
</div>
<div class="callout-body-container callout-body">
<p>Simple but needs minimal fine-tuning to observe the best results.</p>
</div>
</div>
<p>Rotary position embeddings (RoPE) used by LLAMA and Falcon are a variant of the positional encoding and they have very.nice mathematical properties, but it turns out they are realaly bad at extrapolating, But turns out we can linearly interpolate by simply Downscaling and divide the position index by a scaling factor. Adn this method of Linear scaling was independently and simultaneously discovered by the Reddit user /u/kaiokendev and the <a href="https://arxiv.org/abs/2306.15595">Meta team</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="linear_interpolation.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">An illustration of our Position Interpolation method. Consider a Llama model pre-trained with a 2048 context window length. Upper left illustrates the normal usage of an LLM model: input position indices (blue dots) are within the pre-trained range. Upper right illustrates length extrapolation where models are required to operate unseen positions (red dots) up to 4096. Lower left illustrates Position Interpolation where we downscale the position indices (blue and green dots) themselves from [0, 4096] to [0, 2048] to force them to reside in the pretrained range</figcaption>
</figure>
</div>
</section>
<section id="dynamic-scaling-trick" class="level2">
<h2 class="anchored" data-anchor-id="dynamic-scaling-trick">Dynamic Scaling Trick</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR
</div>
</div>
<div class="callout-body-container callout-body">
<p>More advanced technique, works okay without fine-tuning, but can benefit from it.</p>
</div>
</div>
<p>Turns out Scaling the&nbsp;RoPE&nbsp;linearly&nbsp;is not&nbsp;optimal to evenly distribute&nbsp;information, In Fourier space</p>
<p>As an alternative to the linear scaling trick, the dynamic scaling trick wasm suggeste by <a href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/?utm_source=share&amp;utm_medium=web2x&amp;context=3">u//bloc97/</a> provides an even more efficient solution. Basically if you apply Neural Tangent Kernel (NTK) theory to this problem, it becomes clear that simply interpolating the RoPE‚Äôs fourier space ‚Äúlinearly‚Äù is very sub-optimal, as it prevents the network to distinguish the order and positions of tokens that are very close by. Borrowing from NTK literature, scaling down the fourier features too much will eventually even prevent succesful finetunes (this is corroborated by the recent paper by Meta that suggests an upper bound of ~600x)</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>There is Significant performance drop of RoPe Scaling methods due to static quantization, dynamic quantization should be good to go</p>
</div>
</div>
</section>
<section id="curse-of-naive-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="curse-of-naive-evaluation">Curse of Naive Evaluation</h2>
<p>After rise of this tricks many in open-source community started to use them, but they were all evaluting via perplixity, while it is good strating point it is not the best way to evaluate the performance of the model, but</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
The first ~5 tokens in each segment contribute <em>most</em> of the loss. The shorter model has to deal with <em>double</em> the amount of early token areas!<br><br>We showed in our Shortformer paper that if you use this na√Øve evaluation method, longer models will always appear to be better‚Äì&gt; <a href="https://t.co/mbHsIwrYxe">pic.twitter.com/mbHsIwrYxe</a>
</p>
‚Äî Ofir Press (<span class="citation" data-cites="OfirPress">@OfirPress</span>) <a href="https://twitter.com/OfirPress/status/1679015108675239937?ref_src=twsrc%5Etfw">July 12, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</section>
<section id="do-we-really-need-it" class="level2">
<h2 class="anchored" data-anchor-id="do-we-really-need-it">Do We Really Need It?</h2>
<blockquote class="blockquote">
<p>Current language models do not effectively use their entire context, and that retrieval is still a crucial ingredient for effectively augmenting language models with external knowledge.<br>
from [Information Overload: The Challenges of Expanding Context Windows in Large Language Models]https://samaya.ai/blog/</p>
</blockquote>
<p>I would agree to this quote, and add that this seems to be behaviur of Transformer based architecture, the ML community have really being paying ‚Äúattention‚Äù to this architecture, magic.dev is doing for example, they are not using Transformers and they recently announced LTM-1: an LLM with a 5,000,000 token context window, which is way above, but being an undiscoloed architecture i am highly hopeful and optimistic that it utilizes the context better than Transformers, especially after speaking to the ceo of magic Eric Steinberger I was coninced they are cooking something really good.</p>
</section>
<section id="takeaways" class="level2">
<h2 class="anchored" data-anchor-id="takeaways">Takeaways</h2>
<p>Navigating the realm of Transformers, we‚Äôve unveiled both their potential and the challenges they present, especially concerning context length. While the quadratic complexity poses significant hurdles, our exploration shows that there are strategies and techniques to mitigate these challenges. As we push the boundaries of what‚Äôs possible, it‚Äôs essential to remember that every challenge in technology is an invitation to innovate. And as we innovate, we not only solve problems but also discover new horizons.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>