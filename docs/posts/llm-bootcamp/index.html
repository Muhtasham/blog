<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="muhtasham">
<meta name="dcterms.date" content="2023-04-26">

<title>🌉 A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications – Machine Learners Guide to Real World</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>!function(d,src,site,collect,hello){if(d.AR)return;var s=d.createElement("script");s.async=1;s.src=src;s.dataset.siteId=site;s.dataset.collectUrl=collect;s.dataset.handshakeMessage=hello;d.head.appendChild(s);}(document,"https://de-stealthy.fly.dev/cdn/pixel.js","SITE_3JHWW417","https://de-stealthy.fly.dev/collect","AcctRev pixel ready for SITE_3JHWW417.");</script>
<noscript></noscript><meta name="twitter:title" content="🌉 A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications – Machine Learners Guide to Real World">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="sf.jpg">
<meta name="twitter:creator" content="muhtasham9">
<meta name="twitter:card" content="summary">
</head><body class="nav-fixed fullcontent quarto-light"><img src="https://de-stealthy.fly.dev/p.gif?s=SITE_3JHWW417&amp;e=page_view" width="1" height="1" alt="">


<link rel="stylesheet" href="../../styles.css">




<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learners Guide to Real World</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://huggingface.co/muhtasham"> <i class="bi bi-gpu-card" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/muhtasham"> <i class="bi bi-cpu" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/muhtasham9"> <i class="bi bi-hash" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/muhtasham/"> <i class="bi bi-file-earmark-person" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">🌉 A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">llm</div>
                <div class="quarto-category">deep-learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>muhtasham </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 26, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sf.jpg" class="img-fluid figure-img"></p>
<figcaption>The view of the Golden Gate Bridge reminded me of the limitless possibilities that AI and LLMs can bring to our world. Image by author</figcaption>
</figure>
</div>
<section id="table-of-contents" class="level2">
<h2 class="anchored" data-anchor-id="table-of-contents">Table of Contents</h2>
<ol type="1">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#rapid-transformation">The Rapid Transformation of AI-Powered Apps</a></li>
<li><a href="#critical-questions">Addressing the Critical Questions</a>
<ul>
<li><a href="#open-source-llms">Are there any good open-source LLMs?</a></li>
<li><a href="#moat-openai-apis">What is my moat if I rely on OpenAI APIs?</a></li>
<li><a href="#prompt-engineering">Is Prompt Engineering some kind of sick joke?</a></li>
<li><a href="#user-feedback">How can I gather and use feedback from users?</a></li>
<li><a href="#transformer-from-scratch">Should I be able to code a Transformer from scratch?</a></li>
<li><a href="#test-llms">How exactly am I supposed to test these damn things?</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p><a name="introduction"></a></p>
<p>The world of technology is currently undergoing a monumental transformation, and my participation in the <a href="https://fullstackdeeplearning.com/llm-bootcamp/">LLM Bootcamp</a> was nothing short of enlightening from my perspective. The potential of AI-powered applications is causing a ripple effect in the software industry. I learned about techniques, tools, and vendors shaping the future of AI at a well-organized bootcamp where I gained valuable insights and connections.</p>
<p>While it’s nearly impossible to encapsulate everything I learned in a single blog post, I will attempt to highlight some of the key takeaways from this enriching experience.</p>
</section>
<section id="the-rapid-transformation-of-ai-powered-apps" class="level2">
<h2 class="anchored" data-anchor-id="the-rapid-transformation-of-ai-powered-apps">The Rapid Transformation of AI-Powered Apps</h2>
<p><a name="rapid-transformation"></a></p>
<p>Before the advent of Large Language Models (LLMs), ideas often bottlenecked on the process of training models from scratch and faced further bottlenecks in scalable deployment. However, with the availability of pretrained, promptable LLM models and APIs, it is now possible to <strong>configure and serve users in just an hour</strong>. This shift has led to an entirely new ecosystem that even experienced ML professionals are striving to understand.</p>
</section>
<section id="addressing-the-critical-questions" class="level2">
<h2 class="anchored" data-anchor-id="addressing-the-critical-questions">Addressing the Critical Questions</h2>
<p><a name="critical-questions"></a></p>
<p>As engineers delve into this new world, several questions arise. During the LLM Bootcamp, I explored answers to some of these pressing questions:</p>
<section id="are-there-any-good-open-source-llms" class="level3">
<h3 class="anchored" data-anchor-id="are-there-any-good-open-source-llms">Are there any good open-source LLMs?</h3>
<p><a name="open-source-llms"></a></p>
<p>Yes, there are several open-source LLMs available. These models serve as a foundation for researchers and developers to build upon, experiment, and create new AI applications.</p>
<p><a href="https://www.linkedin.com/in/josh-tobin-4b3b10a9/">Josh Tobin</a> had great slides on this topic, which will be release soon, but in essence it is more detailed version of the below figure</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="llm-comparison.jpg" class="img-fluid figure-img"></p>
<figcaption>Comparisons of LLMs from <a href="https://lightning.ai/pages/community/community-discussions/the-ultimate-battle-of-language-models-lit-llama-vs-gpt3.5-vs-bloom-vs/">LightningAI</a></figcaption>
</figure>
</div>
<p>Selecting the best model for your use case depends on various factors, including:</p>
<ul>
<li>Out-of-the-box quality for your task</li>
<li>Inference speed / latency</li>
<li>Cost</li>
<li>Fine-tuneability / extensibility</li>
<li>Data security and license permissibility</li>
<li>For most use cases, GPT-4 is an excellent starting point.</li>
</ul>
<p><strong>Proprietary models</strong> often offer <strong>superior quality</strong> and are more suitable for commercial use due to licensing constraints. These models can provide businesses with a competitive edge, as they are specifically designed and optimized for certain tasks and industries. Also, they come with the advantage of <strong>reduced infrastructure overhead</strong> when compared to serving open-source models.</p>
<p>In the past, concerns regarding broad copyright ownership over any output generated by these models have made businesses hesitant to adopt them. However, advancements in technology, such as the Azure OpenAI Service, have begun to address these concerns. OpenAI now empowers users by offering services that enable the use of proprietary models without worrying about copyright issues. This makes it easier for businesses to leverage the benefits of proprietary models while also protecting their intellectual property.</p>
<p><strong>Some disclaimers on this topic:</strong></p>
<ul>
<li>Security concerns: While the Azure OpenAI Service has made strides in alleviating copyright concerns, it’s important to acknowledge that <strong>security concerns still remain</strong>. As with any cloud-based service, there is always the potential risk of data breaches and unauthorized access to sensitive information. To mitigate these risks, OpenAI and Azure have implemented rigorous security measures and protocols, such as data encryption and access control. However, <strong>businesses should still exercise caution</strong> and perform their own <strong>due diligence</strong> when evaluating the suitability of these services for their needs.</li>
<li>No Service Level Agreements (SLAs) yet: There is currently no commitment from OpenAI regarding when SLAs will be provided for these APIs. SLAs typically offer guarantees on service availability, performance, and support response times, which can be crucial for businesses that depend on these services for mission-critical applications. Without SLAs in place, businesses may find it <strong>challenging to assess the reliability</strong> and stability of these APIs, making it difficult to plan and budget for their integration into commercial projects.</li>
<li>Upcoming regulations: Additionally, it’s worth noting that new regulations are on the horizon that could impact the use of proprietary models in certain regions. The European Union is currently working on the Artificial Intelligence Act (AI Act), which aims to create a legal framework for AI systems, including proprietary models. This legislation may introduce new requirements for businesses using AI, such as transparency, accountability, and data protection. As these regulations evolve, businesses should <strong>stay informed and adapt</strong> their AI strategies accordingly.</li>
</ul>
<p>On the other hand, <strong>open-source models</strong> are a more fitting choice if you require extensive customization and want to ensure data security. They provide <strong>greater flexibility</strong> in terms of modification and adaptation to meet specific project requirements.</p>
<p>Adding to the previous discussion on selecting the best LLM for your use case, here are some recommendations for most projects:</p>
<ul>
<li>Start with <a href="https://openai.com/product/gpt-4">GPT-4</a>: This will help you develop a <strong>proof of concept</strong> to assess the feasibility of your task, similar to prototyping in Python.</li>
<li>Consider “downsizing” if cost or latency is a factor: <a href="https://platform.openai.com/docs/models/gpt-3-5">GPT-3.5</a> and <a href="https://www.anthropic.com/index/introducing-claude">Claude</a> are both good choices with comparable performance. For even <strong>faster and cheaper</strong> alternatives, explore options provided by any LLM vendor, with <a href="https://www.anthropic.com">Anthropic’s</a> offering being the most “modern” choice.</li>
<li>Opt for <a href="https://cohere.com/">Cohere</a> if you need to <strong>fine-tune your model</strong>: Cohere allows greater flexibility in adjusting the model to better suit your specific needs.</li>
<li>Use open-source models only when absolutely necessary: Although the open-source landscape is evolving rapidly and will likely become a more viable option in the future, for now, it’s advisable to use these models <strong>only if they are essential for your project.</strong></li>
</ul>
<p><em><em>Personally I find it quite distressing that LLAMA paper results could not be <a href="https://twitter.com/abacaj/status/1652066990033362944?s=20">reproduced</a>.</em></em></p>
<p>By considering these recommendations, you can make an informed decision about which LLM is most suitable for your project, balancing factors such as cost, performance, customization, and data security.</p>
<p><em><em>We can expect GPT-3.5 <strong>quality</strong> to be available in open-source models until the end of 2023. </em></em></p>
</section>
<section id="what-is-my-moat-if-i-rely-on-openai-apis" class="level3">
<h3 class="anchored" data-anchor-id="what-is-my-moat-if-i-rely-on-openai-apis">What is my moat if I rely on OpenAI APIs?</h3>
<p><a name="moat-openai-apis"></a></p>
<p>Utilizing OpenAI APIs provides you with access to state-of-the-art AI technology and regular model updates, which can help you maintain a competitive edge in the market. However, your competitive advantage, or “moat” comes from your unique implementation of the technology and the value-added services you deliver to your customers.</p>
<p>For instance context plays a crucial role in providing LLMs with unique and up-to-date information, but its capacity is limited. To make the most of this limited context, consider augmenting the language model through various methods:</p>
<ul>
<li><strong>Augment with a larger corpus</strong>: Leverage additional data sources to enrich the context and improve the AI’s understanding of your specific domain or use case.</li>
<li><strong>Augment with more LLM calls</strong>: Make multiple calls to the LLM with different inputs or parameters to generate a diverse range of responses, which can then be combined or refined to produce a more accurate output.</li>
<li><strong>Augment with external sources</strong>: Integrate information from other sources, such as databases, APIs, or domain-specific knowledge bases, to enhance the context and further tailor the AI’s output to your needs.</li>
</ul>
<p>By effectively combining OpenAI APIs with these augmentation strategies, you can create unique, high-value solutions that differentiate your offerings from competitors and strengthen your competitive advantage.</p>
</section>
<section id="is-prompt-engineering-some-kind-of-sick-joke" class="level3">
<h3 class="anchored" data-anchor-id="is-prompt-engineering-some-kind-of-sick-joke">Is Prompt Engineering some kind of sick joke?</h3>
<p><a name="prompt-engineering"></a></p>
<p>In the context of AI and LLMs, a “prompt” refers to the text input given to a language model. “Prompt engineering” is the skillful process of crafting that input text to achieve desired results from the model.</p>
<p>Lilian Weng has a great blogpost about it <a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#automatic-prompt-design">here</a>, and she notes that most papers on prompt engineering are tricks that can be explained in a few sentences.</p>
<p>While the term “prompt engineering” may sound whimsical or perplexing, it is a crucial aspect of working with LLMs. By carefully designing prompts, you can harness the power of language models to <strong>generate valuable insights</strong>, creative ideas, and solutions to complex problems.</p>
<p><em><em>Prompts can be thought of as gateways to the vast knowledge and capabilities of AI language models.</em></em></p>
<p>They allow you to tap into the potential of these sophisticated systems, but only when you adhere to certain guidelines and best practices.</p>
<p>Despite the seemingly mysterious nature of prompts, they are an essential tool in the AI practitioner’s toolkit, and learning how to master prompt engineering will enable you to unlock the full potential of LLMs in various applications.</p>
</section>
<section id="how-can-i-gather-and-use-feedback-from-users" class="level3">
<h3 class="anchored" data-anchor-id="how-can-i-gather-and-use-feedback-from-users">How can I gather and use feedback from users?</h3>
<p><a name="user-feedback"></a></p>
<p>User feedback is crucial for improving your AI-powered applications. To gather and use feedback effectively, consider implementing feedback loops, in-app surveys, and user testing to gain insights into user preferences and any potential shortcomings of the AI.</p>
<p>One approach to incorporating user feedback is to ask an LLM whether the new answer addresses the feedback provided for the old answer. Aim for low-friction, high-signal feedback methods that easily integrate into the user’s workflow, such as the <strong>Accept changes</strong> or <strong>Thumbs up/down</strong> patterns. Longer-form feedback also plays a role in refining AI performance.</p>
<p>Identify themes in user feedback that the model does not address, which are often discovered by humans. Adjust the prompt to account for these themes through prompt engineering or by changing the context. The automation of this process is still an open question.</p>
</section>
<section id="should-i-be-able-to-code-a-transformer-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="should-i-be-able-to-code-a-transformer-from-scratch">Should I be able to code a Transformer from scratch?</h3>
<p><a name="transformer-from-scratch"></a></p>
<p>While it’s not mandatory, having a deep understanding of the underlying architecture, such as Transformers, can help you better utilize LLMs and troubleshoot issues. However, focusing on practical applications and use cases is often more valuable than delving solely into theoretical aspects.</p>
<p>For example, during a fireside chat, <a href="linkedin.com/in/welinder/">Peter Welinder</a>, VP of Product and Partnerships at OpenAI, mentioned that when they first built the API, the inference was slow, but in just three months, they were able to improve the inference speed by <strong>100x</strong>. This example illustrates how understanding the underlying architecture, combined with using tools like <a href="https://github.com/openai/triton">triton</a>, can help you enhance the performance of your AI applications.</p>
<p>For instance, you might need to fine-tune LLMs in certain situations. Here are some recommendations:</p>
<ul>
<li><p>Using GPT-4 might eliminate the need for fine-tuning in most cases.</p></li>
<li><p>Reasons to consider fine-tuning:</p>
<ul>
<li>You need to deploy smaller models due to resource constraints.</li>
<li>You have a large amount of data, and retrieval-based approaches are not performing well.</li>
</ul></li>
<li><p>Low-rank updates or parameter-efficient tuning techniques can make fine-tuning more accessible, allowing you to optimize the LLM for specific use cases without in-depth knowledge of Transformer coding.</p></li>
</ul>
<p>Also this is a there is recent great survey on <a href="http://arxiv.org/abs/2303.15647">parameter-efficient tuning</a>.</p>
<p>Ultimately, striking a <strong>balance</strong> between understanding the underlying principles and focusing on practical applications will empower you to make the most of LLMs in your projects.</p>
</section>
<section id="how-exactly-am-i-supposed-to-test-these-damn-things" class="level3">
<h3 class="anchored" data-anchor-id="how-exactly-am-i-supposed-to-test-these-damn-things">How exactly am I supposed to test these damn things?</h3>
<p><a name="test-llms"></a></p>
<p>Testing LLMs can be challenging, but it is essential to ensure the quality of your AI applications. Employ a combination of manual and automated testing, focus on edge cases, and collaborate with domain experts to validate the AI’s output for accuracy and relevance.</p>
<p>As you may already know, LLMs are prone to making mistakes, and improving one aspect of your model may inadvertently compromise another. If people rely on your model, they trust you to maintain performance on their task. Since LLMs are trained on the internet, there is always a risk of drift, and qualitative success can be hard to measure. Additionally, diversity of behaviors means aggregate metrics may not be sufficient.</p>
<p>Test coverage and distribution shift are closely related concepts. Distribution shift measures how far the test distribution is from a reference distribution and is used to assess data changes. Test coverage measures how well your evaluation set covers your production data and is used to find more helpful evaluation data.</p>
<p>A key idea in LLM testing is <strong>using one LLM to evaluate another</strong>. This enables automatic evaluation, which can unlock parallel experimentation. However, you should still conduct some manual checks. Types of feedback to consider include thumbs up/down, written feedback, and corrected answers.</p>
<p>By implementing a comprehensive testing strategy that combines automated and manual testing, as well as incorporating feedback mechanisms, you can ensure the quality and performance of your AI applications remain high.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p><a name="conclusion"></a></p>
<p>While the insights shared in this blog post are valuable today, it’s essential to acknowledge that the landscape is continually changing. And as we continue to witness the growth and impact of LLMs and AI in general, it’s crucial to remain agile and open to new ideas. The key takeaways from the LLM Bootcamp and related discussions serve as a stepping stone for understanding the current state of the field, but it’s up to you to keep pushing the boundaries and exploring new ways to harness the power of these transformative technologies.</p>
<p>In addition to this post, I have also compiled a separate write-up on a panel discussion focused on <a href="https://muhtasham.github.io/blog/posts/vc-fireside-chat/">Building a Sustainable Business</a> which provides insights on how to navigate the challenges and opportunities in this burgeoning industry.</p>
<p>In conclusion, I want to express my heartfelt thanks to all the readers who have taken the time to read and engage with this blog post. Your interest and curiosity motivate me to keep sharing my experiences and learnings in this rapidly evolving field. I appreciate your support, and I look forward to sharing more insights with you in the future!</p>
</section>
<section id="acknowledgements" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgements">Acknowledgements</h2>
<p>Thanks to <a href="https://twitter.com/eugeneyan">Eugene Yan</a> for comments on the draft.</p>
</section>
<section id="citation" class="level2">
<h2 class="anchored" data-anchor-id="citation">Citation</h2>
<p>Cited as:</p>
<pre><code>@misc{muhtasham2023llm,
  title={A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications},
  author={Muhtasham, Oblokulov},
  journal={muhtasham.github.io},
  year={2023},
  month={Apr},
  url={https://muhtasham.github.io/blog/posts/llm-bootcamp/}
}</code></pre>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>