<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="muhtasham">
<meta name="dcterms.date" content="2023-10-22">

<title>2Ô∏è‚É£ Concepts from Operating Systems That Found Their Way in LLMs ‚Äì Machine Learners Guide to Real World</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>!function(d,src,site,collect,hello){if(d.AR)return;var s=d.createElement("script");s.async=1;s.src=src;s.dataset.siteId=site;s.dataset.collectUrl=collect;s.dataset.handshakeMessage=hello;d.head.appendChild(s);}(document,"https://de-stealthy.fly.dev/cdn/pixel.js","SITE_3JHWW417","https://de-stealthy.fly.dev/collect","AcctRev pixel ready for SITE_3JHWW417.");</script>
<noscript></noscript><meta name="twitter:title" content="2Ô∏è‚É£ Concepts from Operating Systems That Found Their Way in LLMs ‚Äì Machine Learners Guide to Real World">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="punchcard.jpeg">
<meta name="twitter:creator" content="muhtasham9">
<meta name="twitter:card" content="summary">
</head><body class="nav-fixed quarto-light"><img src="https://de-stealthy.fly.dev/p.gif?s=SITE_3JHWW417&amp;e=page_view" width="1" height="1" alt="">


<link rel="stylesheet" href="../../styles.css">




<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learners Guide to Real World</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://huggingface.co/muhtasham"> <i class="bi bi-gpu-card" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/muhtasham"> <i class="bi bi-cpu" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/muhtasham9"> <i class="bi bi-hash" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/muhtasham/"> <i class="bi bi-file-earmark-person" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">2Ô∏è‚É£ Concepts from Operating Systems That Found Their Way in LLMs</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">llm</div>
                <div class="quarto-category">papers</div>
                <div class="quarto-category">security</div>
                <div class="quarto-category">tokenizer</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>muhtasham </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 22, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">January 17, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#branch-prediction-in-cpus" id="toc-branch-prediction-in-cpus" class="nav-link" data-scroll-target="#branch-prediction-in-cpus">Branch Prediction in CPUs</a>
  <ul class="collapse">
  <li><a href="#traditional-use-in-cpus" id="toc-traditional-use-in-cpus" class="nav-link" data-scroll-target="#traditional-use-in-cpus">Traditional Use in CPUs</a></li>
  <li><a href="#how-llms-use-this-idea" id="toc-how-llms-use-this-idea" class="nav-link" data-scroll-target="#how-llms-use-this-idea">How LLMs Use This Idea</a></li>
  </ul></li>
  <li><a href="#virtual-memory-paging" id="toc-virtual-memory-paging" class="nav-link" data-scroll-target="#virtual-memory-paging">Virtual Memory Paging</a>
  <ul class="collapse">
  <li><a href="#traditional-use-in-computers" id="toc-traditional-use-in-computers" class="nav-link" data-scroll-target="#traditional-use-in-computers">Traditional Use in Computers</a></li>
  <li><a href="#how-llms-use-this-idea-1" id="toc-how-llms-use-this-idea-1" class="nav-link" data-scroll-target="#how-llms-use-this-idea-1">How LLMs Use This Idea</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#top-recommendation" id="toc-top-recommendation" class="nav-link" data-scroll-target="#top-recommendation">Top recommendation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="punchcard.jpeg" class="img-fluid figure-img"></p>
<figcaption>This IBM punched card, an early form of data encoding, reminds us that at the core, computers don‚Äôt see strings or words but numerical values. A nod to the origins, it‚Äôs the OG tokenizer that set the stage for today‚Äôs intricate LLMs Image by Author</figcaption>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Diving into the intricacies of technology often uncovers unexpected parallels. Recently, I‚Äôve been struck by how foundational computer operating system concepts are making waves in the domain of Large Language Models (LLMs), especially the transformer variants. I will try to spotlight two concepts that I observed so far: Branch Prediction and Virtual Memory Paging.</p>
</section>
<section id="branch-prediction-in-cpus" class="level2">
<h2 class="anchored" data-anchor-id="branch-prediction-in-cpus">Branch Prediction in CPUs</h2>
<section id="traditional-use-in-cpus" class="level3">
<h3 class="anchored" data-anchor-id="traditional-use-in-cpus">Traditional Use in CPUs</h3>
<p>Branch prediction is a tool used in computers to speed up how they work. When a computer has to choose between two sets of instructions, it tries to guess which set will be used next. Here‚Äôs a breakdown:</p>
<ol type="1">
<li><strong>Prediction:</strong> The computer makes a guess about the next set of instructions.</li>
<li><strong>Speculative Sampling:</strong> Based on that guess, the computer starts working on those instructions, even if it‚Äôs not sure it‚Äôs the right choice.</li>
<li><strong>Correction:</strong> If the guess is right, the computer keeps going. If it‚Äôs wrong, the computer starts over and chooses the other set.</li>
</ol>
</section>
<section id="how-llms-use-this-idea" class="level3">
<h3 class="anchored" data-anchor-id="how-llms-use-this-idea">How LLMs Use This Idea</h3>
<p>Google Deepmind used similar idea to make LLMs faster in <a href="https://arxiv.org/abs/2302.01318">Accelerating Large Language Model Decoding with Speculative Sampling</a>. Their algorithm uses a smaller draft model to make initial guesses and a larger primary model to validate them. If the draft often guesses right, operations become faster, reducing latency.</p>
<p>There are some people speculating that OpenAI might be using speculative decoding on <a href="https://news.ycombinator.com/item?id=36710170">GPT-4‚Äôs inference</a>. If you‚Äôre interested in a visual explanation, an insightful visualization on <a href="https://huggingface.co/blog/assisted-generation#greedy-decoding-with-assisted-generation">speculative decoding/assisted generation</a> is worth checking out.</p>
<p>Moreover, there‚Äôs an intriguing recent paper, <a href="https://arxiv.org/abs/2310.07177">Online Speculative Decoding (OSD)</a>, that takes this concept to another level. The essence of OSD is to continually refine (multiple) draft model(s) based on observed user query data, leveraging the spare computational resources in LLM serving clusters. This approach is especially useful when the draft model is not as accurate as the target model, as it can be continually refined to improve its predictive accuracy.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Caution</span>Potential Security Vulnerabilities in LLMs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>As neural network models, especially Large Language Models (LLMs), become pivotal in various applications, understanding and addressing their security vulnerabilities is crucial.</p>
<section id="stealing-the-decoding-algorithms-of-language-models" class="level3">
<h3 class="anchored" data-anchor-id="stealing-the-decoding-algorithms-of-language-models">Stealing the Decoding Algorithms of Language Models</h3>
<p>A key component of generating text from modern LLMs is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners.</p>
<p>In <a href="https://people.cs.umass.edu/~amir/papers/CCS23-LM-stealing.pdf">this</a> work, authors show, that an adversary with typical API access to an LLM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs.</p>
<p><strong>Spectre and Its Implications for LLMs</strong></p>
<p><a href="https://en.wikipedia.org/wiki/Spectre_(security_vulnerability)">Spectre</a> is one of the key transient execution CPU vulnerabilities, which involves timing side-channel attacks affecting modern microprocessors. The speculative execution in these processors might expose private data through observable side effects. While LLMs operate based on speculative principles similar in concept, it‚Äôs important to note that their speculative nature is fundamentally different. However, given that neural networks can be susceptible to timing side-channel attacks especially when LLMs interface with runtime environments capable of executing code, there is a potential for such attacks to be exploited.</p>
<p><strong>The Risk of LLMs Interacting with Code Interpreters</strong></p>
<p>A significant concern arises when LLMs have access to environments capable of running code, such as the <a href="https://chat.openai.com/?model=gpt-4-code-interpreter">Code Interpreter</a>. In such scenarios, vulnerabilities could be exploited to make the LLMs run malicious code, posing even more significant security threats. It‚Äôs important to exercise caution and ensure secure barriers when deploying LLMs in such settings.</p>
<p><strong>Safeguarding Model Weights</strong></p>
<p>Weights of machine learning models are often stored in formats, such as pickle, that are vulnerable to security breaches. An alternative to consider is ‚Äú<a href="https://huggingface.co/docs/safetensors/index">Safetensors</a>‚Äù ‚Äî a more secure format ensuring the safety of tensor data. Not only is it a secure choice, but Safetensors also has impressive speed.</p>
<p><strong>LLMs: Deep Neural Networks Behind the Scenes</strong></p>
<p>It‚Äôs important to remember that LLMs are deep neural networks (DNNs) at their core. This association brings inherent security concerns, as highlighted by these research papers:</p>
<ol type="1">
<li><strong><a href="https://arxiv.org/abs/1812.11720">Stealing Neural Networks via Timing Side Channels</a>:</strong> This paper underscores the susceptibility of Neural Networks to timing side channel attacks and proposes a black-box Neural Network extraction technique.</li>
<li><strong><a href="https://ieeexplore.ieee.org/document/8835365">Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks</a>:</strong> This research emphasizes the opaque nature of DNNs, making them prone to backdoor attacks. It introduces methods to detect and counter such hidden threats.</li>
</ol>
<p><strong>Learning from the Past: Avoiding the Mistakes of Operating Systems</strong></p>
<p>As we explore more about Large Language Models (LLMs) and how they can be used, it‚Äôs clear that they share some similarities with operating systems. Just like operating systems had certain issues and vulnerabilities, LLMs could have them too. It‚Äôs important for us to learn from those past mistakes to make sure we don‚Äôt run into similar problems, like the security issues seen with Spectre, a vulnerability found in many modern microprocessors.</p>
</section>
</div>
</div>
</div>
</section>
</section>
<section id="virtual-memory-paging" class="level2">
<h2 class="anchored" data-anchor-id="virtual-memory-paging">Virtual Memory Paging</h2>
<section id="traditional-use-in-computers" class="level3">
<h3 class="anchored" data-anchor-id="traditional-use-in-computers">Traditional Use in Computers</h3>
<p>Virtual memory is an abstraction provided by the operating system that makes it seem to an application as if it has access to more RAM than is physically available. When the actual RAM gets filled up, the operating system uses a portion of the computer‚Äôs storage space (typically the hard drive) as an extension of RAM. This process enables the computer to handle more tasks concurrently by mapping the application‚Äôs memory addresses to actual physical locations, which could be in RAM, on the hard disk, or even other storage mediums.</p>
<p>In essence, virtual memory gives applications the illusion they‚Äôre utilizing a large, contiguous chunk of RAM, even though the reality behind the scenes might be quite different.</p>
</section>
<section id="how-llms-use-this-idea-1" class="level3">
<h3 class="anchored" data-anchor-id="how-llms-use-this-idea-1">How LLMs Use This Idea</h3>
<p>Transformers, especially LLMs, feature a mechanism called ‚ÄúKV cache,‚Äù similar to RAM, that temporarily stores key-value pairs during attention operations for quick access. To efficiently handle longer sequences that don‚Äôt fit in memory, they could potentially adopt techniques inspired by virtual memory paging.</p>
<section id="vllm-virtual-paging-for-kv-cache" class="level4">
<h4 class="anchored" data-anchor-id="vllm-virtual-paging-for-kv-cache">vLLM: virtual paging for KV cache</h4>
<p>Researchers from UC Berkeley introduced this idea in a study called <a href="https://arxiv.org/abs/2309.06180">Efficient Memory Management for Large Language Model Serving with PagedAttention</a> also dubbed as vLLM.</p>
<p>The heart of vLLM is PagedAttention. It‚Äôs a fresh take on how attention works in transformers, borrowing from the paging idea in computer OS. Remarkably, without changing the original model, PagedAttention allows batching up to 5x more sequences. This means better use of GPU resources and faster operations.</p>
<p>Also here‚Äôs a rapid breakdown of some crucial state of the art LLM serving techniques as of Oct 2023:</p>
<ol type="1">
<li><strong>Continuous Batching:</strong> Increases throughput by allowing requests to immediately jump onto an ongoing GPU batch, minimizing wait time.</li>
<li><strong>PagedAttention:</strong> Much like OS‚Äôs virtual paging but tailored for KV cache in LLMs, allowing 3x more simultaneous requests and thereby tripling throughput.</li>
<li><strong>Speculative Decoding:</strong> Uses a smaller draft model to make initial guesses and a larger primary model to validate them. If the draft often guesses right, operations become faster, reducing latency, like we described in the previous section.</li>
</ol>
</section>
<section id="memgpt-virtually-extending-llms-limited-context-windows" class="level4">
<h4 class="anchored" data-anchor-id="memgpt-virtually-extending-llms-limited-context-windows">MemGPT: ‚ÄúVirtually‚Äù Extending LLM‚Äôs Limited Context Windows</h4>
<p><a href="https://arxiv.org/abs/2310.08560">MemGPT: Towards LLMs as Operating Systems</a> also from UC Berkley is a new way to help LLMs like GPT-4 remember more information. Think of it as adding an extra brain to the LLM. This extra brain has two parts:</p>
<ol type="1">
<li><strong>Internal Memory (LLM RAM):</strong> A small space where the LLM keeps important information.</li>
<li><strong>External Memory (LLM HDD):</strong> A much larger space where the LLM can store and retrieve data when needed.</li>
</ol>
<p>When the LLM needs data from the external memory, it breaks it into smaller pieces that fit into the internal memory. This lets LLMs handle big tasks that need lots of information.</p>
<p>MemGPT makes it easier to use LLMs for tasks that need a lot of memory. With this tool, we don‚Äôt have to worry about the LLM running out of space.</p>
<p>In the realm of LLMs, context and memory are kind of like the foundational, ‚ÄúRAM‚Äù of our era. Andrej Karpathy has already made this comparison:</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
The analogy between GPTs of today to the CPUs of early days of computing are interesting. GPT is a funny kind of programmable text computer. Have to think through it more ü§î but e.g.:<br><br>## Memory<br>GPT-4 RAM is ~log2(50K vocab size)*(32K context length)/(8 bits/byte) ~= 64kB,‚Ä¶
</p>
‚Äî Andrej Karpathy (<span class="citation" data-cites="karpathy">@karpathy</span>) <a href="https://twitter.com/karpathy/status/1644183721405464576?ref_src=twsrc%5Etfw">April 7, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Ideas and strategies often flow between different fields of tech, leading to innovations. In this case, traditional computer systems concepts are helping to improve transformer-based LLMs. This was a brief share of my learnings, and I genuinely invite and appreciate feedback, insights, and further discussions on this topic.</p>
</section>
<section id="top-recommendation" class="level2">
<h2 class="anchored" data-anchor-id="top-recommendation">Top recommendation</h2>
<p>If you want to read more comperhensive write-up on this topic, I totally recommend, (At the Intersection of LLMs and Kernels - Research Roundup) [https://charlesfrye.github.io/programming/2023/11/10/llms-systems.html] by Charles Frye</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>