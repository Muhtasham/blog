<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="muhtasham">
<meta name="dcterms.date" content="2023-08-07">

<title>Machine Learners Guide to Real World - üóæ Exploring Ways to Extend Context Length in Transformers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learners Guide to Real World</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://huggingface.co/muhtasham"><i class="bi bi-gpu-card" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/muhtasham"><i class="bi bi-cpu" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/muhtasham9"><i class="bi bi-hash" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/muhtasham/"><i class="bi bi-file-earmark-person" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">üóæ Exploring Ways to Extend Context Length in Transformers</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">LLM</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>muhtasham </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 7, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#which-parts-of-transformer-architecture-can-we-hack-to-extend-context-length" id="toc-which-parts-of-transformer-architecture-can-we-hack-to-extend-context-length" class="nav-link" data-scroll-target="#which-parts-of-transformer-architecture-can-we-hack-to-extend-context-length">Which parts of Transformer architecture, can we hack to extend context length?</a></li>
  <li><a href="#the-linear-scaling-trick" id="toc-the-linear-scaling-trick" class="nav-link" data-scroll-target="#the-linear-scaling-trick">The Linear Scaling Trick</a></li>
  <li><a href="#dynamic-scaling-trick" id="toc-dynamic-scaling-trick" class="nav-link" data-scroll-target="#dynamic-scaling-trick">Dynamic Scaling Trick</a></li>
  <li><a href="#curse-of-naive-evaluation" id="toc-curse-of-naive-evaluation" class="nav-link" data-scroll-target="#curse-of-naive-evaluation">Curse of Naive Evaluation</a></li>
  <li><a href="#do-we-really-need-it" id="toc-do-we-really-need-it" class="nav-link" data-scroll-target="#do-we-really-need-it">Do We Really Need It?</a></li>
  <li><a href="#closing-thoughts" id="toc-closing-thoughts" class="nav-link" data-scroll-target="#closing-thoughts">Closing Thoughts</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Diving deep into the intricacies of large language models (LLMs), one hurdle quickly becomes evident: the context length limitation. While many recognize its implications, the question remains: how can we train models on more extensive context lengths without falling into the predicament of quadratic complexity?</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Expand to learn quadratic complexity in Transformers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Transformers, especially their self-attention mechanism, have a quadratic computational complexity in relation to the input sequence length. This arises from each token in the sequence interacting with every other token.</p>
<p>For an input sequence of length (N), there are (N^2) interactions, causing the computational cost to be proportional to (N^2). This means that as the sequence length doubles, the computational cost quadruples. This rapid growth in computational demands becomes prohibitive for very long sequences, posing challenges in tasks where extended context is crucial.</p>
</div>
</div>
</div>
<p>To put things in perspective and provide a clearer understanding, the following chart compares the context lengths of various models. This visualization can help elucidate the inherent limitations and possibilities of different Transformer-based models.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="token_count.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Here is a comparison of the context and how much it is human terms</figcaption><p></p>
</figure>
</div>
<p>The context length of the base model varies, with Falcon allowing up to 2K tokens and LLAMA2 accommodating up to 4K tokens, but we also have models like MPT trained with Alibi attention that can in theory support up to infinite context lengths.</p>
</section>
<section id="which-parts-of-transformer-architecture-can-we-hack-to-extend-context-length" class="level2">
<h2 class="anchored" data-anchor-id="which-parts-of-transformer-architecture-can-we-hack-to-extend-context-length">Which parts of Transformer architecture, can we hack to extend context length?</h2>
<p>As you may know by heart, Transformer has 4 main parts:</p>
<ul>
<li>Tokenization</li>
<li>Embedding</li>
<li>Positional encoding</li>
<li>Transformer block (several of these)</li>
<li>Softmax</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="arch.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">The architecture of a Transformer model Source: LLM University</figcaption><p></p>
</figure>
</div>
<p>Our quest to stretch context length will zero in on the positional encoding and the attention mechanism. Given the computational heft of attention, it‚Äôs worth diving deeper into Rotary Positional Encoding (RoPE).</p>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Expand to learn about positional encoding
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Transformers process input, not sequentially but in parallel, hence we need a way to know the order of the words, after tokening and getting the embeddings, we need to know the order of the words, and that is what positional encoding does, it adds a positional vector to each word, in order to keep track of the positions of the words. So if you want to give lot of context to LLM, you need to make sure that you don‚Äôt essentially lose the order of the words.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="pos_emb.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Here is simple visualisation of positional encoding Source: LLM University</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="the-linear-scaling-trick" class="level2">
<h2 class="anchored" data-anchor-id="the-linear-scaling-trick">The Linear Scaling Trick</h2>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
TL;DR
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Simple but needs minimal fine-tuning to observe the best results.</p>
</div>
</div>
</div>
<p>Rotary position embeddings (RoPE) used by LLAMA and Falcon are a variant of the positional encoding and they have very nice mathematical properties, but it turns out they are really bad at extrapolating, But turns out we can linearly interpolate by simply downscaling and dividing the position index by a scaling factor.</p>
<p>Fun fact: this method of Linear scaling was independently and simultaneously discovered by the Reddit user /u/kaiokendev and the <a href="https://arxiv.org/abs/2306.15595">Meta team</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="linear_interpolation.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">An illustration of our Position Interpolation method ‚Ä¶ Lower left illustrates Position Interpolation where we downscale the position indices (blue and green dots) themselves from [0, 4096] to [0, 2048] to force them to reside in the pretrained range Source: Extending Context Window of Large Language Models via Positional Interpolation</figcaption><p></p>
</figure>
</div>
</section>
<section id="dynamic-scaling-trick" class="level2">
<h2 class="anchored" data-anchor-id="dynamic-scaling-trick">Dynamic Scaling Trick</h2>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
TL;DR
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>More advanced technique, works okay without fine-tuning, but can benefit from it.</p>
</div>
</div>
</div>
<p>Turns out Scaling the RoPE linearly is not optimal to evenly distribute information, in Fourier space as an alternative to the linear scaling trick, the dynamic scaling trick was suggested by <a href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/?utm_source=share&amp;utm_medium=web2x&amp;context=3">u/bloc97/</a> provides an even more efficient solution.</p>
<ol type="a">
<li>works quite well without fine-tuning&nbsp;</li>
<li>doesn‚Äôt degrade the performance if the model is used with short sequences c)&nbsp;gracefully scales&nbsp;to long&nbsp;sequences, under a fixed&nbsp;parameterization</li>
</ol>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>There is significant performance drop of the above mentioned RoPe scaling methods due to static quantization, dynamic quantization should be good to go though.</p>
</div>
</div>
</div>
</section>
<section id="curse-of-naive-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="curse-of-naive-evaluation">Curse of Naive Evaluation</h2>
<p>After the rise of these tricks many in open-source community started to use them, but they were all evaluating via perplexity, while it is good starting point it is not the best way to evaluate the performance of the model</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
The first ~5 tokens in each segment contribute <em>most</em> of the loss. The shorter model has to deal with <em>double</em> the amount of early token areas!<br><br>We showed in our Shortformer paper that if you use this na√Øve evaluation method, longer models will always appear to be better‚Äì&gt; <a href="https://t.co/mbHsIwrYxe">pic.twitter.com/mbHsIwrYxe</a>
</p>
‚Äî Ofir Press (<span class="citation" data-cites="OfirPress">@OfirPress</span>) <a href="https://twitter.com/OfirPress/status/1679015108675239937?ref_src=twsrc%5Etfw">July 12, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</section>
<section id="do-we-really-need-it" class="level2">
<h2 class="anchored" data-anchor-id="do-we-really-need-it">Do We Really Need It?</h2>
<p>An observation from <a href="https://samaya.ai/blog/">Information Overload: The Challenges of Expanding Context Windows in Large Language Models</a>:</p>
<blockquote class="blockquote">
<p>Current language models do not effectively use their entire context, and that retrieval is still a crucial ingredient for effectively augmenting language models with external knowledge.</p>
</blockquote>
<p>I agree with this quote and would like to add that this behavior is typical of the Transformer-based architecture. The ML community has been paying significant ‚Äúattention‚Äù to this architecture. For instance, magic.dev is not using Transformers, and they recently announced <a href="https://magic.dev/blog/ltm-1">LTM-1</a>, an LLM with a staggering <em>5,000,000</em> token context window. After speaking with Eric Steinberger, the CEO of magic, I am highly optimistic that their approach utilizes context more effectively than Transformers.</p>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h2>
<p>Our journey through the intricacies of extending context length in LLMs showcases the constant innovation in this space. The challenges are many, but with every hurdle, the community finds novel ways to leap forward.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>