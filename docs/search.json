[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my blog",
    "section": "",
    "text": "ICLR‚Äô23 Transfer Learning Related Papers and their TLDR\n\n\n\n\n\n\n\npapers\n\n\niclr\n\n\ntransfer-learning\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n  \n\n\n\n\n‚è≥ From Alps to NLP: A 2022 Recap of Exploration and Growth\n\n\n\n\n\n\n\nreflections\n\n\ngoals\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2022\n\n\nmuhtasham\n\n\n\n\n\n\n  \n\n\n\n\nHerzlich Wilkommen!\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2022\n\n\nmuhtasham\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Herzlich Wilkommen!",
    "section": "",
    "text": "Welcome to my new blog and my new home on the internet!\nMy name is Muhtasham and I am excited to share my journey and learning in the field of machine learning with all of you. After publishing on Medium for a while, I have decided to make the switch to here in order to have a more personalized space to share my thoughts, experiences, and insights on this rapidly evolving field.\nOn this blog, you can expect to find articles and tutorials on a wide range of machine learning topics, including Machine Learning and Information Theory. I am passionate about staying up-to-date with the latest advancements in the field and sharing that knowledge with others. In addition to written content, I will also be sharing code tutorials and demonstrations to help readers better understand and apply the concepts I discuss.\nI hope that this blog will not only serve as a source of information and inspiration for those interested in machine learning, but also as a way for me to connect with like-minded individuals and continue learning from others in the community. Thank you for joining me on this journey and I look forward to sharing my knowledge and growth with all of you. This is my new home on the internet and I am excited to have you all as my virtual neighbors. Let‚Äôs learn and grow together!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hallo world, I‚Äôm Muhtasham Oblokulov. I design, build, and operate machine learning systems. I also (try to) write and speak about ML systems, engineering, and career.\nIn the past I have been honoured to work under supervision of great educators and researchers such as Prof.Mujdat Cetin from University of Rochester, Prof.Huseyin Ozkan from Sabanci University, Prof.¬†Suayb Arsalan from Massachusetts Institute of Technology, also great mentors Dr.Erdem Yoruk and Prof.¬†Dr.¬†Ren√© Brunner.\nI‚Äôm currently working as a Machine Learning Engineer at MunichRe. Previously, I worked at Alyne and Datamics.\nMy current interests are efficient and scalable machine learning systems and fast and scalable AI in production.\nTo learn more about me, you can check out my linkedin profilehere.\nIf you want to get in touch, you can send me an email. I‚Äôm always happy to hear interesting ideas and projects and to meet new people."
  },
  {
    "objectID": "posts/reflections22/index.html",
    "href": "posts/reflections22/index.html",
    "title": "‚è≥ From Alps to NLP: A 2022 Recap of Exploration and Growth",
    "section": "",
    "text": "üìù¬†Preface\nAs I look back on the year 2022, I can‚Äôt help but feel a sense of nostalgia and wonder.\n\n\n\nüåä¬†Intro\nIt‚Äôs been a challenging year for many of us, with all the ups and downs that come with life. But despite the difficulties, I remain hopeful that things will get better.\n\n\n‚ú®¬†Some highlights of the year\nOne of the biggest highlights of the year was completing my master‚Äôs thesis. This was a major milestone for me, and one that I dedicated to my late grandfather Boboi Usto, who was an educator and convinced my father to move our family to the city from a distant village to get a better education. I‚Äôm grateful for my grandfather‚Äôs influence and for the opportunity to receive a good education, and I‚Äôm proud of all the hard work I put into my thesis.\nThe experience of completing my thesis was immersive and full of learning, and I‚Äôm now working on making it more generalizable. It‚Äôs an exciting process, and I‚Äôm looking forward to refining my findings and sharing them with a wider audience.\nAs I reflect on the role that my grandfather played in my education and career, I‚Äôm reminded of the importance of appreciating our ancestors and the sacrifices they made to provide opportunities for future generations. It‚Äôs easy to take the things we have for granted, but it‚Äôs important to remember that they are often the result of hard work and dedication on the part of those who came before us.\nIn addition to completing my thesis, I also had the chance to participate in four hackathons this year, and I‚Äôm proud to say that I won in three of them. It was a great experience to work with a team to come up with innovative solutions to real-world problems, and I‚Äôm grateful for the opportunity to put my skills to the test.\nI also took the opportunity to share my thoughts and experiences through writing. I published two blog posts on Medium, which gave me the chance to reflect on my journey and share my insights with others.\nüî• One of the other major highlights of the year was co-founding MunichNLP community in May and being able to organise 17 events in a short time span. And exchanging thoughts with great researchers from Google Research and Brain team.\nIt‚Äôs been great to be able to bring together individuals who are passionate about machine learning, and I‚Äôm looking forward to continuing to grow and develop the group in the coming year.\nThe last but by no means least, I found new HOME at MunichRe, on a very interesting and challenging project. I plan to share my learning next year through my writings, so stay tuned.\n\n\nüåÑ¬†Wanderlust within me\nIn addition to my professional and academic pursuits, I also made the most of my time by exploring the beautiful Alps and nature surrounding Bavaria. I also had the chance to travel to new countries, including Italy üáÆüáπ and Switzerland üá®üá≠¬†\nüçù In Italy, I fell in love with the delicious and flavourful cuisine, which was unlike anything I had ever tasted before. From the classic pasta dishes to the mouthwatering pizzas, I couldn‚Äôt get enough of the fresh, high-quality ingredients and simple, yet satisfying flavours.\nüèîÔ∏è In Switzerland, I was mesmerised by the stunning natural beauty of the country, with its soaring peaks and crystal-clear lakes. The mountains were rugged and wild, with trails that led up to breathtaking vistas. And the lakes were so clear and pure that you could see right down to the bottom.\nThese were unforgettable experience and one that I will always treasure.\n\n\nüß©¬†Outro\nAs I look ahead to the next year, I have a few goals in mind. First, I want to make my fitness more consistent. I also hope to give a talk at PyData Berlin conference and attend NeurIPS ‚Äô23 (aka The Deep Learning conference of the year). I believe that setting goals is an important way to stay motivated and to continue to grow and learn, and I‚Äôm looking forward to working towards these goals in the coming year.\nTo conclude, I‚Äôm grateful for all the opportunities I‚Äôve had and for the support of my family, friends, and colleagues. I‚Äôm excited to see what the next year brings, and I‚Äôm looking forward to continuing to pursue my passions and goals.\nAs Albert Einstein once said,\n\nI have no special talent, I am only passionately curious.\n\nI hope to stay curious and continue to learn and grow in the coming year.\n\nThank you for reading my year in review. I hope that you have enjoyed learning about my experiences and adventures in 2022. I wish you all the best and hope that your future is filled with joy and success. Thank you for your support and here‚Äôs to a bright future ahead!"
  },
  {
    "objectID": "about.html#favorite-quotes",
    "href": "about.html#favorite-quotes",
    "title": "About",
    "section": "favorite quotes",
    "text": "favorite quotes\n\n‚ÄúThe best way to predict the future is to invent it.‚Äù - Alan Kay"
  },
  {
    "objectID": "posts/iclr23/index.html",
    "href": "posts/iclr23/index.html",
    "title": "ICLR‚Äô23 Transfer Learning Related Papers and their TLDR",
    "section": "",
    "text": "ICLR?\nThe International Conference on Learning Representations (ICLR) is one of the top conferences in the field of machine learning, and this year‚Äôs conference (ICLR 23) features several papers on the topic of transfer learning. Transfer learning is a technique that allows a model trained on one task to be applied to a different but related task, potentially improving performance and reducing the amount of data and computation required. Here are a few papers on transfer learning that are worth keeping an eye on at ICLR 23:\n\n\nPapers\nLearning Uncertainty for Unknown Domains with Zero-Target-Assumption This paper introduces a new framework called Maximum-Entropy Rewarded Reinforcement Learning (MERRL) for selecting training data for more accurate Natural Language Processing (NLP). The authors argue that conventional data selection methods, which select training samples based on test domain knowledge and not on real-life data, frequently fail in unknown domains like patent and Twitter. MERRL addresses this issue by selecting training samples that maximize information uncertainty measured by entropy, including observation entropy like empirical Shannon entropy, Min-entropy, R‚Äôenyi entropy, and prediction entropy using mutual information. The authors show that their MERRL framework using regularized A2C and SAC achieves significant improvements in language modeling, sentiment analysis, and named entity recognition over various domains, demonstrating strong generalization power on unknown test sets.\n\nAs an EE background, the author expresses genuine affinity to Shannon and his theory, which is the foundation of information theory and a powerful tool for understanding the limits of communication and computation\n\nRepresentational Dissimilarity Metric Spaces for Stochastic Neural Networks TL;DR: Representational dissimilarity metrics that account for noise geometry in biological and artificial neural responses.\nThis paper addresses the problem of quantifying similarity between neural representations, such as hidden layer activation vectors, in deep learning and neuroscience research. Existing methods for comparing deterministic or trial-averaged responses ignore the scale and geometric structure of noise, which are important in neural computation. To address this, the authors propose a new approach that generalizes previously proposed shape metrics to quantify differences in stochastic representations. These new distances can be used for supervised and unsupervised analyses and are practical for large-scale data. The authors show that this approach provides insights that cannot be measured with existing metrics, such as being able to more accurately predict certain network attributes from its position in stochastic shape space.\nTowards Estimating Transferability using Hard Subsets TL;DR: Authors propose HASTE, a strategy that ensures better transferability estimation using just a hard subset of target data.\nThis paper presents a new strategy called HASTE (HArd Subset TransfErability) for estimating the transferability of a source model to a particular target task, using only a harder subset of target data. HASTE introduces two techniques to identify harder subsets, one class-agnostic and another class-specific. It can be used with any existing transferability metric to improve their reliability. The authors analyze the relation between HASTE and the optimal average log-likelihood and negative conditional entropy, and empirically validate theoretical bounds. The results of experiments across multiple source model architectures, target datasets, and transfer learning tasks show that HASTE-modified metrics are consistently better or on par with the state-of-the-art transferability metrics.\n\n\nSome thougts\nIt is unfortunate that the double-blind review process, while well-intentioned, can sometimes result in good research being overlooked. The process, designed to prevent bias in the selection of papers, can also make it difficult for reviewers to properly assess the work of researchers from underrepresented groups or from institutions with less prestige.\nAdditionally, for the researchers whose work was not accepted at this year‚Äôs ICLR conference, it can be disheartening and discouraging. But it‚Äôs important to remember that the review process is highly competitive and that getting a paper accepted at a top conference like ICLR is a significant accomplishment. And for those whose work was not accepted, it doesn‚Äôt mean that their research is not valuable or that they are not good researchers. It is also important to note that there are many other conferences, journals and outlets to present the research and get the recognition it deserves.\nIt‚Äôs also important to remember that rejection is a common experience in any field, especially in research. It‚Äôs a part of the process of discovery and innovation. It‚Äôs important to keep pushing forward, to continue to conduct valuable research, and to keep submitting to conferences and journals. It‚Äôs also important to keep an open mind and to look for feedback and constructive criticism. And most importantly, don‚Äôt give up.\nIn short, while the double-blind review process has its drawbacks, it‚Äôs important to remember that rejection is a part of the process and that good research can come from any institution or researcher. And for those whose work was not accepted, keep pushing forward and don‚Äôt give up. Here is some advice from Jonathan Frankle, the author of the infamous ‚ÄúThe Lottery Ticket Hypothesis‚Äù paper:\n\n\nTime for my usual refrain: Most papers weren't accepted to ICLR, and don't let Twitter fool you into thinking otherwise. Plenty of smart people and great papers didn't get the outcome they wanted, and you're in very good company if that's you right now.\n\n‚Äî Jonathan Frankle (@jefrankle) January 22, 2023\n\n\n\n\nConlusion\nOverall, transfer learning is a powerful technique that allows models to be applied to different but related tasks, potentially improving performance and reducing the amount of data and computation required. The papers discussed in this blog post highlight some of the latest research in transfer learning, including methods for transferring knowledge between language and images, between different modalities, and for graph-structured data.\nIt is clear that transfer learning is an active and rapidly growing area of research, and we can expect to see many more exciting developments in the coming years. We look forward to seeing the outcome of these papers and more at ICLR 23.\nIn conclusion, transfer learning has many practical applications and these papers give a glimpse of the future possibilities of transfer learning, it is a promising area of research and have a lot of room for improvement. These papers will give an insight into the latest developments in the field and open up the doors for new research opportunities.\n\n\nOutro\nIf you enjoyed this post, please consider sharing it with your friends and colleagues."
  }
]