[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey there I’m Muhtasham! I hope your world’s going well. I’ll bet you’re cool.\nI spent my pre-training days in Urmetan village and did some supervised fine-tuning (SFT) in Dushanbe, Tajikistan.\nDuring high school, I enjoyed tinkering with computers and participating in various olympiads.\nI speak five languages: Tajik, Turkish, Russian, English, and German. I’m also reasonably well-versed in several programming languages.\nWhile it’s not easy thing I am trying concentrating intensely on the right problems, and remaining open to fresh perspectives.\nFounders: I also participate in angel rounds. Noe warms needed, decision in 15min, your terms."
  },
  {
    "objectID": "posts/starcoder/index.html",
    "href": "posts/starcoder/index.html",
    "title": "💫 StarCoder: A Revolutionary Code Generation Model",
    "section": "",
    "text": "Starry night in San Francisco Image by Author\nAre you tired of spending hours writing repetitive code? Do you find yourself searching through documentation and Stack Overflow for code snippets? Look no further! Introducing StarCoder, the ultimate code generation model that will revolutionize your coding experience."
  },
  {
    "objectID": "posts/starcoder/index.html#understanding-starcoder",
    "href": "posts/starcoder/index.html#understanding-starcoder",
    "title": "💫 StarCoder: A Revolutionary Code Generation Model",
    "section": "Understanding StarCoder",
    "text": "Understanding StarCoder\n\nThe Power of 15 Billion Parameters\nThe Power of 15 Billion Parameters: StarCoder is a cutting-edge code generation model that boasts an impressive 15 billion parameters. Built on a decoder architecture, this model has been fine-tuned to excel in generating high-quality code across various programming languages.\n\n\nTrained on The Stack\nThe Stack, a vast collection of programming languages and metadata with a staggering 6.4 TB of permissively licensed code, served as the training ground for StarCoder. With 800GB of code spanning 86 popular programming languages, including GitHub Issues, Jupyter Notebooks, and Git Commits, StarCoder has acquired a deep understanding of diverse codebases.\n\n\nRecent Techniques for Enhanced Performance\nStarCoder utilizes advanced techniques such as Multi Query Attention and Flash Attention to optimize memory usage and improve context length. With an impressive context window of 8,192 tokens, equivalent to several pages of code, StarCoder enables you to generate comprehensive code snippets.\n\n\nStarPII: Safeguarding Privacy\nBefore training StarCoder, a separate model called StarPII was developed to ensure the removal of personally identifiable information (PII) such as IP addresses, names, and passwords. This commitment to data privacy ensures the responsible use of code generation capabilities.\n\n\nOutperforming the Giants\nIn rigorous Human Evaluation (Human Eval) benchmarks, StarCoder surpassed many large models, including PaLM 1 540B and LLaMa 66. StarCoder’s superior performance is a testament to its finely tuned code generation capabilities."
  },
  {
    "objectID": "posts/starcoder/index.html#unleashing-the-power-of-starcoder",
    "href": "posts/starcoder/index.html#unleashing-the-power-of-starcoder",
    "title": "💫 StarCoder: A Revolutionary Code Generation Model",
    "section": "Unleashing the Power of StarCoder",
    "text": "Unleashing the Power of StarCoder\nWith StarCoder, the possibilities are endless. Let’s explore some exciting use cases where StarCoder’s productivity gains truly shine:\n\n1. Efficient Code Generation\nStarCoder simplifies code generation by automating repetitive tasks. With just a few lines of code, you can leverage StarCoder’s capabilities to generate code snippets tailored to your specific requirements. Whether you need a function definition or a complete code block, StarCoder has got you covered.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigcode/starcoderbase-1b\"\ndevice = \"cuda\"  # Set to \"cuda\" for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n\nPro Tip: You can use google collab to avoid memory issues\n\n\n\n2. Fill-in-the-Middle Code Completion\nStarCoder excels at fill-in-the-middle code completion tasks. By providing context and using special tokens, you can generate code snippets with missing parts accurately filled. This feature is particularly useful when you have an existing codebase and need to complete or modify code segments efficiently.\ninput_text = \"&lt;fim-prefix&gt;def print_hello_world():\\n    &lt;fim-suffix&gt;\\n    print('Hello world!')&lt;fim-middle&gt;\"\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n\n\n3. Enhancing Reasoning and Conversational Tasks\nSurprisingly, StarCoder demonstrates remarkable performance in reasoning and conversational tasks, despite not being explicitly trained on text data. The combination of well-crafted prompts and the model’s inherent reasoning abilities derived from code pre-training contribute to its success. In fact, in the Helm evaluation, StarCoder achieved scores comparable to models such as Anthropic V4 and GPT-3, which are significantly larger in size.\n\n\n4. Interacting with StarCoder through Python API\nFor developers looking to integrate StarCoder’s capabilities directly into their Python applications, the model offers a straightforward API. Below, is a Python script that demonstrates how to interact with the StarCoder model using the Hugging Face API. You can use this script to query the model and get code generation results right in your Python environment:\nimport requests\nimport json \nimport os\n\nMODEL_ID = \"bigcode/starcoder\"\nAPI_TOKEN = os.environ.get('HUGGINGFACE_API_TOKEN')\n\nif not API_TOKEN:\n    raise EnvironmentError(\"Please set your Hugging Face API token as an environment variable named 'HUGGINGFACE_API_TOKEN'\")\n\ndef query(payload, model_id, api_token):\n    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n    API_URL = f\"https://api-inference.huggingface.co/models/{MODEL_ID}\"\n    response = requests.post(API_URL, headers=headers, json=payload)\n    print(\"Status Code:\", response.status_code)\n    print(\"Response Content:\", response.text)\n\n    try:\n        return response.json()\n    except json.JSONDecodeError:\n        print(\"Failed to decode the response as JSON.\")\n        return None\n\npayload = {\"inputs\": \"def convert_color_hsl_to_rgb(rgb_tuple):\",    \n            \"parameters\": {\"max_new_tokens\": 512},\n}\n\ndata = query(payload, MODEL_ID, API_TOKEN)\nIn the script above, the API_TOKEN is obtained from an environment variable named HUGGINGFACE_API_TOKEN. You can set this environment variable in your operating system or in your Python environment before running the script. Remember to keep your API token confidential to prevent unauthorized access. You can obtain your API token from the Hugging Face website. Remember to keep your API token confidential to prevent unauthorized access.\nThis script uses the requests and json Python libraries to send a POST request to the Hugging Face API, passing a code snippet as the input and receiving a generated code snippet as the output. Remember to install the necessary Python packages by running pip install requests before using the script.\nBy using this script, developers can seamlessly integrate StarCoder’s code generation capabilities into their Python applications, enhancing productivity and efficiency in coding tasks."
  },
  {
    "objectID": "posts/starcoder/index.html#fine-tuning-and-commercial-use",
    "href": "posts/starcoder/index.html#fine-tuning-and-commercial-use",
    "title": "💫 StarCoder: A Revolutionary Code Generation Model",
    "section": "Fine-tuning and Commercial Use",
    "text": "Fine-tuning and Commercial Use\nStarCoder offers the flexibility of fine-tuning to cater to specific use cases. By following the steps provided in the GitHub repository, you can fine-tune the model according to your requirements. This makes StarCoder an ideal choice for enterprises with strict usage requirements and specialized code generation needs.\nFurthermore, StarCoder is available for commercial use, enabling businesses to leverage its power to boost productivity and efficiency in their software development workflows."
  },
  {
    "objectID": "posts/starcoder/index.html#training-insights",
    "href": "posts/starcoder/index.html#training-insights",
    "title": "💫 StarCoder: A Revolutionary Code Generation Model",
    "section": "Training Insights",
    "text": "Training Insights\nFor those curious about the technical aspects of StarCoder’s training, here are some key details:\n\nArchitecture: StarCoder is built upon the GPT-2 model, utilizing multi-query attention and the Fill-in-the-Middle objective.\nPretraining Steps: StarCoder underwent 600K pretraining steps to acquire its vast code generation capabilities.\nPretraining Tokens: During pretraining, StarCoder processed a staggering 236 billion tokens, allowing it to learn from an extensive corpus of code.\nPrecision: StarCoder was trained with float16 precision, striking a balance between training efficiency and model performance."
  },
  {
    "objectID": "posts/starcoder/index.html#open-source-community",
    "href": "posts/starcoder/index.html#open-source-community",
    "title": "💫 StarCoder: A Revolutionary Code Generation Model",
    "section": "Open-Source Community",
    "text": "Open-Source Community\nStarCoder is not just a powerful code generation model, but also an open-source project driven by a collaborative community. The development of StarCoder has been made possible by the collective efforts of developers, researchers, and coding enthusiasts like yourself.\nContributions from community members play a crucial role in refining and expanding the capabilities of StarCoder. If you’re eager to contribute to this revolutionary project, you can join the official Slack community by filling out the form on BigCode’s website. By becoming a part of the community, you can actively shape the future of code generation and make a meaningful impact in the field.\nThe collaborative spirit of StarCoder fosters innovation, encourages knowledge sharing, and ensures that the model remains dynamic and adaptive to the evolving needs of developers and organizations worldwide.\nChat with the model here: StarCoder Chat\nFor models, demos, papers, VSCode extensions, and more, visit: Big-Code Official Space\nUnleash the power of StarCoder, embrace collaboration, and revolutionize your coding journey today!"
  },
  {
    "objectID": "posts/os-concepts-llm/index.html",
    "href": "posts/os-concepts-llm/index.html",
    "title": "2️⃣ Concepts from Operating Systems That Found Their Way in LLMs",
    "section": "",
    "text": "This IBM punched card, an early form of data encoding, reminds us that at the core, computers don’t see strings or words but numerical values. A nod to the origins, it’s the OG tokenizer that set the stage for today’s intricate LLMs Image by Author"
  },
  {
    "objectID": "posts/os-concepts-llm/index.html#introduction",
    "href": "posts/os-concepts-llm/index.html#introduction",
    "title": "2️⃣ Concepts from Operating Systems That Found Their Way in LLMs",
    "section": "Introduction",
    "text": "Introduction\nDiving into the intricacies of technology often uncovers unexpected parallels. Recently, I’ve been struck by how foundational computer operating system concepts are making waves in the domain of Large Language Models (LLMs), especially the transformer variants. I will try to spotlight two concepts that I observed so far: Branch Prediction and Virtual Memory Paging."
  },
  {
    "objectID": "posts/os-concepts-llm/index.html#branch-prediction-in-cpus",
    "href": "posts/os-concepts-llm/index.html#branch-prediction-in-cpus",
    "title": "2️⃣ Concepts from Operating Systems That Found Their Way in LLMs",
    "section": "Branch Prediction in CPUs",
    "text": "Branch Prediction in CPUs\n\nTraditional Use in CPUs\nBranch prediction is a tool used in computers to speed up how they work. When a computer has to choose between two sets of instructions, it tries to guess which set will be used next. Here’s a breakdown:\n\nPrediction: The computer makes a guess about the next set of instructions.\nSpeculative Sampling: Based on that guess, the computer starts working on those instructions, even if it’s not sure it’s the right choice.\nCorrection: If the guess is right, the computer keeps going. If it’s wrong, the computer starts over and chooses the other set.\n\n\n\nHow LLMs Use This Idea\nGoogle Deepmind used similar idea to make LLMs faster in Accelerating Large Language Model Decoding with Speculative Sampling. Their algorithm uses a smaller draft model to make initial guesses and a larger primary model to validate them. If the draft often guesses right, operations become faster, reducing latency.\nThere are some people speculating that OpenAI might be using speculative decoding on GPT-4’s inference. If you’re interested in a visual explanation, an insightful visualization on speculative decoding/assisted generation is worth checking out.\nMoreover, there’s an intriguing recent paper, Online Speculative Decoding (OSD), that takes this concept to another level. The essence of OSD is to continually refine (multiple) draft model(s) based on observed user query data, leveraging the spare computational resources in LLM serving clusters. This approach is especially useful when the draft model is not as accurate as the target model, as it can be continually refined to improve its predictive accuracy.\n\n\n\n\n\n\nCautionPotential Security Vulnerabilities in LLMs\n\n\n\n\n\nAs neural network models, especially Large Language Models (LLMs), become pivotal in various applications, understanding and addressing their security vulnerabilities is crucial.\n\nStealing the Decoding Algorithms of Language Models\nA key component of generating text from modern LLMs is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners.\nIn this work, authors show, that an adversary with typical API access to an LLM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs.\nSpectre and Its Implications for LLMs\nSpectre is one of the key transient execution CPU vulnerabilities, which involves timing side-channel attacks affecting modern microprocessors. The speculative execution in these processors might expose private data through observable side effects. While LLMs operate based on speculative principles similar in concept, it’s important to note that their speculative nature is fundamentally different. However, given that neural networks can be susceptible to timing side-channel attacks especially when LLMs interface with runtime environments capable of executing code, there is a potential for such attacks to be exploited.\nThe Risk of LLMs Interacting with Code Interpreters\nA significant concern arises when LLMs have access to environments capable of running code, such as the Code Interpreter. In such scenarios, vulnerabilities could be exploited to make the LLMs run malicious code, posing even more significant security threats. It’s important to exercise caution and ensure secure barriers when deploying LLMs in such settings.\nSafeguarding Model Weights\nWeights of machine learning models are often stored in formats, such as pickle, that are vulnerable to security breaches. An alternative to consider is “Safetensors” — a more secure format ensuring the safety of tensor data. Not only is it a secure choice, but Safetensors also has impressive speed.\nLLMs: Deep Neural Networks Behind the Scenes\nIt’s important to remember that LLMs are deep neural networks (DNNs) at their core. This association brings inherent security concerns, as highlighted by these research papers:\n\nStealing Neural Networks via Timing Side Channels: This paper underscores the susceptibility of Neural Networks to timing side channel attacks and proposes a black-box Neural Network extraction technique.\nNeural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks: This research emphasizes the opaque nature of DNNs, making them prone to backdoor attacks. It introduces methods to detect and counter such hidden threats.\n\nLearning from the Past: Avoiding the Mistakes of Operating Systems\nAs we explore more about Large Language Models (LLMs) and how they can be used, it’s clear that they share some similarities with operating systems. Just like operating systems had certain issues and vulnerabilities, LLMs could have them too. It’s important for us to learn from those past mistakes to make sure we don’t run into similar problems, like the security issues seen with Spectre, a vulnerability found in many modern microprocessors."
  },
  {
    "objectID": "posts/os-concepts-llm/index.html#virtual-memory-paging",
    "href": "posts/os-concepts-llm/index.html#virtual-memory-paging",
    "title": "2️⃣ Concepts from Operating Systems That Found Their Way in LLMs",
    "section": "Virtual Memory Paging",
    "text": "Virtual Memory Paging\n\nTraditional Use in Computers\nVirtual memory is an abstraction provided by the operating system that makes it seem to an application as if it has access to more RAM than is physically available. When the actual RAM gets filled up, the operating system uses a portion of the computer’s storage space (typically the hard drive) as an extension of RAM. This process enables the computer to handle more tasks concurrently by mapping the application’s memory addresses to actual physical locations, which could be in RAM, on the hard disk, or even other storage mediums.\nIn essence, virtual memory gives applications the illusion they’re utilizing a large, contiguous chunk of RAM, even though the reality behind the scenes might be quite different.\n\n\nHow LLMs Use This Idea\nTransformers, especially LLMs, feature a mechanism called “KV cache,” similar to RAM, that temporarily stores key-value pairs during attention operations for quick access. To efficiently handle longer sequences that don’t fit in memory, they could potentially adopt techniques inspired by virtual memory paging.\n\nvLLM: virtual paging for KV cache\nResearchers from UC Berkeley introduced this idea in a study called Efficient Memory Management for Large Language Model Serving with PagedAttention also dubbed as vLLM.\nThe heart of vLLM is PagedAttention. It’s a fresh take on how attention works in transformers, borrowing from the paging idea in computer OS. Remarkably, without changing the original model, PagedAttention allows batching up to 5x more sequences. This means better use of GPU resources and faster operations.\nAlso here’s a rapid breakdown of some crucial state of the art LLM serving techniques as of Oct 2023:\n\nContinuous Batching: Increases throughput by allowing requests to immediately jump onto an ongoing GPU batch, minimizing wait time.\nPagedAttention: Much like OS’s virtual paging but tailored for KV cache in LLMs, allowing 3x more simultaneous requests and thereby tripling throughput.\nSpeculative Decoding: Uses a smaller draft model to make initial guesses and a larger primary model to validate them. If the draft often guesses right, operations become faster, reducing latency, like we described in the previous section.\n\n\n\nMemGPT: “Virtually” Extending LLM’s Limited Context Windows\nMemGPT: Towards LLMs as Operating Systems also from UC Berkley is a new way to help LLMs like GPT-4 remember more information. Think of it as adding an extra brain to the LLM. This extra brain has two parts:\n\nInternal Memory (LLM RAM): A small space where the LLM keeps important information.\nExternal Memory (LLM HDD): A much larger space where the LLM can store and retrieve data when needed.\n\nWhen the LLM needs data from the external memory, it breaks it into smaller pieces that fit into the internal memory. This lets LLMs handle big tasks that need lots of information.\nMemGPT makes it easier to use LLMs for tasks that need a lot of memory. With this tool, we don’t have to worry about the LLM running out of space.\nIn the realm of LLMs, context and memory are kind of like the foundational, “RAM” of our era. Andrej Karpathy has already made this comparison:\n\n\nThe analogy between GPTs of today to the CPUs of early days of computing are interesting. GPT is a funny kind of programmable text computer. Have to think through it more 🤔 but e.g.:## MemoryGPT-4 RAM is ~log2(50K vocab size)*(32K context length)/(8 bits/byte) ~= 64kB,…\n\n— Andrej Karpathy (@karpathy) April 7, 2023"
  },
  {
    "objectID": "posts/os-concepts-llm/index.html#conclusion",
    "href": "posts/os-concepts-llm/index.html#conclusion",
    "title": "2️⃣ Concepts from Operating Systems That Found Their Way in LLMs",
    "section": "Conclusion",
    "text": "Conclusion\nIdeas and strategies often flow between different fields of tech, leading to innovations. In this case, traditional computer systems concepts are helping to improve transformer-based LLMs. This was a brief share of my learnings, and I genuinely invite and appreciate feedback, insights, and further discussions on this topic."
  },
  {
    "objectID": "posts/os-concepts-llm/index.html#top-recommendation",
    "href": "posts/os-concepts-llm/index.html#top-recommendation",
    "title": "2️⃣ Concepts from Operating Systems That Found Their Way in LLMs",
    "section": "Top recommendation",
    "text": "Top recommendation\nIf you want to read more comperhensive write-up on this topic, I totally recommend, (At the Intersection of LLMs and Kernels - Research Roundup) [https://charlesfrye.github.io/programming/2023/11/10/llms-systems.html] by Charles Frye"
  },
  {
    "objectID": "posts/batching-strategies/index.html",
    "href": "posts/batching-strategies/index.html",
    "title": "🚀 LLM Inference Deep Dive: Metrics, Batching & GPU Optimization",
    "section": "",
    "text": "Welcome back, reader. Long time no see. This year I was locked in working on my skill issues, hence did not have time to write much. But wanted to close the year with a deep technical dive into LLM inference optimization.\nIn this post, we’ll dissect the key performance metrics of LLM inference engines - from TTFT and ITL to throughput measurements. We’ll explore GPU memory/compute bounds, analyze batching strategies like in-flight batching (IFB), and simulate their effects on system performance. Whether you’re optimizing inference latency or scaling deployment, understanding these fundamentals is crucial for building efficient LLM systems."
  },
  {
    "objectID": "posts/batching-strategies/index.html#introduction",
    "href": "posts/batching-strategies/index.html#introduction",
    "title": "🚀 LLM Inference Deep Dive: Metrics, Batching & GPU Optimization",
    "section": "",
    "text": "Welcome back, reader. Long time no see. This year I was locked in working on my skill issues, hence did not have time to write much. But wanted to close the year with a deep technical dive into LLM inference optimization.\nIn this post, we’ll dissect the key performance metrics of LLM inference engines - from TTFT and ITL to throughput measurements. We’ll explore GPU memory/compute bounds, analyze batching strategies like in-flight batching (IFB), and simulate their effects on system performance. Whether you’re optimizing inference latency or scaling deployment, understanding these fundamentals is crucial for building efficient LLM systems."
  },
  {
    "objectID": "posts/batching-strategies/index.html#latency-metrics",
    "href": "posts/batching-strategies/index.html#latency-metrics",
    "title": "🚀 LLM Inference Deep Dive: Metrics, Batching & GPU Optimization",
    "section": "Latency Metrics",
    "text": "Latency Metrics\nSeveral important metrics are available in the benchmarks:\n\nTTFT (Time To First Token): measures the time it takes for the model to generate the first token of a response.\nE2E Latency (End-to-End Latency): measures the total time it takes for the model to generate a complete response.\nITL (Inter-Token Latency): also known as Time Per Output Token (TPOT), measures the average time the client waits between consecutive tokens in a response in the streaming scenario.\n\nTo separate the prefill characteristics from the decoding ones, TTFT and ITL are reported independently in tools like GenAI-Perf, as shown in the image below.\n\n\n\nmetrics\n\n\n\nEstimating Inter-Token Latency (ITL) For Human Perception\nLet us try to estimate typical inter-token latencies for human perception. Consider the following details:\n\nNormal reading for comprehension speed is about 200–230 words per minute (wpm).\nSkimming speed is 700 wpm (see wiki).\nWe can assume that an arbitrary token accounts for around 0.75 words, on average (as a standard simplifying assumption used a lot for English).\n\nLet’s convert these to ITL-compatible units of ms/token. Using reasonable average input statistics, we can expect to get decent average output statistics via some simple unit conversion.\ndef calculate_itl(wpm, words_per_token=0.75):\n    ## NOTE: 60 seconds to a minute, 1000 milliseconds to a second\n\n    ## NOTE: Unit arithmetic of [t/s] = [(words/min) * (tokens/word) * (min/s)] = [(words/min) / (words/tokens) / (s/min)]\n    tokens_per_second = wpm / words_per_token / 60\n\n    ## NOTE: Unit arithmetic of [ms/t] = [(ms/s * s/token)] = [(ms/s) / (token/s)]\n    inter_token_latency_ms = 1000 / tokens_per_second\n\n    return inter_token_latency_ms\n\nprint(f\"READING:  230 words per minute correspond to {calculate_itl(230):.0f}ms between tokens, on average\")  \n## 196\nprint(f\"SKIMMING: 700 words per minute correspond to {calculate_itl(700):.0f}ms between tokens, on average\")  \n## 64\nIn the vast majority of the LLM inference setups, ITL is much lower than these reference values, typically around 20–40 ms. Still, it’s important to keep these numbers in mind as minimum thresholds for comfort.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nIn addition to defining a minimum threshold for comfort, you may also find it useful to identify a maximum threshold for speed in certain contexts. For example, a chat application with a lower-than-usual load might be so fast that it dumps output at an uncomfortable rate and causes the text view to scroll automatically, making it uncomfortable to read. For these cases, artificially sleeping between or iterating over streaming yields may be desirable.\nLLMPerf, another common benchmarking SW, incorporates first-token latency into inter-token latency computation. Beware of comparing directly the results from different benchmarking tools."
  },
  {
    "objectID": "posts/batching-strategies/index.html#simulator",
    "href": "posts/batching-strategies/index.html#simulator",
    "title": "🚀 LLM Inference Deep Dive: Metrics, Batching & GPU Optimization",
    "section": "Simulator",
    "text": "Simulator\nUsing this simulator, we can model our properties of interest to their extremes and see how our system performs in asymptotic cases. Lets try to understand how Tensor-LLM assembles the requests into batches.\n\n\n\nsimulation\n\n\nYou see two plots above. In the top plot, you can see batch composition depending on time. Each column is one LLM evaluation from the first layers to the last. The color of the cell and the letter represent the current stage of the request in the batch:\nEach cell in the plot represents one of three states:\n\nprefill (pink) - Initial processing of the prompt\ndecoding (yellow) - Generating each output token\nempty slot (blue) - No active request\n\nThe width of cells indicates timing: - Prefill width = TTFT (2 ticks in this example)\n- Decoding width = ITL (1 tick in this example)\nIn the bottom plot, you can see the measured latencies at the moments of measurement. For example, the pink \\((2, 2)\\) point represents the TTFT measurement of our request: at the time of \\(2\\) ticks the prefill has been completed and it took \\(2\\) ticks for our request. Similarly, we represent the E2E latency of \\(5\\) measured at \\(5\\) ticks by the green-yellow point. Additionally, we show the current queue size by the orange line. Since we send only one request in this example, it is always equal to 0. Note that the latency scale uses the left y-axis and the queue size uses the right one. The x-axis is shared between both plots.\nYou can check the repo for more examples and details, simulating different batching strategies and their effects on the system."
  },
  {
    "objectID": "posts/batching-strategies/index.html#batching-the-key-to-efficient-throughput",
    "href": "posts/batching-strategies/index.html#batching-the-key-to-efficient-throughput",
    "title": "🚀 LLM Inference Deep Dive: Metrics, Batching & GPU Optimization",
    "section": "Batching: The Key to Efficient Throughput",
    "text": "Batching: The Key to Efficient Throughput\nGPUs are very good at processing highly-parallelized and concurrent tasks. For example, an NVIDIA H100 GPU has 16,896 FP32 Cores per GPU and 528 Tensor Cores organized into 132 streaming multiprocessors (SMs) (see the datasheet), where each core can execute an independent thread of mathematical computation and each SM can parallelize hundreds of threads (either core-enabled math operations or parallelizable memory operations) at a time. The most efficient way to utilize the GPU is to make sure all the SMs always have something to compute and some memory operations to run at any given time.\n\nMemory-Bound and Compute-Bound Functions\nNow consider a simplified model where a function reads its input from memory, performs math operations, and then writes its output to memory. Let’s assume \\(T_{mem}\\) time is spent in accessing memory and \\(T_{math}\\) time is spent performing math operations. If we further assume that memory and math portions of different threads can be overlapped:\n\nThe total time for the function is \\(max(T_{mem}, T_{math})\\), and the longer of the two times demonstrates what limits performance.\nIf math time \\(T_{math}\\) is longer we say that a function is math limited or compute-bound.\nIf memory time \\(T_{mem}\\) is longer then it is memory limited or memory-bound.\n\nLet’s connect these topics to the LLM operations we take for granted in typical use.\n\n\nPrefill:\nDuring prefill, most operations are compute-bound.\n\nPropagating the initial context requires larger matrices to interact to resolve attention across the entire prefill context.\nThe intermediate results of the calculation are written to KV-cache (cached attention matrix values stored in memory), but that requires few memory operations.\nCompute-bound property generally manifests after a certain prefill token limit (in my tests, around 300 tokens or more cause \\(T_{math} &gt; T_{mem}\\) on my GPU setup).\n\n\n\nDecoding:\nDuring decoding, most operations are memory-bound.\n\nGenerating one token at a time means the input for each next token is a single embedded token and many cached components, which results in small matrix operations during forward propagation.\nThe KV-cache from previously provided/generated tokens continues to grow, so retrieving requires more resources.\n\nTo improve the efficiency, you need to increase computations per read byte of memory. The simplest way to do it is by batching the requests together. With the batch size b the system can load the weights of the LLM from the GPU memory to the SMs once, but compute b next tokens.\n\n\nArithmetic Intensity\nTo help support this model, arithmetic intensity is a common metric for evaluating the compute-boundedness of a given function. It is defined as the ratio of floating-point operations to the number of data elements accessed by the function - usually in FLOPs/byte - with a high arithmetic intensity indicating a high computational load.\nBelow is a figure from the paper SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills demonstrating the arithmetic intensity of prefills and decodes for LLaMA-13B on A6000 GPU. The different colors represent different operations within the transformer block: preproj for preprojection, a single matrix multiplication; attn for attention computation; preproj for postprojection; and ffn for feed-forward network.\n\n\n\nsarathi"
  },
  {
    "objectID": "posts/batching-strategies/index.html#throughput-metrics",
    "href": "posts/batching-strategies/index.html#throughput-metrics",
    "title": "🚀 LLM Inference Deep Dive: Metrics, Batching & GPU Optimization",
    "section": "Throughput Metrics",
    "text": "Throughput Metrics\nOne of the advantages of using latency metrics is their ease of interpretation and lack of ambiguity. They can be measured regardless of the inference system’s organization, and they require no awkward normalization to become immediately interpretable!\nHowever, to get from latency benchmarks to GPU count estimations, one also needs some throughput metrics. These metrics measure the capacity of a system to process data per unit of time. Here’s a breakdown of the various throughput metrics and their implications:\n\nTokens per second per model instance:\n\nAcross all phases: Measures the total processing capability of a single model instance, including pre-processing, generation, and post-processing stages.\nOnly in the generation phase (a.k.a 1/ITL): Focuses on the model’s ability to generate tokens, offering a direct measure of the model’s generative performance.\n\nTokens per second per GPU:\n\nThis metric can be specified for either only the generation phase or all phases, indicating how effectively a GPU is being utilized to process tokens. This helps in assessing the efficiency of the GPU in handling specific tasks within the inference pipeline.\n\nTokens per second per server:\n\nSimilar to the per GPU metric but scaled up to the server level. This measures the overall throughput of an entire server, which may contain multiple GPUs and model instances. It’s crucial for evaluating server-level performance and infrastructure scalability.\n\nPrompts per second:\n\nPer Model Instance: This measures how many complete prompts a single model instance can handle in one second, providing a straightforward metric of model instance efficiency.\nPer GPU: Reflects the number of prompts a GPU can process per second, useful for gauging GPU performance in a real-world application scenario.\nPer Server: Measures the capacity of a server to handle prompts, indicating the throughput at the server scale.\n\nConcurrent Metrics:\n\nConcurrent Requests: Refers to the number of requests a system can handle at the same time. It’s a critical measure of system robustness and concurrency handling.\nConcurrent Clients: Indicates how many clients can simultaneously interact with the system without degrading performance, essential for understanding the scalability of client-server interactions.\n\n\n\nChoosing The Right Throughput Metric\nOften, benchmarking software shortens the units to just tokens/second, leading to ambiguity about the applied normalization. For the purposes of sizing, the most convenient throughput metric is prompts/second/server, which allows benchmarkers to choose between different combinations of tensor parallelism strategies (from now on TP) with the number of servers being a natural parameter. In our established benchmarks, we normalize by the standard servers with 8 GPUs, meaning that we consider the throughput of 2 instances of TP4 or 8 instances of TP1. This metric also highlights the dependence of the throughput on the specific composition of requests, including input and output lengths."
  },
  {
    "objectID": "posts/batching-strategies/index.html#inflight-batching-ifb",
    "href": "posts/batching-strategies/index.html#inflight-batching-ifb",
    "title": "🚀 LLM Inference Deep Dive: Metrics, Batching & GPU Optimization",
    "section": "Inflight Batching (IFB)",
    "text": "Inflight Batching (IFB)\nIFB is a technique used during LLM inference to balance GPU memory with compute utilization and reduce latency.\nDuring auto-regressive inference, the LLM is evaluated from the first layers to the last for every token to generate, using previous tokens to generate the next ones. The process involves:\n\nThe first call to the LLM producing the prefill token\nSubsequent calls generating the decoding tokens\n\nIFB enables sequences at different stages (prefill and decoding) to be processed within the same batch, without requiring all requests to be completed before the next one can enter the batch.\nKey Benefits of IFB:\n\nAllows for a nearly constant batch size for each token, resulting in higher GPU utilization\nEnables new request execution to start quicker when slots are available, as the scheduler only needs to wait for the generation of the next token, not the completion of current requests\n\nSee the illustration below for a visual representation of in-flight batching in TensorRT-LLM:\n\n\n\nifb_trt-llm\n\n\n\nChunked Context\nTo optimize performance, you can separate the prefill into chunks and batch together one chunk of prefill and multiple deco dings to attempt a balance between \\(T_{mem}\\) and \\(T_{math}\\). This technique is implemented in TensorRT-LLM as Chunked Context. It is important to keep chunks large enough to still be able to reach compute-boundness.\n\n\nMax Batch Size\nTensorRT-LLM engines have two parameters called max_batch_size:\n\nOne is set for the engine build and is used during the kernel selection process to make sure the resulting batch-size-capable system fits into memory.\nOne is set for runtime and specifies how many requests can be batched together. This is the one we use in our simulation.\n\nNote that the second one should be less than or equal to the first one. See the docs for details."
  },
  {
    "objectID": "posts/batching-strategies/index.html#outro",
    "href": "posts/batching-strategies/index.html#outro",
    "title": "🚀 LLM Inference Deep Dive: Metrics, Batching & GPU Optimization",
    "section": "Outro",
    "text": "Outro\nHope it was useful. See you at the top!"
  },
  {
    "objectID": "posts/llm-bootcamp/index.html",
    "href": "posts/llm-bootcamp/index.html",
    "title": "🌉 A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications",
    "section": "",
    "text": "The view of the Golden Gate Bridge reminded me of the limitless possibilities that AI and LLMs can bring to our world. Image by author"
  },
  {
    "objectID": "posts/llm-bootcamp/index.html#table-of-contents",
    "href": "posts/llm-bootcamp/index.html#table-of-contents",
    "title": "🌉 A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nIntroduction\nThe Rapid Transformation of AI-Powered Apps\nAddressing the Critical Questions\n\nAre there any good open-source LLMs?\nWhat is my moat if I rely on OpenAI APIs?\nIs Prompt Engineering some kind of sick joke?\nHow can I gather and use feedback from users?\nShould I be able to code a Transformer from scratch?\nHow exactly am I supposed to test these damn things?\n\nConclusion"
  },
  {
    "objectID": "posts/llm-bootcamp/index.html#introduction",
    "href": "posts/llm-bootcamp/index.html#introduction",
    "title": "🌉 A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications",
    "section": "Introduction",
    "text": "Introduction\n\nThe world of technology is currently undergoing a monumental transformation, and my participation in the LLM Bootcamp was nothing short of enlightening from my perspective. The potential of AI-powered applications is causing a ripple effect in the software industry. I learned about techniques, tools, and vendors shaping the future of AI at a well-organized bootcamp where I gained valuable insights and connections.\nWhile it’s nearly impossible to encapsulate everything I learned in a single blog post, I will attempt to highlight some of the key takeaways from this enriching experience."
  },
  {
    "objectID": "posts/llm-bootcamp/index.html#the-rapid-transformation-of-ai-powered-apps",
    "href": "posts/llm-bootcamp/index.html#the-rapid-transformation-of-ai-powered-apps",
    "title": "🌉 A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications",
    "section": "The Rapid Transformation of AI-Powered Apps",
    "text": "The Rapid Transformation of AI-Powered Apps\n\nBefore the advent of Large Language Models (LLMs), ideas often bottlenecked on the process of training models from scratch and faced further bottlenecks in scalable deployment. However, with the availability of pretrained, promptable LLM models and APIs, it is now possible to configure and serve users in just an hour. This shift has led to an entirely new ecosystem that even experienced ML professionals are striving to understand."
  },
  {
    "objectID": "posts/llm-bootcamp/index.html#addressing-the-critical-questions",
    "href": "posts/llm-bootcamp/index.html#addressing-the-critical-questions",
    "title": "🌉 A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications",
    "section": "Addressing the Critical Questions",
    "text": "Addressing the Critical Questions\n\nAs engineers delve into this new world, several questions arise. During the LLM Bootcamp, I explored answers to some of these pressing questions:\n\nAre there any good open-source LLMs?\n\nYes, there are several open-source LLMs available. These models serve as a foundation for researchers and developers to build upon, experiment, and create new AI applications.\nJosh Tobin had great slides on this topic, which will be release soon, but in essence it is more detailed version of the below figure\n\n\n\nComparisons of LLMs from LightningAI\n\n\nSelecting the best model for your use case depends on various factors, including:\n\nOut-of-the-box quality for your task\nInference speed / latency\nCost\nFine-tuneability / extensibility\nData security and license permissibility\nFor most use cases, GPT-4 is an excellent starting point.\n\nProprietary models often offer superior quality and are more suitable for commercial use due to licensing constraints. These models can provide businesses with a competitive edge, as they are specifically designed and optimized for certain tasks and industries. Also, they come with the advantage of reduced infrastructure overhead when compared to serving open-source models.\nIn the past, concerns regarding broad copyright ownership over any output generated by these models have made businesses hesitant to adopt them. However, advancements in technology, such as the Azure OpenAI Service, have begun to address these concerns. OpenAI now empowers users by offering services that enable the use of proprietary models without worrying about copyright issues. This makes it easier for businesses to leverage the benefits of proprietary models while also protecting their intellectual property.\nSome disclaimers on this topic:\n\nSecurity concerns: While the Azure OpenAI Service has made strides in alleviating copyright concerns, it’s important to acknowledge that security concerns still remain. As with any cloud-based service, there is always the potential risk of data breaches and unauthorized access to sensitive information. To mitigate these risks, OpenAI and Azure have implemented rigorous security measures and protocols, such as data encryption and access control. However, businesses should still exercise caution and perform their own due diligence when evaluating the suitability of these services for their needs.\nNo Service Level Agreements (SLAs) yet: There is currently no commitment from OpenAI regarding when SLAs will be provided for these APIs. SLAs typically offer guarantees on service availability, performance, and support response times, which can be crucial for businesses that depend on these services for mission-critical applications. Without SLAs in place, businesses may find it challenging to assess the reliability and stability of these APIs, making it difficult to plan and budget for their integration into commercial projects.\nUpcoming regulations: Additionally, it’s worth noting that new regulations are on the horizon that could impact the use of proprietary models in certain regions. The European Union is currently working on the Artificial Intelligence Act (AI Act), which aims to create a legal framework for AI systems, including proprietary models. This legislation may introduce new requirements for businesses using AI, such as transparency, accountability, and data protection. As these regulations evolve, businesses should stay informed and adapt their AI strategies accordingly.\n\nOn the other hand, open-source models are a more fitting choice if you require extensive customization and want to ensure data security. They provide greater flexibility in terms of modification and adaptation to meet specific project requirements.\nAdding to the previous discussion on selecting the best LLM for your use case, here are some recommendations for most projects:\n\nStart with GPT-4: This will help you develop a proof of concept to assess the feasibility of your task, similar to prototyping in Python.\nConsider “downsizing” if cost or latency is a factor: GPT-3.5 and Claude are both good choices with comparable performance. For even faster and cheaper alternatives, explore options provided by any LLM vendor, with Anthropic’s offering being the most “modern” choice.\nOpt for Cohere if you need to fine-tune your model: Cohere allows greater flexibility in adjusting the model to better suit your specific needs.\nUse open-source models only when absolutely necessary: Although the open-source landscape is evolving rapidly and will likely become a more viable option in the future, for now, it’s advisable to use these models only if they are essential for your project.\n\nPersonally I find it quite distressing that LLAMA paper results could not be reproduced.\nBy considering these recommendations, you can make an informed decision about which LLM is most suitable for your project, balancing factors such as cost, performance, customization, and data security.\nWe can expect GPT-3.5 quality to be available in open-source models until the end of 2023. \n\n\nWhat is my moat if I rely on OpenAI APIs?\n\nUtilizing OpenAI APIs provides you with access to state-of-the-art AI technology and regular model updates, which can help you maintain a competitive edge in the market. However, your competitive advantage, or “moat” comes from your unique implementation of the technology and the value-added services you deliver to your customers.\nFor instance context plays a crucial role in providing LLMs with unique and up-to-date information, but its capacity is limited. To make the most of this limited context, consider augmenting the language model through various methods:\n\nAugment with a larger corpus: Leverage additional data sources to enrich the context and improve the AI’s understanding of your specific domain or use case.\nAugment with more LLM calls: Make multiple calls to the LLM with different inputs or parameters to generate a diverse range of responses, which can then be combined or refined to produce a more accurate output.\nAugment with external sources: Integrate information from other sources, such as databases, APIs, or domain-specific knowledge bases, to enhance the context and further tailor the AI’s output to your needs.\n\nBy effectively combining OpenAI APIs with these augmentation strategies, you can create unique, high-value solutions that differentiate your offerings from competitors and strengthen your competitive advantage.\n\n\nIs Prompt Engineering some kind of sick joke?\n\nIn the context of AI and LLMs, a “prompt” refers to the text input given to a language model. “Prompt engineering” is the skillful process of crafting that input text to achieve desired results from the model.\nLilian Weng has a great blogpost about it here, and she notes that most papers on prompt engineering are tricks that can be explained in a few sentences.\nWhile the term “prompt engineering” may sound whimsical or perplexing, it is a crucial aspect of working with LLMs. By carefully designing prompts, you can harness the power of language models to generate valuable insights, creative ideas, and solutions to complex problems.\nPrompts can be thought of as gateways to the vast knowledge and capabilities of AI language models.\nThey allow you to tap into the potential of these sophisticated systems, but only when you adhere to certain guidelines and best practices.\nDespite the seemingly mysterious nature of prompts, they are an essential tool in the AI practitioner’s toolkit, and learning how to master prompt engineering will enable you to unlock the full potential of LLMs in various applications.\n\n\nHow can I gather and use feedback from users?\n\nUser feedback is crucial for improving your AI-powered applications. To gather and use feedback effectively, consider implementing feedback loops, in-app surveys, and user testing to gain insights into user preferences and any potential shortcomings of the AI.\nOne approach to incorporating user feedback is to ask an LLM whether the new answer addresses the feedback provided for the old answer. Aim for low-friction, high-signal feedback methods that easily integrate into the user’s workflow, such as the Accept changes or Thumbs up/down patterns. Longer-form feedback also plays a role in refining AI performance.\nIdentify themes in user feedback that the model does not address, which are often discovered by humans. Adjust the prompt to account for these themes through prompt engineering or by changing the context. The automation of this process is still an open question.\n\n\nShould I be able to code a Transformer from scratch?\n\nWhile it’s not mandatory, having a deep understanding of the underlying architecture, such as Transformers, can help you better utilize LLMs and troubleshoot issues. However, focusing on practical applications and use cases is often more valuable than delving solely into theoretical aspects.\nFor example, during a fireside chat, Peter Welinder, VP of Product and Partnerships at OpenAI, mentioned that when they first built the API, the inference was slow, but in just three months, they were able to improve the inference speed by 100x. This example illustrates how understanding the underlying architecture, combined with using tools like triton, can help you enhance the performance of your AI applications.\nFor instance, you might need to fine-tune LLMs in certain situations. Here are some recommendations:\n\nUsing GPT-4 might eliminate the need for fine-tuning in most cases.\nReasons to consider fine-tuning:\n\nYou need to deploy smaller models due to resource constraints.\nYou have a large amount of data, and retrieval-based approaches are not performing well.\n\nLow-rank updates or parameter-efficient tuning techniques can make fine-tuning more accessible, allowing you to optimize the LLM for specific use cases without in-depth knowledge of Transformer coding.\n\nAlso this is a there is recent great survey on parameter-efficient tuning.\nUltimately, striking a balance between understanding the underlying principles and focusing on practical applications will empower you to make the most of LLMs in your projects.\n\n\nHow exactly am I supposed to test these damn things?\n\nTesting LLMs can be challenging, but it is essential to ensure the quality of your AI applications. Employ a combination of manual and automated testing, focus on edge cases, and collaborate with domain experts to validate the AI’s output for accuracy and relevance.\nAs you may already know, LLMs are prone to making mistakes, and improving one aspect of your model may inadvertently compromise another. If people rely on your model, they trust you to maintain performance on their task. Since LLMs are trained on the internet, there is always a risk of drift, and qualitative success can be hard to measure. Additionally, diversity of behaviors means aggregate metrics may not be sufficient.\nTest coverage and distribution shift are closely related concepts. Distribution shift measures how far the test distribution is from a reference distribution and is used to assess data changes. Test coverage measures how well your evaluation set covers your production data and is used to find more helpful evaluation data.\nA key idea in LLM testing is using one LLM to evaluate another. This enables automatic evaluation, which can unlock parallel experimentation. However, you should still conduct some manual checks. Types of feedback to consider include thumbs up/down, written feedback, and corrected answers.\nBy implementing a comprehensive testing strategy that combines automated and manual testing, as well as incorporating feedback mechanisms, you can ensure the quality and performance of your AI applications remain high."
  },
  {
    "objectID": "posts/llm-bootcamp/index.html#conclusion",
    "href": "posts/llm-bootcamp/index.html#conclusion",
    "title": "🌉 A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhile the insights shared in this blog post are valuable today, it’s essential to acknowledge that the landscape is continually changing. And as we continue to witness the growth and impact of LLMs and AI in general, it’s crucial to remain agile and open to new ideas. The key takeaways from the LLM Bootcamp and related discussions serve as a stepping stone for understanding the current state of the field, but it’s up to you to keep pushing the boundaries and exploring new ways to harness the power of these transformative technologies.\nIn addition to this post, I have also compiled a separate write-up on a panel discussion focused on Building a Sustainable Business which provides insights on how to navigate the challenges and opportunities in this burgeoning industry.\nIn conclusion, I want to express my heartfelt thanks to all the readers who have taken the time to read and engage with this blog post. Your interest and curiosity motivate me to keep sharing my experiences and learnings in this rapidly evolving field. I appreciate your support, and I look forward to sharing more insights with you in the future!"
  },
  {
    "objectID": "posts/llm-bootcamp/index.html#acknowledgements",
    "href": "posts/llm-bootcamp/index.html#acknowledgements",
    "title": "🌉 A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Eugene Yan for comments on the draft."
  },
  {
    "objectID": "posts/llm-bootcamp/index.html#citation",
    "href": "posts/llm-bootcamp/index.html#citation",
    "title": "🌉 A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications",
    "section": "Citation",
    "text": "Citation\nCited as:\n@misc{muhtasham2023llm,\n  title={A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications},\n  author={Muhtasham, Oblokulov},\n  journal={muhtasham.github.io},\n  year={2023},\n  month={Apr},\n  url={https://muhtasham.github.io/blog/posts/llm-bootcamp/}\n}"
  },
  {
    "objectID": "posts/neurips23/index.html",
    "href": "posts/neurips23/index.html",
    "title": "🎷 NeurIPS 2023: Trends, Talks, and Technologies",
    "section": "",
    "text": "“You find as a writer that there are certain spots on the planet where you write better than others, and I believe in that. And New Orleans is one of them.” — Jimmy Buffet\n\n\nI don’t think the above quote applies to me, but nevertheless wanted to share my first NeurIPS experience. To start with it was an exhilarating whirlwind, packed with a wealth of knowledge and insights. As I reflect on this incredible experience, the sheer magnitude of the event still astounds me: a staggering 3,500+ papers were accepted, and the conference welcomed over 15,000 attendees. This blog post is my attempt to distill the essence of this unforgettable event and share some of the most striking highlights.\n\nA Tweet That Resonated\nIn the whirl of discussions and discoveries at NeurIPS 2023, a particular tweet by @Thom_Wolf, and shared by Christopher Manning, stood out for its poignant encapsulation of a key trend at the conference. It speaks volumes about the shifting landscape in the world of AI research:\n\n\n.@Thom_Wolf: “Academia is back as we saw at NeurIPS 2023. With many private and open-source labs closing the doors on publishing their results and data, academia rises again in visibility and is shining with many impactful papers in 2023 and exciting new work coming.” https://t.co/tlOiuLOJhC\n\n— Christopher Manning (@chrmanning) December 18, 2023\n\n\n\n\nInsights and Interactions\n\nMath Meets AI\n\nMath Field Medalists and LLMs: One of the most fascinating aspects of NeurIPS 2023 was the intersection of mathematics and large language models (LLMs),\n\n\n\nA Look at the Conference’s Heart\n\nA Glimpse into the Latent Space:The conference offered a deep dive into its rich content through an array of papers and posters. For those who couldn’t attend or wish to revisit the highlights, here are some curated links that offer a window into the latent space of AI innovations::\n\nCheck out the Papers.\nHave a look at the Posters.\n\n\n\n\nMeeting the Teams Behind the Tech\n\nGemini and Mistral Teams: Getting to meet the people who work on Gemini and Mistral was a highlight for me. Talking with them about how they build their tech was really insightful.\n\n\n\nStars of the AI World\n\nCelebrity Encounters: The conference had a lot of famous people from the AI field. It was exciting to see them in person.\n\n\n\n\nSharing moment with Tri Dao, Dylan Patel, Teknium and co at Open-Source party organized by Mistral\n\n\n\n\nBusiness Interest in AI\n\nVC Presence: I noticed more business people, like venture capitalists, at the conference this year. This shows that more businesses are getting interested in AI and its possibilities.\n\n\n\n\nAlso attended event with Daniel Gross of AI Grant, where we discussed shelf life of transformers and future AI products\n\n\n\n\n\nKey Insights from Christopher Ré’s NeurIPS23 Keynote\nChristopher Ré gave a standout keynote at NeurIPS23, focusing on the building blocks for foundation models. You can check out his presentation here (pptx) and watch the video here.\n\nIntegrating Classic Database Techniques\n\nFlashAttention: He talked about how ideas from old-school databases are now being used in Machine Learning systems. For instance, FlashAttention is like a method from classical databases but speeds up LLM processing by 6-10 times without losing accuracy, and it also uses 10-20 times less memory.\n\n\n\nApproaches to Handling Long Sequences\n\nImproving Hardware for Transformers: Technologies like FlashAttention, MQA, and Blockwise Attention are making transformers better and faster.\nApproximate Attention: This is about making ML models pay ‘just enough’ attention without overdoing it. Some examples are Reformer, Performer, and Linformer.\nUsing Alternatives to Traditional Models: He mentioned using different kinds of models, like RNNs, and applying signal processing techniques – a common method in machine learning – to improve them. Models like S4, Hyena, and CKConv are part of this approach.\n\n\n\nAttention as Signal Processing\nChristopher Ré discussed a fresh way of looking at ML systems: treating attention in transformers like handling signals. This involves taking in information, processing it, and outputting it in a way that’s useful. A key part of this is making sure the system stays stable and reliable.\n\n\nTakeaways\nMy main takeaway from his talk is that foundation models are just starting out. There’s a lot of challenges ahead, but there’s also a lot of potential for making ML systems more powerful and efficient. This includes better inference, handling more data at once, working with new types of data, and building better architectures for ML systems.\nFor those interested in the technical details, you can explore their work on GitHub for SysAI building blocks.\n\n\n\nEmerging Trends to Watch\n\nLeading Developments\n\nState-Space Models: These are gaining traction and could shape future directions.\nGrounded Agents: Agents like those from MultiOn and Minion, demonstrating practical applications beyond theoretical concepts.\n\n\n\nConcluding Thoughts on NeurIPS 2023\nTo wrap up, NeurIPS 2023 went beyond a typical conference. It was a vibrant showcase of evolving technologies, offering insights that are not only academically significant but also practically influential in shaping our interaction with technology."
  },
  {
    "objectID": "posts/sama-fireside-chat/index.html#introduction",
    "href": "posts/sama-fireside-chat/index.html#introduction",
    "title": "🔮 Navigating the AI Landscape: My Takeaways from Sam Altman’s Discussion",
    "section": "Introduction",
    "text": "Introduction\nAt a recent event at my alma mater, TU Munich featuring Sam Altman, CEO of OpenAI, attendees had the opportunity to delve into a range of topics. From AI regulation, the release of future OpenAI models, to the importance of the user interface in technology, the event was a wellspring of insights. But before I share my key takeaways, let me tell you about my intriguing journey to the event.\nTickets for this event were sold out within 20 minutes. However, as adage goes\n\nModern problems, require modern solutions.\n\nSo with some innovation and the assistance of ChatGPT, I secured a ticket.\n\n\n\nGuess what? Used chatGPT to get access to event featuring @samaFirst, generated JavaScript code to check availability of new tickets, did not succeed. Crafted a killer press representative email for @MunichNlp and got it! 🎫Big shoutout to @DLDConference for organizing this pic.twitter.com/k6GtTZ0TJo\n\n— Muhtasham Oblokulov (@Muhtasham9) May 25, 2023"
  },
  {
    "objectID": "posts/sama-fireside-chat/index.html#ai-regulation-a-pragmatic-view",
    "href": "posts/sama-fireside-chat/index.html#ai-regulation-a-pragmatic-view",
    "title": "🔮 Navigating the AI Landscape: My Takeaways from Sam Altman’s Discussion",
    "section": "AI Regulation: A Pragmatic View",
    "text": "AI Regulation: A Pragmatic View\nAltman expressed his support for AI regulation, but emphasized the necessity for such regulations to be implemented correctly, and not just for the sake of regulation. His belief is that these should come into effect above a specific capability threshold, implying a nuanced approach that doesn’t stifle innovation but still ensures safety and ethical use of AI technology."
  },
  {
    "objectID": "posts/sama-fireside-chat/index.html#the-role-of-ai-tools-in-research",
    "href": "posts/sama-fireside-chat/index.html#the-role-of-ai-tools-in-research",
    "title": "🔮 Navigating the AI Landscape: My Takeaways from Sam Altman’s Discussion",
    "section": "The Role of AI Tools in Research",
    "text": "The Role of AI Tools in Research\nOpenAI engineers and researchers are utilizing tools such as Copilot in their work. Copilot, which builds on the capability of the codex model by OpenAI, exemplifies how AI has become instrumental in driving innovative research, enabling researchers to augment their abilities and achieve more than was previously possible."
  },
  {
    "objectID": "posts/sama-fireside-chat/index.html#the-importance-of-user-interface",
    "href": "posts/sama-fireside-chat/index.html#the-importance-of-user-interface",
    "title": "🔮 Navigating the AI Landscape: My Takeaways from Sam Altman’s Discussion",
    "section": "The Importance of User Interface",
    "text": "The Importance of User Interface\nAltman stressed on the significance of the User Interface (UI) in technology. He posited that natural language turns out to be an excellent UI for humans, reinforcing the premise that technology should be built to align with human behavior and preferences, rather than the other way around. This underlines the importance of intuitive, user-friendly design in making complex AI systems accessible and easy to use.\n\n\n\nPlayfully tossing the Worldcoin ball and contemplating its UI while sharing a moment with Sam Altman"
  },
  {
    "objectID": "posts/sama-fireside-chat/index.html#openais-future-plans",
    "href": "posts/sama-fireside-chat/index.html#openais-future-plans",
    "title": "🔮 Navigating the AI Landscape: My Takeaways from Sam Altman’s Discussion",
    "section": "OpenAI’s Future Plans",
    "text": "OpenAI’s Future Plans\nAltman revealed plans to release more open-source models in the future, however, he clarified that this would not extend to the upcoming GPT-5 etc. Despite being a non-profit, Altman acknowledged the significance of the open-source market, indicating OpenAI’s continued commitment to the wider community.\nHe also highlighted the importance of getting the 200 small things right and jointly optimizing across the stack. These ‘small things’ could refer to everything from the model’s architecture, to training data, to the end application UI, demonstrating the meticulous nature of OpenAI’s approach."
  },
  {
    "objectID": "posts/sama-fireside-chat/index.html#whats-next-after-large-language-models-llms",
    "href": "posts/sama-fireside-chat/index.html#whats-next-after-large-language-models-llms",
    "title": "🔮 Navigating the AI Landscape: My Takeaways from Sam Altman’s Discussion",
    "section": "What’s Next After Large Language Models (LLMs)?",
    "text": "What’s Next After Large Language Models (LLMs)?\nThe future of AI, according to Altman, involves looking for the next big thing after LLMs. While the specifics were not discussed, the pursuit of groundbreaking innovation is clearly at the core of OpenAI’s mission."
  },
  {
    "objectID": "posts/sama-fireside-chat/index.html#views-of-llms",
    "href": "posts/sama-fireside-chat/index.html#views-of-llms",
    "title": "🔮 Navigating the AI Landscape: My Takeaways from Sam Altman’s Discussion",
    "section": "Views of LLMs",
    "text": "Views of LLMs\nA noteworthy insight from Altman was his characterization of LLMs. He emphasized that people should not think of LLMs as databases, they are not updated in real-time, but rather they should be thought of as reasoning engines. With the help of certain retrievers, these LLMs can address the issue of up-to-date knowledge, suggesting the future development of LLMs may involve closer integration with real-time data sources.\nAltman also mentioned prompt injection for the first time in a public. While also noting the possibility that LLMs might not be suited for certain purposes, he assured attendees that new approaches are being worked on, indicating exciting advancements on the horizon."
  },
  {
    "objectID": "posts/sama-fireside-chat/index.html#conclusion",
    "href": "posts/sama-fireside-chat/index.html#conclusion",
    "title": "🔮 Navigating the AI Landscape: My Takeaways from Sam Altman’s Discussion",
    "section": "Conclusion",
    "text": "Conclusion\nSam Altman’s talk offered valuable perspectives on the path of AI, emphasizing effective AI regulations, the importance of user-friendly interfaces, and the continued commitment to the open-source ecosystem. He underscored the role of Large Language Models (LLMs) as reasoning engines and touched upon their future development. In essence, Altman’s insights paint a future of AI that’s effectively regulated, user-centric, and collaborative, while continuously evolving to meet real-time information needs. His conversation reinforces the dynamic nature of AI and the importance of responsible stewardship as we navigate this rapidly evolving landscape.\nThanks for reading! If you enjoyed this article, please consider sharing it on social media. You can also follow me on Twitter for more content like this.\nFor a more immersive experience, you can watch a segment of Sam Altman’s talk in the video below:"
  },
  {
    "objectID": "posts/iclr23/index.html",
    "href": "posts/iclr23/index.html",
    "title": "🖼️ Exploring the Latest Advancements in Transfer Learning: A Summary of ICLR’23 Transfer Learning-Related Papers",
    "section": "",
    "text": "Intro\nThe International Conference on Learning Representations (ICLR) is one of the top conferences in the field of machine learning, and this year’s conference (ICLR 23) features several papers on the topic of transfer learning. Transfer learning is a technique that allows a model trained on one task to be applied to a different but related task, potentially improving performance and reducing the amount of data and computation required. Here are a few papers on transfer learning that are worth keeping an eye on at ICLR 23:\n\n\n\nPapers\n\nLearning Uncertainty for Unknown Domains with Zero-Target-Assumption\n\nTL;DR: New framework that maximizes information uncertainty measured by entropy to select training data in NLP.\n\nThis paper introduces a new framework called Maximum-Entropy Rewarded Reinforcement Learning (MERRL) for selecting training data for more accurate Natural Language Processing (NLP). The authors argue that conventional data selection methods, which select training samples based on test domain knowledge and not on real-life data, frequently fail in unknown domains like patent and Twitter. MERRL addresses this issue by selecting training samples that maximize information uncertainty measured by entropy, including observation entropy like empirical Shannon entropy, Min-entropy, R’enyi entropy, and prediction entropy using mutual information. The authors show that their MERRL framework using regularized A2C and SAC achieves significant improvements in language modeling, sentiment analysis, and named entity recognition over various domains, demonstrating strong generalization power on unknown test sets.\nAs an Electronics Engineering background, I expresses genuine affinity to Shannon and his theory, which is the foundation of information theory and a powerful tool for understanding the limits of communication and computation.\n\n\nRepresentational Dissimilarity Metric Spaces for Stochastic Neural Networks\n\nTL;DR: Representational dissimilarity metrics that account for noise geometry in biological and artificial neural responses.\n\nThis paper addresses the problem of quantifying similarity between neural representations, such as hidden layer activation vectors, in deep learning and neuroscience research. Existing methods for comparing deterministic or trial-averaged responses ignore the scale and geometric structure of noise, which are important in neural computation. To address this, the authors propose a new approach that generalizes previously proposed shape metrics to quantify differences in stochastic representations. These new distances can be used for supervised and unsupervised analyses and are practical for large-scale data. The authors show that this approach provides insights that cannot be measured with existing metrics, such as being able to more accurately predict certain network attributes from its position in stochastic shape space.\n\n\nTowards Estimating Transferability using Hard Subsets\n\nTL;DR: Authors propose HASTE, a strategy that ensures better transferability estimation using just a hard subset of target data.\n\nThis paper presents a new strategy called HASTE (HArd Subset TransfErability) for estimating the transferability of a source model to a particular target task, using only a harder subset of target data. HASTE introduces two techniques to identify harder subsets, one class-agnostic and another class-specific. It can be used with any existing transferability metric to improve their reliability. The authors analyze the relation between HASTE and the optimal average log-likelihood and negative conditional entropy, and empirically validate theoretical bounds. The results of experiments across multiple source model architectures, target datasets, and transfer learning tasks show that HASTE-modified metrics are consistently better or on par with the state-of-the-art transferability metrics.\n\n\nThe Role of Pre-training Data in Transfer Learning\n\nTL;DR: We investigate the role of pretraining distribution, data curation, size, and loss and downstream transfer learning\n\nThis paper examines the effect of the pre-training distribution on transfer learning in the context of image classification. The study finds that the pre-training dataset is initially important for low-shot transfer, but the difference between distributions is reduced as more data is available for fine-tuning. Fine-tuning still outperforms training from scratch. The study also investigates the effect of dataset size, observing that larger pre-training datasets lead to better accuracy, but the largest difference in accuracy is seen in the few-shot regime. Additionally, the study looks at the effect of pre-training method, and finds that image-image contrastive pre-training leads to better transfer accuracy compared to language-image contrastive pre-training.\n\n\n\nSome thougts\nIt is unfortunate that the double-blind review process, while well-intentioned, can sometimes result in good research being overlooked. The process, designed to prevent bias in the selection of papers, can also make it difficult for reviewers to properly assess the work of researchers from underrepresented groups or from institutions with less prestige.\nAdditionally, for the researchers whose work was not accepted at this year’s ICLR conference, it can be disheartening and discouraging. But it’s important to remember that the review process is highly competitive and that getting a paper accepted at a top conference like ICLR is a significant accomplishment. And for those whose work was not accepted, it doesn’t mean that their research is not valuable or that they are not good researchers. It is also important to note that there are many other conferences, journals and outlets to present the research and get the recognition it deserves.\nIt’s also important to remember that rejection is a common experience in any field, especially in research. It’s a part of the process of discovery and innovation. It’s important to keep pushing forward, to continue to conduct valuable research, and to keep submitting to conferences and journals. It’s also important to keep an open mind and to look for feedback and constructive criticism. And most importantly, don’t give up.\nIn short, while the double-blind review process has its drawbacks, it’s important to remember that rejection is a part of the process and that good research can come from any institution or researcher. And for those whose work was not accepted, keep pushing forward and don’t give up. Here is some advice from Jonathan Frankle, the author of the infamous “The Lottery Ticket Hypothesis” paper:\n\n\nTime for my usual refrain: Most papers weren't accepted to ICLR, and don't let Twitter fool you into thinking otherwise. Plenty of smart people and great papers didn't get the outcome they wanted, and you're in very good company if that's you right now.\n\n— Jonathan Frankle (@jefrankle) January 22, 2023\n\n\n\n\nConlusion\nOverall, transfer learning is a powerful technique that allows models to be applied to different but related tasks, potentially improving performance and reducing the amount of data and computation required. The papers discussed in this blog post highlight some of the latest research in transfer learning, including methods for transferring knowledge between language and images, between different modalities, and for graph-structured data.\nIt is clear that transfer learning is an active and rapidly growing area of research, and we can expect to see many more exciting developments in the coming years. We look forward to seeing the outcome of these papers and more at ICLR 23.\nIn conclusion, transfer learning has many practical applications and these papers give a glimpse of the future possibilities of transfer learning, it is a promising area of research and have a lot of room for improvement. These papers will give an insight into the latest developments in the field and open up the doors for new research opportunities.\n\nIf you liked this article, you can also find me on Twitter and LinkedIn where I share more content related to machine learning and AI."
  },
  {
    "objectID": "posts/explore-context/index.html",
    "href": "posts/explore-context/index.html",
    "title": "📈 Exploring Ways to Extend Context Length in Transformers",
    "section": "",
    "text": "Diving deep into the intricacies of large language models (LLMs), one hurdle quickly becomes evident: the context length limitation. While many recognize its implications, the question remains: how can we train models on more extensive context lengths without falling into the predicament of quadratic complexity?\n\n\n\n\n\n\nNoteQuadratic Complexi-what?\n\n\n\n\n\nTransformers, especially their self-attention mechanism, have a quadratic computational complexity in relation to the input sequence length. It’s like that one friend who has to say hi to everyone at a party – each token in the sequence has to interact with every other token.\nFor an input sequence of length (N), there are (N^2) interactions. So, if (N) is the number of guests at the party, imagine the time it’d take if every guest had to greet every other guest! This results in a computational cost proportional to (N^2). This means that as the sequence length (or party size) doubles, the computational cost (or time to greet everyone) quadruples. And while it’s great to be social, this rapid growth in computational demands becomes a major party pooper for very long sequences, posing challenges in tasks where extended context is crucial.\n\n\n\nTo put things in perspective and provide a clearer understanding, the following chart compares the context lengths of various models. This visualization can help elucidate the inherent limitations and possibilities of different Transformer-based models.\n\n\n\nHere is a comparison of the context and how much it is human terms, Source: LLM Bootcamp\n\n\nThe context length of the base model varies, with Falcon allowing up to 2K tokens and LLAMA2 accommodating up to 4K tokens, but we also have models like MPT trained with Alibi attention that can in theory support up to infinite context lengths."
  },
  {
    "objectID": "posts/explore-context/index.html#introduction",
    "href": "posts/explore-context/index.html#introduction",
    "title": "📈 Exploring Ways to Extend Context Length in Transformers",
    "section": "",
    "text": "Diving deep into the intricacies of large language models (LLMs), one hurdle quickly becomes evident: the context length limitation. While many recognize its implications, the question remains: how can we train models on more extensive context lengths without falling into the predicament of quadratic complexity?\n\n\n\n\n\n\nNoteQuadratic Complexi-what?\n\n\n\n\n\nTransformers, especially their self-attention mechanism, have a quadratic computational complexity in relation to the input sequence length. It’s like that one friend who has to say hi to everyone at a party – each token in the sequence has to interact with every other token.\nFor an input sequence of length (N), there are (N^2) interactions. So, if (N) is the number of guests at the party, imagine the time it’d take if every guest had to greet every other guest! This results in a computational cost proportional to (N^2). This means that as the sequence length (or party size) doubles, the computational cost (or time to greet everyone) quadruples. And while it’s great to be social, this rapid growth in computational demands becomes a major party pooper for very long sequences, posing challenges in tasks where extended context is crucial.\n\n\n\nTo put things in perspective and provide a clearer understanding, the following chart compares the context lengths of various models. This visualization can help elucidate the inherent limitations and possibilities of different Transformer-based models.\n\n\n\nHere is a comparison of the context and how much it is human terms, Source: LLM Bootcamp\n\n\nThe context length of the base model varies, with Falcon allowing up to 2K tokens and LLAMA2 accommodating up to 4K tokens, but we also have models like MPT trained with Alibi attention that can in theory support up to infinite context lengths."
  },
  {
    "objectID": "posts/explore-context/index.html#which-parts-of-transformer-architecture-can-we-tinker-with-to-extend-context-length",
    "href": "posts/explore-context/index.html#which-parts-of-transformer-architecture-can-we-tinker-with-to-extend-context-length",
    "title": "📈 Exploring Ways to Extend Context Length in Transformers",
    "section": "Which parts of Transformer architecture can we tinker with to extend context length?",
    "text": "Which parts of Transformer architecture can we tinker with to extend context length?\nThe Transformers (not the Autobots, mind you), have 5 main parts:\n\nTokenization\nEmbedding\nPositional encoding\nTransformer block (several of these)\nSoftmax\n\n\n\n\nThe architecture of a Transformer model, Source: LLM University\n\n\nTypically, to extend the context length in Transformers, we often tweak the positional encoding and/or the attention mechanism. However, considering that the attention mechanism is the most computationally demanding part of the Transformer, and given the growing “attention” (pun intended) towards open-access models like LLAMA2 and Falcon, it’s beneficial to delve deeper into the nuances of positional encoding. Specifically, let’s focus on the Rotary Positional Encoding (RoPE).\n\n\n\n\n\n\nCautionPositional encoding\n\n\n\n\n\nTransformers process input in parallel, not sequentially. Hence, after tokenization and embedding extraction, we need a way to maintain the order of words. This is achieved through positional encoding. It adds a positional vector to each word to keep track of word positions, ensuring the context is not lost in longer sequences.\n\n\n\nHere’s a bird’s-eye view of positional encoding. Think of it as a GPS for words, Source: LLM University"
  },
  {
    "objectID": "posts/explore-context/index.html#the-linear-scaling-trick",
    "href": "posts/explore-context/index.html#the-linear-scaling-trick",
    "title": "📈 Exploring Ways to Extend Context Length in Transformers",
    "section": "The Linear Scaling Trick",
    "text": "The Linear Scaling Trick\n\n\n\n\n\n\nTipTL;DR\n\n\n\n\n\nSimple but needs minimal fine-tuning to observe the best results.\n\n\n\nRotary position embeddings (RoPE) used by LLAMA and Falcon are a variant of the positional encoding and they have very nice mathematical properties, but it turns out they are really bad at extrapolating, But turns out we can linearly interpolate by simply downscaling and dividing the position index by a scaling factor.\nFun fact: this method of Linear scaling was independently and simultaneously discovered by the Reddit user /u/kaiokendev and the Meta team\n\n\n\nAn illustration of our Position Interpolation method … Lower left illustrates Position Interpolation where we downscale the position indices (blue and green dots) themselves from [0, 4096] to [0, 2048] to force them to reside in the pretrained range Source: Extending Context Window of Large Language Models via Positional Interpolation"
  },
  {
    "objectID": "posts/explore-context/index.html#dynamic-scaling-trick",
    "href": "posts/explore-context/index.html#dynamic-scaling-trick",
    "title": "📈 Exploring Ways to Extend Context Length in Transformers",
    "section": "Dynamic Scaling Trick",
    "text": "Dynamic Scaling Trick\n\n\n\n\n\n\nTipTL;DR\n\n\n\n\n\nA more advanced technique that performs well even without fine-tuning but can benefit further from it.\n\n\n\nThe simple linear interpolation in RoPE’s Fourier space isn’t the most effective way to distribute information across tokens. Drawing from the Neural Tangent Kernel (NTK) theory, u/bloc97/ suggested a dynamic scaling trick. This approach changes the “rotation” speed of each RoPE dimension relative to the others, ensuring distinctiveness of positions even under extreme stretching.\n\n\n\n\n\n\nTipFourier Space and its Significance\n\n\n\n\n\nImagine you’re listening to a song with multiple instruments playing simultaneously. To the human ear, it’s a blend of sounds, but what if you wanted to understand each instrument’s contribution? The Fourier transform is like a magical tool that separates the song into individual instrument notes, helping you see which instrument plays at what frequency and when.\nFourier space, derived from this transformation, is essential in understanding the underlying frequencies that make up a signal. In the context of machine learning and Transformers, it’s like separating intertwined data patterns, offering insights and optimizations not evident when looking at the combined sequence.\n\n\n\nBenefits of the dynamic scaling trick:\n\nEfficient without the need for fine-tuning\nMaintains performance even with short sequences\nScalable to very long sequences under a fixed parameterization\n\nIf you’re considering the quantization of RoPe-scaled models, it’s essential to choose the right technique. For a deeper understanding of the implications of static and dynamic quantization, refer to the section below.\n\n\n\n\n\n\nWarningStatic vs. Dynamic Quantization and Performance Drop with RoPe Scaling\n\n\n\n\n\nQuantization, in the context of deep learning, refers to the process of reducing the number of bits that represent the weights and biases of a model. This reduction aids in deploying models in resource-constrained environments like mobile devices, but it can also impact the model’s performance.\nStatic Quantization involves converting the entire model to a quantized format at once, based on a single calibration dataset. The primary disadvantage of static quantization with RoPe scaling is its rigidity. Since the quantization process doesn’t consider variations in different parts of the model or the specific nature of RoPe’s behavior, it can lead to significant information loss. This results in a performance drop, especially when the model is scaled to handle longer sequences.\nDynamic Quantization, on the other hand, quantizes layers of the model on-the-fly during model inference. It adjusts the quantization parameters dynamically based on the input data, making it more flexible and adaptive. This adaptability ensures that the quantization process aligns better with RoPe scaling methods, preserving essential details and maintaining performance.\nAlso tangential to this, some folks have started to explore modifications to softmax because quantization ends up being difficult due to large “random” values in the weights. So theres a higher than expected entropy in the weight distribution.\nIn summary, while static quantization’s one-size-fits-all approach can negatively impact RoPe-scaled models, dynamic quantization’s ability to adjust ensures a more robust performance."
  },
  {
    "objectID": "posts/explore-context/index.html#yet-another-rope-extension-method-yarn",
    "href": "posts/explore-context/index.html#yet-another-rope-extension-method-yarn",
    "title": "📈 Exploring Ways to Extend Context Length in Transformers",
    "section": "Yet another RoPE extensioN method (YaRN)",
    "text": "Yet another RoPE extensioN method (YaRN)\nWhile folks are still waiting for access of gpt4-32k, 128k open source models with YaRN is out. In the paper authors successfully extended the context length of LLAMA 2 13B to 128k training only for additional 400 steps!\nYaRN allows you to extend the context using only 0.1% of original pre-training data with negligible performance loss on standardized benchmarks compared to the original model.\nThe only catch is that 13B model requires approximatively 360GB of VRAM for the full 128k context size using Flash Attention 2, which is not really in reach for GPU poors today, but definitely impressive and a significant innovation."
  },
  {
    "objectID": "posts/explore-context/index.html#curse-of-naive-evaluation",
    "href": "posts/explore-context/index.html#curse-of-naive-evaluation",
    "title": "📈 Exploring Ways to Extend Context Length in Transformers",
    "section": "Curse of Naive Evaluation",
    "text": "Curse of Naive Evaluation\nThe rise of these scaling tricks has prompted many in the open-source community to adopt them. However, most have been evaluating model performance using perplexity. Although it’s a good starting point, it’s not the most comprehensive metric. As highlighted by the tweet below, there are nuances in evaluation that can be overlooked:\n\n\nThe first ~5 tokens in each segment contribute most of the loss. The shorter model has to deal with double the amount of early token areas!We showed in our Shortformer paper that if you use this naïve evaluation method, longer models will always appear to be better–&gt; pic.twitter.com/mbHsIwrYxe\n\n— Ofir Press (@OfirPress) July 12, 2023\n\n\nThis tweet emphasizes the importance of considering token positions when evaluating model performance, a factor that can significantly influence perceived model efficacy."
  },
  {
    "objectID": "posts/explore-context/index.html#do-we-really-need-it",
    "href": "posts/explore-context/index.html#do-we-really-need-it",
    "title": "📈 Exploring Ways to Extend Context Length in Transformers",
    "section": "Do We Really Need It?",
    "text": "Do We Really Need It?\nHarm De Vries, in his excellent post argues that we may be wasting attention overhead on randomly concatenated files, since about 80-90% of CommonCrawl and Github examples are shorter than 2K tokens.\nNot to mention that, LLMs can be easily distracted by irrelevant context\n\n\nThis came to mind :)https://t.co/hRpaUSDdHq\n\n— Muhtasham O. (@Muhtasham9) September 4, 2023"
  },
  {
    "objectID": "posts/explore-context/index.html#reflection-on-transformer-architectures",
    "href": "posts/explore-context/index.html#reflection-on-transformer-architectures",
    "title": "📈 Exploring Ways to Extend Context Length in Transformers",
    "section": "Reflection on Transformer Architectures",
    "text": "Reflection on Transformer Architectures\nI resonate with this quote and wish to highlight the evident trajectory within the domain of Transformer-based architectures in machine learning. The community, quite fittingly, has cast significant “attention” towards this design paradigm.\nInterestingly, while many gravitate towards Transformers, there are noteworthy deviations. Case in point, magic has opted out of using Transformers for their novel LTM-1 model, a Language Model that boasts an awe-inspiring 5,000,000 token context window. That being said, I also do find the work on sub-quadratic attention models Hyena, RWKV, S4etc quite exciting\nFollowing my insightful interaction with Eric Steinberger, the CEO of magic, at the NVIDIA Berlin event last month, I am inclined to believe that their approach may very well harness context in ways that could surpass traditional Transformers."
  },
  {
    "objectID": "posts/explore-context/index.html#closing-thoughts",
    "href": "posts/explore-context/index.html#closing-thoughts",
    "title": "📈 Exploring Ways to Extend Context Length in Transformers",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nThe journey into the depths of extending context lengths in LLMs is a testament to the relentless pursuit of innovation in the field. While the challenges presented are not trivial, they serve as catalysts for groundbreaking solutions. It’s inspiring to witness how the community comes together, turning each challenge into an opportunity for growth and discovery despite arms race for GPUs. As we continue to push the boundaries, it’s evident that the future of LLMs holds even more promise and potential, LFG!"
  },
  {
    "objectID": "posts/nvidia-berlin/index.html",
    "href": "posts/nvidia-berlin/index.html",
    "title": "🚀 Reflections from an Evening with NVIDIA’s Jensen Huang",
    "section": "",
    "text": "Sharing moment with Jensen\nListening to a fireside chat with Jensen Huang, the dynamic Founder and CEO of NVIDIA, alongside three pioneering startup founders— Dr. Jaroslaw “Jarek” Kutylowski of DeepL SE, Michael Putz of blackshark.ai, and Eric Steinberger of Magic—was a unique opportunity to delve into the collective wisdom of industry trailblazers. The insightful conversation mirrored a CEO self-help group, shedding light on the peaks and valleys of the entrepreneurial journey. In this post, I will share my key takeaways from this enlightening exchange."
  },
  {
    "objectID": "posts/nvidia-berlin/index.html#techs-evolving-landscape-and-the-decade-ahead",
    "href": "posts/nvidia-berlin/index.html#techs-evolving-landscape-and-the-decade-ahead",
    "title": "🚀 Reflections from an Evening with NVIDIA’s Jensen Huang",
    "section": "Tech’s Evolving Landscape and the Decade Ahead",
    "text": "Tech’s Evolving Landscape and the Decade Ahead\nJensen Huang’s conviction in AI’s potential to bridge existing technological gaps was a recurring theme in our discussions. He pinpointed that while sectors like heavy industries lag in technological advancements, AI could become a game-changer, particularly for the unsung heroes of climate modeling, who could utilize a foundational model emulating multi-physics to revolutionize their work.\nJensen’s infectious enthusiasm for a ‘software-defined world’ over the next decade reflected his perspective on the early days at NVIDIA. He reminisced about recognizing the CPU’s ‘miraculous’ potential, leading to NVIDIA’s revolutionary addition of GPUs to the tech arena."
  },
  {
    "objectID": "posts/nvidia-berlin/index.html#nvidias-concentrated-focus",
    "href": "posts/nvidia-berlin/index.html#nvidias-concentrated-focus",
    "title": "🚀 Reflections from an Evening with NVIDIA’s Jensen Huang",
    "section": "NVIDIA’s Concentrated Focus",
    "text": "NVIDIA’s Concentrated Focus\nNVIDIA, under Jensen’s leadership, has honed its focus on three strategic areas: accelerated computing, AI as a stack, and AI factories. According to Jensen, long-term forecasts should be rooted in first principles, declared early on in a company’s trajectory.\nJensen’s analogy of ‘taping out your software’ served as a reminder of the significant investments post ‘Software 2.0’ companies make in procuring GPUs to supercharge their computational capabilities. He reminscented how back in the days, most of the raised budget for NVIDIA would go to taping chips and chip fabrications."
  },
  {
    "objectID": "posts/nvidia-berlin/index.html#startups-against-all-odds",
    "href": "posts/nvidia-berlin/index.html#startups-against-all-odds",
    "title": "🚀 Reflections from an Evening with NVIDIA’s Jensen Huang",
    "section": "Startups: Against All Odds",
    "text": "Startups: Against All Odds\nJensen’s account of his journey building NVIDIA was enlightening. The company had to navigate its way through a free graphics card market from Intel and the limitations imposed by Moore’s Law. Yet, they found a niche small enough that they could dominate with their resources against big players like Intel.\nDuring the discussion, the other startup founders also shared their perspectives when Jensen asked why they chose to compete against well-established alternatives. Their response can be summarized as follows:\n\nMicrosoft Flight Simulator uses blackshark.ai because it fits their needs much better. They also update the world map more frequently, and with sufficient GPUs, they can do it in real-time.\nMagic is up against the giants of OpenAI and GitHub Copilot, aiming to create the best AGI to empower software engineers. Magic’s goal is to generate relevant code without the need for fine-tuning or large memory requirements. Nat Friedman, former CEO of GitHub, who was responsible for Copilot during his time, invested in Magic, which speaks volumes. As Jensen noted, startups by definition lack enough engineers, so Magic can help them in that aspect.\nDeepL was also up against Google Translate, but they have significantly improved their translation capabilities."
  },
  {
    "objectID": "posts/nvidia-berlin/index.html#navigating-leadership-and-succession",
    "href": "posts/nvidia-berlin/index.html#navigating-leadership-and-succession",
    "title": "🚀 Reflections from an Evening with NVIDIA’s Jensen Huang",
    "section": "Navigating Leadership and Succession",
    "text": "Navigating Leadership and Succession\nWhen asked about the criteria for his successor, Jensen shared that NVIDIA rejected traditional succession planning in favor of leadership development. The next leader of NVIDIA, he said, should have a balance of confidence and doubt, the ability to reevaluate their approach every day, and the resilience to face setbacks.\nJensen shared his perspective on the importance of strategic retreat, not considering it a failure. He highlighted that the enterprise space is large enough to accommodate many startups, making strategic positioning vital."
  },
  {
    "objectID": "posts/nvidia-berlin/index.html#controversial-question-to-jensen",
    "href": "posts/nvidia-berlin/index.html#controversial-question-to-jensen",
    "title": "🚀 Reflections from an Evening with NVIDIA’s Jensen Huang",
    "section": "Controversial question to Jensen",
    "text": "Controversial question to Jensen\nAmidst the flood of questions and requests after the event, I took the opportunity to ask Jensen a question I have been curious, it was about regulations around AI and rising concerns of some people regarding the nationalization of NVIDIA. In response, Jensen briefly stated:\n\nNVIDIA will not be nationalized; that’s not possible. If it were to happen, many nations would complain …"
  },
  {
    "objectID": "posts/nvidia-berlin/index.html#summary",
    "href": "posts/nvidia-berlin/index.html#summary",
    "title": "🚀 Reflections from an Evening with NVIDIA’s Jensen Huang",
    "section": "Summary",
    "text": "Summary\nIn conclusion, every CEO’s journey is a challenging endeavor, a road trip to a destination often shrouded in fog. Yet, the defining characteristic of these leaders, much like NVIDIA itself, is resilience. This event served as a powerful reminder of the importance of belief, endurance, and the relentless pursuit of one’s dreams."
  },
  {
    "objectID": "posts/modcon23/index.html",
    "href": "posts/modcon23/index.html",
    "title": "🔥 The Future of AI is Modular: Insights from ModCon 2023",
    "section": "",
    "text": "The iconic Macintosh Lisa at the Computer History Museum, symbolizing the shift from early computing to the AI age. Source code available here if you want to rewrite in Mojo 🔥"
  },
  {
    "objectID": "posts/modcon23/index.html#introduction",
    "href": "posts/modcon23/index.html#introduction",
    "title": "🔥 The Future of AI is Modular: Insights from ModCon 2023",
    "section": "Introduction",
    "text": "Introduction\nToday, the landscape of AI development is on the brink of a transformative change. I had the opportunity to witness this first-hand at the ModCon conference in San Francisco, where the convergence of ideas and technology marked a new era in AI and computing."
  },
  {
    "objectID": "posts/modcon23/index.html#the-shift-in-computing-paradigm",
    "href": "posts/modcon23/index.html#the-shift-in-computing-paradigm",
    "title": "🔥 The Future of AI is Modular: Insights from ModCon 2023",
    "section": "The Shift in Computing Paradigm",
    "text": "The Shift in Computing Paradigm\nTraditional computing, once dominated by single-threaded CPU performance and programming in languages like C and C++, is now undergoing a significant transformation. The deceleration of Moore’s Law and the rapid ascent of AI necessitated a new approach - a pivot towards multi-core hardware and accelerated compute.\nHowever, this transition was met with considerable challenges. The software industry struggled to keep pace. A universal solution that could support all AI hardware without sacrificing programmability or performance was sorely missing. This led to inefficiencies, with AI engineers constantly rewriting code, hardware makers facing innovation hurdles, and high costs coupled with underutilization of resources."
  },
  {
    "objectID": "posts/modcon23/index.html#modulars-thoughtful-approach",
    "href": "posts/modcon23/index.html#modulars-thoughtful-approach",
    "title": "🔥 The Future of AI is Modular: Insights from ModCon 2023",
    "section": "Modular’s Thoughtful Approach",
    "text": "Modular’s Thoughtful Approach\nModular’s presentations at ModCon 2023 offered a wealth of insights. I’m eager to share some key takeaways, even as I continue to reflect on the full depth of the information presented. The Modular stack empowers AI developers to program in Python while achieving unparalleled performance. In my conversation with Chris Lattner, it became clear that he and his team are committed to not rushing the development process. They are focused on creating a developer-friendly product, ensuring that the Mojo programming language is not just efficient but also accessible. Mojo, as Chris puts it, is akin to “Python++,” offering the simplicity of Python with the power and precision of languages like Rust - a true beacon for future AI development.\n\n\n\nSharing moment with Chris Lattner, inventor of LLVM, MLIR and CEO of Modular company behind Mojo 🔥"
  },
  {
    "objectID": "posts/modcon23/index.html#key-announcements-at-modcon-2023",
    "href": "posts/modcon23/index.html#key-announcements-at-modcon-2023",
    "title": "🔥 The Future of AI is Modular: Insights from ModCon 2023",
    "section": "Key Announcements at ModCon 2023",
    "text": "Key Announcements at ModCon 2023\n\nMAX: Modular Accelerated Xecution\nThe centerpiece of ModCon 2023 was the unveiling of MAX - Modular Accelerated Xecution. This comprehensive suite simplifies AI infrastructure, providing everything needed to deploy low-latency, high-throughput generative and traditional inference pipelines. Anticipated to be available in both a free Developer Edition and a paid Enterprise Edition in early 2024, MAX is set to revolutionize the field.\n\n\nPartnerships with AWS and NVIDIA\nModular also announced key partnerships. With AWS, they’re introducing the MAX Enterprise Edition exclusively to AWS production services, optimizing performance on the AWS Graviton platform. The collaboration with NVIDIA is set to unify and simplify CPU+GPU development for AI, leveraging NVIDIA’s accelerated compute platform.\n\n\nNew Features in MAX Engine\nMAX Engine now boasts new capabilities, such as Mojo integration, GPU support, and the release of Mojo SDK v0.6. These enhancements are designed to substantially improve both performance and user experience."
  },
  {
    "objectID": "posts/modcon23/index.html#the-future-with-modular-and-max",
    "href": "posts/modcon23/index.html#the-future-with-modular-and-max",
    "title": "🔥 The Future of AI is Modular: Insights from ModCon 2023",
    "section": "The Future with Modular and MAX",
    "text": "The Future with Modular and MAX\nModular’s MAX is a groundbreaking addition to the AI landscape. It’s not just about boosting performance; it’s about streamlining AI development, making it more accessible, and paving the way for groundbreaking AI applications. The partnerships with AWS and NVIDIA further emphasize this, promising unmatched performance and ease of use for AI developers."
  },
  {
    "objectID": "posts/modcon23/index.html#conclusion",
    "href": "posts/modcon23/index.html#conclusion",
    "title": "🔥 The Future of AI is Modular: Insights from ModCon 2023",
    "section": "Conclusion",
    "text": "Conclusion\nReflecting on my ModCon experience, the potential of MAX and Modular’s technology for the future of AI is evident. It’s a future where AI development is more cohesive, efficient, and potent - a future where AI engineers are equipped to unlock new possibilities and bring their creative visions to life.\n\nBack to top"
  },
  {
    "objectID": "posts/phildeeplearning/index.html#table-of-contents",
    "href": "posts/phildeeplearning/index.html#table-of-contents",
    "title": "🗽 A Philosophical Dive into Deep Learning: My Experience at the NYU Conference",
    "section": "Table of contents",
    "text": "Table of contents\n\nIntroduction\nA Thought-Provoking Start: Sensory Grounding in Large Language Models\nYann LeCun’s Bold Prediction\nDavid Chalmers’ Nuanced View\nA Glimpse into Other Talks\nNew York Vibes\nConclusion"
  },
  {
    "objectID": "posts/phildeeplearning/index.html#introduction",
    "href": "posts/phildeeplearning/index.html#introduction",
    "title": "🗽 A Philosophical Dive into Deep Learning: My Experience at the NYU Conference",
    "section": "Introduction",
    "text": "Introduction\n\nLast week, I was fortunate enough to attend the captivating Philosophy of Deep Learning Conference at New York University. This event united experts from diverse fields to explore the philosophical aspects and implications of deep learning and artificial intelligence.In this blog post, I’ll recount some memorable highlights from the conference and share my impressions of the dynamic atmosphere of New York City."
  },
  {
    "objectID": "posts/phildeeplearning/index.html#a-thought-provoking-start-sensory-grounding-in-large-language-models",
    "href": "posts/phildeeplearning/index.html#a-thought-provoking-start-sensory-grounding-in-large-language-models",
    "title": "🗽 A Philosophical Dive into Deep Learning: My Experience at the NYU Conference",
    "section": "A Thought-Provoking Start: Sensory Grounding in Large Language Models",
    "text": "A Thought-Provoking Start: Sensory Grounding in Large Language Models\n\nThe conference kicked off with a thought-provoking pre-conference debate titled “Do large language models need sensory grounding for meaning and understanding?”. The debate featured esteemed speakers such as Yann LeCun and David Chalmers.\nThroughout the conference, every interaction, from keynote speeches to coffee breaks, seemed to be imbued with philosophical depth.\n\nThis meme captured the mood of the conference perfectly. Everyone seemed to be thinking deeply about the implications of their work."
  },
  {
    "objectID": "posts/phildeeplearning/index.html#yann-lecuns-bold-prediction",
    "href": "posts/phildeeplearning/index.html#yann-lecuns-bold-prediction",
    "title": "🗽 A Philosophical Dive into Deep Learning: My Experience at the NYU Conference",
    "section": "Yann LeCun’s Bold Prediction",
    "text": "Yann LeCun’s Bold Prediction\n\nYann LeCun, one of the pioneers in the field of deep learning, was on the “Yes” side of the debate. He made a bold prediction that nobody in their right mind will use autoregressive models 5 years from now. LeCun argued that Auto-Regressive Large Language Models (LLMs) are exponentially diverging diffusion processes, and while the probability of errors can be reduced through training, the problem cannot be entirely eliminated.\nIn other words LLMs are like Micael Scott’s famous line in The Office:\n\nHis solution is to make LLMs non-autoregressive while preserving their fluency. You can find his slides here."
  },
  {
    "objectID": "posts/phildeeplearning/index.html#david-chalmers-nuanced-view",
    "href": "posts/phildeeplearning/index.html#david-chalmers-nuanced-view",
    "title": "🗽 A Philosophical Dive into Deep Learning: My Experience at the NYU Conference",
    "section": "David Chalmers’ Nuanced View",
    "text": "David Chalmers’ Nuanced View\n\nDavid Chalmers, a renowned philosopher, took a more nuanced approach to the debate. He started his talk on the “Yes” side with a bit of philosophical history about the grounding problem, discussing thought experiments from philosophers like Avicenna. Chalmers ultimately concluded with a “No, but – it’s complicated” answer to the debate question. You can find his slides here."
  },
  {
    "objectID": "posts/phildeeplearning/index.html#a-glimpse-into-other-talks",
    "href": "posts/phildeeplearning/index.html#a-glimpse-into-other-talks",
    "title": "🗽 A Philosophical Dive into Deep Learning: My Experience at the NYU Conference",
    "section": "A Glimpse into Other Talks",
    "text": "A Glimpse into Other Talks\n\nOne of the most interesting talks during the conference was by Tal Linzen, titled “What, if Anything, Can Large Language Models Teach Us About Human Language Acquisition?”. Linzen posited that deep learning can be a great tool for testing theoretical claims about children’s linguistic input and inductive biases if used correctly. However, he emphasized that cognitive scientists need to train new models themselves, as the “large language models” from big labs are not relevant to questions about humans. Linzen’s slides can be found here.\n\nUpdate 8 April 2023: Head to https://phildeeplearning.github.io to find individual links in the program, or for the whole Youtube playlist."
  },
  {
    "objectID": "posts/phildeeplearning/index.html#new-york-vibes",
    "href": "posts/phildeeplearning/index.html#new-york-vibes",
    "title": "🗽 A Philosophical Dive into Deep Learning: My Experience at the NYU Conference",
    "section": "New York Vibes",
    "text": "New York Vibes\n\nThe conference experience was made even better by the electric atmosphere of New York City. Walking through the city’s bustling streets, I couldn’t help but feel the energy and excitement that makes New York such a unique place. Between conference sessions, I enjoyed exploring the city’s diverse neighborhoods, trying out new foods, and admiring the stunning skyline. There’s something truly special about being in a city that feels so alive and vibrant, and it was the perfect backdrop for a conference focused on the future of deep learning and AI."
  },
  {
    "objectID": "posts/phildeeplearning/index.html#conclusion",
    "href": "posts/phildeeplearning/index.html#conclusion",
    "title": "🗽 A Philosophical Dive into Deep Learning: My Experience at the NYU Conference",
    "section": "Conclusion",
    "text": "Conclusion\n\nIn conclusion, the Philosophy of Deep Learning conference at NYU was an unforgettable experience that brought together experts from various fields to discuss the latest developments in deep learning and artificial intelligence. The insightful debates and thought-provoking talks challenged our understanding of large language models, their relationship to human cognition, and the philosophical implications of AI. Combined with the thrilling energy of New York City, the conference left me feeling inspired and eager to explore new ideas in the rapidly evolving world of AI and deep learning. I’m already looking forward to attending similar events in the future and further immersing myself in the fascinating intersection of technology and philosophy."
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html",
    "href": "posts/vc-fireside-chat/index.html",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "",
    "text": "The landscape of artificial intelligence (AI) startups is rapidly evolving, and so are the strategies for building a successful and defensible business. A recent discussion panel featured Sarah Catanzaro (Amplify Partners) and Matt Bornstein (a16z), who shared their insights on how to navigate the challenging world of AI startups. The panel was moderated by Josh Tobin (gantry), during LLM bootcamp 2023 in San Francisco."
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html#introduction",
    "href": "posts/vc-fireside-chat/index.html#introduction",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "",
    "text": "The landscape of artificial intelligence (AI) startups is rapidly evolving, and so are the strategies for building a successful and defensible business. A recent discussion panel featured Sarah Catanzaro (Amplify Partners) and Matt Bornstein (a16z), who shared their insights on how to navigate the challenging world of AI startups. The panel was moderated by Josh Tobin (gantry), during LLM bootcamp 2023 in San Francisco."
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html#asking-the-right-questions-to-vcs",
    "href": "posts/vc-fireside-chat/index.html#asking-the-right-questions-to-vcs",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "Asking the Right Questions to VCs:",
    "text": "Asking the Right Questions to VCs:\nThe panelists emphasized the importance of asking the right questions to venture capitalists (VCs). Understanding their perspectives, investment strategies, and areas of interest can provide valuable insights for entrepreneurs seeking funding and support."
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html#challenging-the-status-quo",
    "href": "posts/vc-fireside-chat/index.html#challenging-the-status-quo",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "Challenging the Status Quo:",
    "text": "Challenging the Status Quo:\nEntrepreneurs should not hesitate to test their ideas, even if they contradict the opinions of VCs. Trusting your instincts and validating your hypothesis in the market is crucial for success.\n\n\nWhenever a VC gives you advice, just remember that Apple flouts all the “rules” and is bigger than the entire VC industry.1. “Hardware is Hard”2. “Consumer HW is even harder”3. “Start with a problem, not with a cool technology”4. Can’t differentiate on quality\n\n— Tapa Ghosh (@semiDL) April 30, 2023"
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html#saving-money-on-gpus",
    "href": "posts/vc-fireside-chat/index.html#saving-money-on-gpus",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "Saving Money on GPUs:",
    "text": "Saving Money on GPUs:\nThe panelists advised against investing heavily in GPUs before achieving product-market fit. It is essential to validate your business model and product before committing significant resources to AI hardware."
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html#embracing-change-and-adaptation",
    "href": "posts/vc-fireside-chat/index.html#embracing-change-and-adaptation",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "Embracing Change and Adaptation:",
    "text": "Embracing Change and Adaptation:\nFounders should be prepared to pivot and adapt to the ever-changing AI landscape. Regular reevaluation of business strategies and product offerings is key to staying relevant and competitive."
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html#creating-value-for-users",
    "href": "posts/vc-fireside-chat/index.html#creating-value-for-users",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "Creating Value for Users:",
    "text": "Creating Value for Users:\nA crucial factor in building a defensible business is the amount of value your product or service creates for users. Ensuring that your offering addresses a genuine need and provides tangible benefits can help to build a loyal customer base."
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html#vcs-love-for-tools-and-platforms",
    "href": "posts/vc-fireside-chat/index.html#vcs-love-for-tools-and-platforms",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "VCs’ Love for Tools and Platforms:",
    "text": "VCs’ Love for Tools and Platforms:\nThe panelists highlighted that VCs are particularly interested in investing in tools and platforms, as they can enable the growth of entire ecosystems."
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html#llmops-and-multimodal-markets",
    "href": "posts/vc-fireside-chat/index.html#llmops-and-multimodal-markets",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "LLMOps and Multimodal Markets:",
    "text": "LLMOps and Multimodal Markets:\nAccording to the panelists, the markets for low-latency machine learning operations (LLMOps) and multimodal AI applications are currently booming, presenting numerous opportunities for entrepreneurs."
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html#bringing-products-and-users-to-the-table",
    "href": "posts/vc-fireside-chat/index.html#bringing-products-and-users-to-the-table",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "Bringing Products and Users to the Table:",
    "text": "Bringing Products and Users to the Table:\nIdeally, startups should approach investors with an existing product and user base. Demonstrating traction and market interest can significantly improve the chances of securing funding."
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html#turnover-technology",
    "href": "posts/vc-fireside-chat/index.html#turnover-technology",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "Turnover Technology:",
    "text": "Turnover Technology:\nThe panelists noted that turnover technology is a hot area for investment. Tools that help companies improve their employee turnover rates and streamline operations are particularly attractive to VCs."
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html#the-impermanence-of-ai-models",
    "href": "posts/vc-fireside-chat/index.html#the-impermanence-of-ai-models",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "The Impermanence of AI Models:",
    "text": "The Impermanence of AI Models:\nThe panelists warned against spending too much time and resources on training AI models, as they can become obsolete within months. Additionally, entrepreneurs should consider whether their models will outperform the next generation of AI models, such as GPT-5, before investing significant resources."
  },
  {
    "objectID": "posts/vc-fireside-chat/index.html#conclusion",
    "href": "posts/vc-fireside-chat/index.html#conclusion",
    "title": "🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel",
    "section": "Conclusion:",
    "text": "Conclusion:\nBuilding a defensible AI business requires a combination of perseverance, adaptability, and a keen understanding of market trends. By focusing on creating value for users, targeting high-growth sectors, and making strategic investments in technology, entrepreneurs can increase their chances of success in the ever-evolving world of AI."
  },
  {
    "objectID": "posts/llm-for-compiler-optimization/index.html",
    "href": "posts/llm-for-compiler-optimization/index.html",
    "title": "🖥️​ LLMs for Compiler Optimization: A Deep Dive",
    "section": "",
    "text": "The CDC 6600, a pioneer in the computing world, standing as the first successful supercomputer. Its legacy echoes in the advancements discussed in this blog post. Image captured at the Computer History Museum by the author.\n\n\n\nIntroduction\nThe application of Large Language Models (LLMs) in optimizing LLVM assembly for code size is emerging, but is it truly shaping a new reality or just a theoretical advancement? Over the weekend, I delved deep into a new paper titled LLMs for Compiler Optimization to find out.\nThe paper presents a 7B-parameter transformer model based on LLAMA 2 configuration and trained from scratch to optimize LLVM assembly for code size. The paper claims a 3.0% improvement in reducing instruction counts compared to the compiler, and it reportedly generates compilable code 91% of the time, perfectly emulating the compiler’s output 70% of the time.\nBut how substantial are these improvements in real-world applications? And what are the limitations of this approach?\nFor a deep understanding, I encourage you to read the full paper, but if you’re short on time, this blog post will give you the highlights and my take on it.\n\n\nUnderstanding the Basics\nBefore we dive in, let’s understand a few terms:\n\nLLVM Assembly/LLVM-IR: LLVM Intermediate Representation (IR) is akin to a universal language translator, sitting between high-level languages and machine code. It is a low-level programming language used in the LLVM compiler, a collection of modular and reusable compiler and toolchain technologies, serving as a stable and optimized bridge facilitating clear communication.\nCompiler Optimizations: These are techniques used to enhance the code’s performance and efficiency without altering its functionality. The optimizations are performed on LLVM-IR in this context.\n\n\n\nDeep Dive into the Technical Details\nThe researchers presented a transformer model, trained from scratch to optimize LLVM assembly for code size. This model takes unoptimized assembly as input and suggests a list of compiler options to best optimize the program. It predicts instruction counts before and after optimization, and even the optimized code itself, enhancing its depth of understanding and optimization performance.\nTo appreciate the depth of this research, let’s delve into the technical specifics outlined in the paper:\n\n1. Model Architecture and Tokenizer\n\nModel Architecture: The researchers utilized the Llama 2 architecture with 7B parameters, characterized by 32 attention heads, 4096 hidden dimensions, and 32 layers, and initialized from scratch.\nByte Pair Encoding (BPE) Tokenizer: This tokenizer breaks down text into subwords or smaller units, facilitating the efficient handling of a large vocabulary. It is instrumental in processing the input data into tokens that the model can understand.\n\n\n\n2. Sequence Length\n\n2048 Tokens: The chosen sequence length, representing the maximum number of tokens that can be input into the model in a single batch. This choice is a balancing act between computational resources and the ability to process large LLVM-IR code sequences.\n\n\n\n3. Tokenization Details\n\n2.02 Characters per Token: The Llama 2 tokenizer, on average, represents 2.02 characters of LLVM-IR code in each token.\n2KB Limit: This limit is derived from the tokenization rate, establishing the maximum LLVM-IR sequence size that they can train on, which is approximately 2KB.\n\n\n\n\n\n\n\nNoteFor Further Insight\n\n\n\n\n\nFor those interested in delving deeper into the nuances of tokenization, this tweet by Thomas Wolf offers an excellent analysis comparing different tokenizers and their efficiency in handling large datasets:\n\n\nAnd our answers are out!Running on 1B tokens from the web (filtered and mostly in English as details in https://t.co/8hXhqmK3ND) we got🥁- GPT4 tokenizer (100k vocab) gives you 0.997B tokens - Falcon tokenizer (64k vocab) gives you ~5% more tokens (1.04B)- Llama2 tokenizer… https://t.co/suxqhHmOWp\n\n— Thomas Wolf (@Thom_Wolf) September 11, 2023\n\n\n\n\n\n\n\n\nSurprising Abilities of LLMs\nContrary to the initial assumption that the paper would highlight the shortcomings of LLMs, it revealed that a sufficiently trained LLM could predict and directly apply the best optimizations to an input code, bypassing the need for a compiler altogether. This was a surprising revelation, showcasing the untapped potential of LLMs in code optimization.\n\n\nAuxiliary Learning Tasks\n\n\n\nFigure 1 from the paper\n\n\nThe model was tasked with two auxiliary functions to foster a deep understanding of code optimization mechanics, but one might wonder if this approach truly covers all the necessary grounds for optimization.\n\nGenerating instruction counts before and after applying optimizations.\nProducing the output IR post-optimization.\n\nThese tasks were hypothesized to facilitate better pass-ordering decisions, a crucial aspect of compiler optimizations, and they did indeed yield promising results.\n\n\nChallenges and Future Directions\nWhile the results are promising, one cannot overlook the significant limitations of the approach, including the restricted sequence length of inputs, known as the context window. The researchers targeted a 2k-token context window, necessitating the splitting of IRs into individual functions to fit within this window. This approach, albeit effective, curtails intra-function optimization and limits the context available for making optimization decisions.\nThe paper suggests that evolving techniques for handling long sequences could potentially overcome this limitation, hinting at a promising avenue for future research.\n\n\nMath Reasoning and Logic\nThe paper also touched upon the challenges LLMs face in arithmetic reasoning, a vital aspect of compiler optimizations. It proposed a “chain-of-thought” approach, where models are trained to break down complex reasoning problems into incremental steps, offering a promising direction for enhancing LLMs’ capabilities.\n\n\nFurther Listening\nIf you’re keen to delve deeper into the world of compilers and AI, I highly recommend listening to the latest episode of the Latent Space Podcast where Chris Lattner inventor of LLVM, shares insights on the future of AI software and the role of compilers in this exciting era. The podcast touches upon several intriguing points, including:\n\nThe potential of compilers to abstract away complex processes, allowing for more efficient coding.\nThe initiative to enhance Python’s capabilities for parallel computing through Mojo.\nThe efforts by Modular to diversify the architectures and hardware utilized in AI, moving beyond the current reliance on transformer architectures and NVIDIA’s hardware.\n\nIt’s a rich source of information and presents a visionary perspective on the evolving landscape of AI and compiler technologies.\n\n\nMy Take\nWhile I see potential for further exploration in this field, I maintain a cautious optimism given the early stage of this research.\nIt would be fascinating to see if this approach could be leveraged to optimize other metrics such as runtime and energy efficiency. In fact, the practical implications of optimizing compiler configurations are currently being explored in a Kaggle competition hosted by Google.\nScaling to larger model sizes could potentially yield better results too, as the researchers only utilized a 7B parameter model. Moreover, addressing the limitation of the context window through fine-tuning for longer contexts, as discussed in my other blog, could open up avenues for more advanced optimizations.\nIn conclusion, while the paper paints a promising picture for the role of LLMs in compiler optimizations, it is essential to remain cautious. Given that the LLAMA 2 model weights were released, it would be beneficial for the community if the weights of this specific model were also shared. However, I perfectly understand if the researchers wish to spend more time exploring all realms before releasing it to the public. As we anticipate further research in this domain, a skeptical yet hopeful eye remains necessary.\n\nBack to top"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "🤗 Herzlich Wilkommen!",
    "section": "",
    "text": "Welcome to my new blog and my new home on the internet!\nMy name is Muhtasham and I am excited to share my journey and learning in the field of machine learning with all of you. After publishing on Medium for a while, I have decided to make the switch to here in order to have a more personalized space to share my thoughts, experiences, and insights on this rapidly evolving field.\nOn this blog, you can expect to find articles and tutorials on a wide range of machine learning topics, including Machine Learning and Information Theory. I am passionate about staying up-to-date with the latest advancements in the field and sharing that knowledge with others. In addition to written content, I will also be sharing code tutorials and demonstrations to help readers better understand and apply the concepts I discuss.\nI hope that this blog will not only serve as a source of information and inspiration for those interested in machine learning, but also as a way for me to connect with like-minded individuals and continue learning from others in the community. Thank you for joining me on this journey and I look forward to sharing my knowledge and growth with all of you. This is my new home on the internet and I am excited to have you all as my virtual neighbors. Let’s learn and grow together!"
  },
  {
    "objectID": "posts/llm-watermark-attack/index.html",
    "href": "posts/llm-watermark-attack/index.html",
    "title": "👾 Watermark Security in Language Models",
    "section": "",
    "text": "From the pioneering era of Spacewar! on the DEC PDP-1, this post transitions into the contemporary sphere of language model security. Image captured at the Computer History Museum by the author.\nIn this brief exploration, I delve into a groundbreaking concept: a unique watermarking technique for large language models (LLMs), as detailed in A Watermark for Large Language Models. While the idea of watermarking LLMs itself is intriguing, what captivated my attention even more were the various strategies to attack these watermarks. In this post, I’ll share key insights from this research, highlighting adversarial attacks."
  },
  {
    "objectID": "posts/llm-watermark-attack/index.html#the-persistent-challenge-of-jailbreaking-llms",
    "href": "posts/llm-watermark-attack/index.html#the-persistent-challenge-of-jailbreaking-llms",
    "title": "👾 Watermark Security in Language Models",
    "section": "The Persistent Challenge of Jailbreaking LLMs",
    "text": "The Persistent Challenge of Jailbreaking LLMs\nIn the paper “Jailbroken: How Does LLM Safety Training Fail?”, authors discuss the inherent vulnerabilities of LLMs to adversarial attacks, despite rigorous safety training. Large language models like OpenAI’s GPT-4 and Anthropic’s Claude v1.3, trained for safety and harmlessness, remain susceptible to “jailbreak” attacks, designed to elicit undesired behaviors such as harmful content generation or personal information leaks. This vulnerability stems from two fundamental failure modes in safety training:\n\nCompeting Objectives: A conflict between the model’s capabilities and safety goals, leading to a compromise in adhering to safe outputs.\nMismatched Generalization: A failure in safety training to generalize to certain domains within the model’s pretraining corpus, creating loopholes for adversarial inputs.\n\nDespite extensive safety measures, these models exhibit weaknesses against newly designed attacks, emphasizing the need for safety mechanisms as sophisticated as the model’s capabilities themselves."
  },
  {
    "objectID": "posts/llm-watermark-attack/index.html#advancing-adversarial-attacks-on-aligned-llms",
    "href": "posts/llm-watermark-attack/index.html#advancing-adversarial-attacks-on-aligned-llms",
    "title": "👾 Watermark Security in Language Models",
    "section": "Advancing Adversarial Attacks on Aligned LLMs",
    "text": "Advancing Adversarial Attacks on Aligned LLMs\nFurther complicating the security landscape, the paper “Universal and Transferable Adversarial Attacks on Aligned Language Models” introduces a new class of adversarial attacks. These attacks, unlike previous ones requiring significant human ingenuity and being brittle, are automated and highly effective. The method involves appending a specially crafted suffix to queries, maximized to induce affirmative responses from the model, thus bypassing alignment safeguards aimed at preventing harmful content generation. Key highlights include:\n\nAutomated Adversarial Suffix Generation: Utilizing a combination of greedy and gradient-based search techniques, this approach automatically creates adversarial suffixes, significantly improving over past automatic prompt generation methods.\nTransferability Across Models: These attacks are not only effective on the model they were designed for but also transfer successfully to other models, including publicly released LLMs like ChatGPT, Bard, and Claude, as well as open-source models.\n\nThis advancement in adversarial attacks underscores the ongoing arms race in LLM security, where traditional posthoc repair strategies are proving insufficient against increasingly sophisticated attacks."
  },
  {
    "objectID": "posts/llm-watermark-attack/index.html#implications-for-watermarking-in-llms",
    "href": "posts/llm-watermark-attack/index.html#implications-for-watermarking-in-llms",
    "title": "👾 Watermark Security in Language Models",
    "section": "Implications for Watermarking in LLMs",
    "text": "Implications for Watermarking in LLMs\nThe evolving complexity of adversarial attacks presents significant challenges for watermarking LLMs. The need for robust watermarking techniques that can withstand such sophisticated attacks is paramount. As we develop these techniques, we must consider the learnings from these papers to ensure that watermarking mechanisms are not only effective but also resilient against the evolving landscape of adversarial attacks in LLMs."
  },
  {
    "objectID": "posts/llm-watermark-attack/index.html#threat-model-and-adversarial-behavior",
    "href": "posts/llm-watermark-attack/index.html#threat-model-and-adversarial-behavior",
    "title": "👾 Watermark Security in Language Models",
    "section": "Threat Model and Adversarial Behavior",
    "text": "Threat Model and Adversarial Behavior\nExploring the threat model, we understand the diverse ways adversaries might attempt to manipulate or remove watermarks in machine-generated text. These methods range from social media bots to CAPTCHA circumvention and academic dishonesty."
  },
  {
    "objectID": "posts/llm-watermark-attack/index.html#parties-involved",
    "href": "posts/llm-watermark-attack/index.html#parties-involved",
    "title": "👾 Watermark Security in Language Models",
    "section": "Parties Involved",
    "text": "Parties Involved\nTwo primary parties are central to this narrative:\n\nModel Owner: Provides a text generation API, incorporating watermarks to track text origin.\nAttacker: Strives to erase watermarks, aware of their presence and possibly even the underlying technology."
  },
  {
    "objectID": "posts/llm-watermark-attack/index.html#modes-of-operation",
    "href": "posts/llm-watermark-attack/index.html#modes-of-operation",
    "title": "👾 Watermark Security in Language Models",
    "section": "Modes of Operation",
    "text": "Modes of Operation\nThe interaction occurs in two distinct modes:\n\nPublic Mode: Attackers possess complete knowledge of the hashing scheme.\nPrivate Mode: Attackers are aware of the watermark but lack insights into the key mechanisms."
  },
  {
    "objectID": "posts/llm-watermark-attack/index.html#attack-strategies",
    "href": "posts/llm-watermark-attack/index.html#attack-strategies",
    "title": "👾 Watermark Security in Language Models",
    "section": "Attack Strategies",
    "text": "Attack Strategies\nWe categorize attack strategies into three main types:\n\nText Insertion Attacks\nText Deletion Attacks\nText Substitution Attacks\n\nEach type carries unique challenges and implications for text integrity and computational efficiency."
  },
  {
    "objectID": "posts/llm-watermark-attack/index.html#specific-attacks-and-mitigations",
    "href": "posts/llm-watermark-attack/index.html#specific-attacks-and-mitigations",
    "title": "👾 Watermark Security in Language Models",
    "section": "Specific Attacks and Mitigations",
    "text": "Specific Attacks and Mitigations\n\nParaphrasing Attacks: Ranging from manual to automated, these attacks involve rephrasing using weaker models.\nDiscreet Alterations: Small changes like adding whitespaces or misspellings can affect hash computation. Proper text normalization is crucial in defending against these alterations.\nTokenization Attacks: Altering text to modify sub-word tokenization, impacting the watermarking process.\nHomoglyph and Zero-Width Attacks: Using similar-looking Unicode characters or invisible characters to disrupt tokenization, requiring input normalization for defense.\nGenerative Attacks: Prompting the model to change its output in a predictable way, such as the “Emoji Attack,” which affects the watermark’s effectiveness but requires a strong language model."
  },
  {
    "objectID": "posts/llm-watermark-attack/index.html#conclusion",
    "href": "posts/llm-watermark-attack/index.html#conclusion",
    "title": "👾 Watermark Security in Language Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis brief exploration into watermarking techniques and their vulnerabilities in LLMs touches only the surface of a much larger topic. For a more in-depth understanding of Adversarial Attacks on LLMs, Lilian Weng’s blog is an excellent resource. As of 11/20/2023, she remains a pivotal figure at OpenAI, possibly leading the AI Safety team. In light of recent challenges within OpenAI’s board, where attempts at adversarial tactics were met with unity and loyalty from the team, Weng’s insights are particularly relevant and enlightening Lilian Weng’s Blog."
  },
  {
    "objectID": "posts/reflections22/index.html",
    "href": "posts/reflections22/index.html",
    "title": "⏳ From Alps to NLP: A 2022 Recap of Exploration and Growth",
    "section": "",
    "text": "📝 Preface\nAs I look back on the year 2022, I can’t help but feel a sense of nostalgia and wonder.\n\n\n\n🌊 Intro\nIt’s been a challenging year for many of us, with all the ups and downs that come with life. But despite the difficulties, I remain hopeful that things will get better.\n\n\n✨ Some highlights of the year\nOne of the biggest highlights of the year was completing my master’s thesis. This was a major milestone for me, and one that I dedicated to my late grandfather Boboi Usto, who was an educator and convinced my father to move our family to the city from a distant village to get a better education. I’m grateful for my grandfather’s influence and for the opportunity to receive a good education, and I’m proud of all the hard work I put into my thesis.\nThe experience of completing my thesis was immersive and full of learning, and I’m now working on making it more generalizable. It’s an exciting process, and I’m looking forward to refining my findings and sharing them with a wider audience.\nAs I reflect on the role that my grandfather played in my education and career, I’m reminded of the importance of appreciating our ancestors and the sacrifices they made to provide opportunities for future generations. It’s easy to take the things we have for granted, but it’s important to remember that they are often the result of hard work and dedication on the part of those who came before us.\nIn addition to completing my thesis, I also had the chance to participate in four hackathons this year, and I’m proud to say that I won in three of them. It was a great experience to work with a team to come up with innovative solutions to real-world problems, and I’m grateful for the opportunity to put my skills to the test.\nI also took the opportunity to share my thoughts and experiences through writing. I published two blog posts on Medium, which gave me the chance to reflect on my journey and share my insights with others.\n🔥 One of the other major highlights of the year was co-founding MunichNLP community in May and being able to organise 17 events in a short time span. And exchanging thoughts with great researchers from Google Research and Brain team.\nIt’s been great to be able to bring together individuals who are passionate about machine learning, and I’m looking forward to continuing to grow and develop the group in the coming year.\nThe last but by no means least, I found new HOME at MunichRe, on a very interesting and challenging project. I plan to share my learning next year through my writings, so stay tuned.\n\n\n🌄 Wanderlust within me\nIn addition to my professional and academic pursuits, I also made the most of my time by exploring the beautiful Alps and nature surrounding Bavaria. I also had the chance to travel to new countries, including Italy 🇮🇹 and Switzerland 🇨🇭 \n🍝 In Italy, I fell in love with the delicious and flavourful cuisine, which was unlike anything I had ever tasted before. From the classic pasta dishes to the mouthwatering pizzas, I couldn’t get enough of the fresh, high-quality ingredients and simple, yet satisfying flavours.\n🏔️ In Switzerland, I was mesmerised by the stunning natural beauty of the country, with its soaring peaks and crystal-clear lakes. The mountains were rugged and wild, with trails that led up to breathtaking vistas. And the lakes were so clear and pure that you could see right down to the bottom.\nThese were unforgettable experience and one that I will always treasure.\n\n\n🧩 Outro\nAs I look ahead to the next year, I have a few goals in mind. First, I want to make my fitness more consistent. I also hope to give a talk at PyData Berlin conference and attend NeurIPS ’23 (aka The Deep Learning conference of the year). I believe that setting goals is an important way to stay motivated and to continue to grow and learn, and I’m looking forward to working towards these goals in the coming year.\nTo conclude, I’m grateful for all the opportunities I’ve had and for the support of my family, friends, and colleagues. I’m excited to see what the next year brings, and I’m looking forward to continuing to pursue my passions and goals.\nAs Albert Einstein once said,\n\nI have no special talent, I am only passionately curious.\n\nI hope to stay curious and continue to learn and grow in the coming year.\n\nThank you for reading my year in review. I hope that you have enjoyed learning about my experiences and adventures in 2022. I wish you all the best and hope that your future is filled with joy and success. Thank you for your support and here’s to a bright future ahead!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learners Guide to Real World",
    "section": "",
    "text": "🚀 LLM Inference Deep Dive: Metrics, Batching & GPU Optimization\n\n\n\nllm\n\npapers\n\nperformance\n\n\n\nTechnical exploration of LLM inference metrics, batching strategies, and GPU optimization with TensorRT-LLM - from latency metrics to in-flight batching\n\n\n\n\n\nDec 9, 2024\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n🎷 NeurIPS 2023: Trends, Talks, and Technologies\n\n\n\nllm\n\ndeep-learning\n\nconference\n\n\n\n\n\n\n\n\n\nDec 17, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n🔥 The Future of AI is Modular: Insights from ModCon 2023\n\n\n\nllm\n\ndeep-learning\n\nconference\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n👾 Watermark Security in Language Models\n\n\n\nllm\n\npapers\n\nsecurity\n\ntokenizer\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n2️⃣ Concepts from Operating Systems That Found Their Way in LLMs\n\n\n\nllm\n\npapers\n\nsecurity\n\ntokenizer\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n🖥️​ LLMs for Compiler Optimization: A Deep Dive\n\n\n\nllm\n\npapers\n\n\n\n\n\n\n\n\n\nSep 19, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n📈 Exploring Ways to Extend Context Length in Transformers\n\n\n\nllm\n\npapers\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n🚀 Reflections from an Evening with NVIDIA’s Jensen Huang\n\n\n\nllm\n\ndeep-learning\n\n\n\n\n\n\n\n\n\nJul 4, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n🔮 Navigating the AI Landscape: My Takeaways from Sam Altman’s Discussion\n\n\n\nllm\n\n\n\n\n\n\n\n\n\nMay 27, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n💫 StarCoder: A Revolutionary Code Generation Model\n\n\n\nllm\n\npapers\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n🌁 Mastering the Art of Building a Defensible Business: Insights from a Discussion Panel\n\n\n\nventure-captial\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n🌉 A Deep Dive into the LLM Bootcamp Experience: Revolutionizing AI-Powered Applications\n\n\n\nllm\n\ndeep-learning\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n🗽 A Philosophical Dive into Deep Learning: My Experience at the NYU Conference\n\n\n\nconference\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n🖼️ Exploring the Latest Advancements in Transfer Learning: A Summary of ICLR’23 Transfer Learning-Related Papers\n\n\n\npapers\n\niclr\n\ntransfer-learning\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n⏳ From Alps to NLP: A 2022 Recap of Exploration and Growth\n\n\n\nreflections\n\ngoals\n\n\n\n\n\n\n\n\n\nDec 26, 2022\n\n\nmuhtasham\n\n\n\n\n\n\n\n\n\n\n\n\n🤗 Herzlich Wilkommen!\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 25, 2022\n\n\nmuhtasham\n\n\n\n\n\nNo matching items"
  }
]